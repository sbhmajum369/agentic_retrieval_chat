{
  "File_Name": "The_Anatomy_of_a_Personal_Health_Agent.pdf",
  "Section 0": {
    "Text": " \nA. Ali Heydaril,*, Ken Gu*,*, Vidya Srinivas+,*, Hong Yu1,*, Zhihan Zhang, Yuwei Zhang*, Akshay Paruchuri*,Qian Hel, Hamid Palangil, Nova Hammerquist', Ahmed A. Metwally1, Brent Winslowl, Yubin Kim', Kumar.Ayush1, Yuzhe Yangl, Girish Narayanswamy*, Maxwell A. Xu, Jake Garrison', Amy Armento Lee1, Jenny Vafeiadou1, Ben Graef, Isaac R. Galatzer-Levy?, Erik Schenck1, Andrew Barakat1, Javier Perez', Jacqueline Shreibatil, John Hernandez', Anthony Faranesh1, Javier L. Prieto, Conor Heneghan', Yun Liu1, Jiening Zhan*, Mark Malhotral, Shwetak Patell, Tim Althoffl, Xin Liul,t, Daniel McDuffl,t, Xuhai \"Orson' Xu1,3,t, \n1Google Research, 2Google DeepMind, 3Columbia University *Equal Contribution, Equal Leadership. \n#Work done while at Google Research, Work done at Google via Vituity \nHealth is a fundamental pillar of human wellness, and the rapid advancements in large language models (LLMs) have driven the development of a new generation of health agents. However, the application of health agents to fulfill the diverse needs of individuals in daily health settings is underexplored.In this work, we aim to build a comprehensive personal health agent that is able to reason about multimodal personal health data from everyday consumer wellness devices and medical records and provide personalized health recommendations. To understand end-users' needs when interacting with such an assistant, we conducted an in-depth analysis of web search and health forum queries,.alongside qualitative insights from users and health experts gathered through a user-centered design process. Based on these findings, we identified three major skill categories to fulfill consumer health.needs, each of which is supported by a specialist sub-agent in our system: (1) a data science agent that analyzes personal time-series wearable and medical record data, incorporating population-level statistics to provide contextualized numerical health insights, (2) a health domain expert agent that integrates users' health and contextual data to generate accurate, personalized insights based on health domain knowledge, and (3) a health coach agent that synthesizes data insights, drives multi-turn user interactions and interactive goal setting, guiding users using a specified psychological strategy.and tracking users' progress. Furthermore, we propose and develop Personal Health Agent (PHA), a multi-agent framework that enables dynamic, personalized interactions to address individual health needs. To evaluate each sub-agent and the multi-agent system, we conducted automated and human evaluations across 10 benchmark tasks, involving more than 7,o00 annotations and 1,100 hours of effort from health experts and end-users. Our work represents the most comprehensive evaluation of a health agent to date and establishes a strong foundation towards the futuristic vision of a personal health agent.accessible to everyone.#",
    "Image_paths": [],
    "Image_captions": [],
    "Tables": [],
    "Table_captions": [],
    "Equations": {}
  },
  "Title": "The Anatomy of a Personal Health Agent",
  "Section 1": {
    "Text": "1. Introduction\n \nRapid advancements in large language models (LLMs) have driven transformative changes across numerous domains, including health and medicine. With their ability to embed extensive medical knowledge and perform complex reasoning, LLMs have shown promise in assisting with many tasks, such as medical documentation (Lee, 2018), clinical diagnosis (McDuff et al., 2023), decision support (Benary et al., 2023), and drug discovery (Gottweis et al., 2025; Vert, 2023). In parallel,.the adoption of LLMs in consumer health domains is also accelerating. Examples include virtual health assistants (Khasentino et al., 2025), symptom checkers (Fraser et al., 2023), digital coaches &advisors (Oura Team, 2025; wHO0p, 2023), and clinical examination dialogues (Tu et al., 2024).These applications primarily focus on leveraging LLMs' linguistic and contextual understanding to provide accessible health and wellness insights. \nWith the growing adoption of everyday wearable devices, vast amounts of personal health data are generated passively and continuously. These data streams offer a granular, real-time view of personal health behaviors and biomarkers. Signals such as physical activity levels, sleep patterns, heart rate variability (HRv), and a wide range of physiological biomarkers can reflect various aspects of personal health and wellness (Zheng et al., 2024). There is growing evidence that consumer wearable devices can significantly contribute to promoting healthy behaviors (Ringeval et al., 2020). Increasingly,communities across artificial intelligence (AI), human-computer interaction (HCl), health informatics,and other domains have started exploring how LLMs can analyze and interpret such data streams to decode rich information of an individual's health and uncover actionable insights (Khasentino et al., 2025; Merrill et al., 2024a). This creates new opportunities to explore how insights from wearable data, when integrated with personal medical records, could provide personalized health assessments, generate tailored evidence-based wellness recommendations, and encourage behavior change (Jorke et al., 2025; Kim et al., 2019). However, LLM reasoning with complex, numerical time-series data remains a significant challenge (Merrill et al., 2024b) and the prior work falls short of creating comprehensive agentic systems to address a wide range of user queries. For instance,Khasentino et al. (2025) evaluated LLMs on case studies with wearable data, but their analysis lacked a deep assessment of numerical reasoning and focused only on a narrow set of fitness and sleep insights. In a separate study, while Merrill et al. (2024a) tackled a wider range of user queries on numerical data, their work did not evaluate an agent's grasp of domain knowledge or its ability to perform expert-level interactive coaching. These limitations illustrate a necessary shift beyond single-purpose health agents. Given the critical role of integrated individual information for personal health journeys (Graham et al., 2024) and building upon the multifaceted development of LLMs'capabilities, we envision that the era of intelligent personal health agents is arriving.. \nPrior research has found that individual needs for daily health and wellness support are highly diverse and a range of capabilities are required (Merrill et al., 2024a; Srinivas et al., 2025). For instance, a closed-ended query of \"On average, how many hours have I been sleeping this last month?\" requires different skills versus an open-ended query of \"what can I do to improve my sleep quality?\". Therefore,personal health agents need to be equipped with a broad set of capabilities to handle various requests from users. In this work, we aim to build a comprehensive personal health agent that integrates wearable sensor data and medical records with LLMs to provide dynamic, personalized interactions capable of addressing a broad range of individual health needs.. \nWe adopted a user-centered design process to architect a health agent that covers a wide range of general wellness, fitness, and medical topics. To better understand users' specific needs, we collected and analyzed real-world data from multiple sources across end-user online queries and discussion,survey results, as well as a expert design workshop. Synthesizing these data revealed four critical categories of user queries (see Supplemental Table S1), including (1) general health and wellness knowledge, (2) personal data, (3) wellness advice, and (4) personal health symptoms.. \nThese four categories highlight several important capabilities that a personal health agent needs to be equipped with: analyzing personal data, interpreting the data in health contexts, and providing personalized actionable suggestions to users. This sheds light on the three synergistic key roles of our personal health agent, each empowered by an advanced LLM sub-agent with specific skills or roles:The Data Science Agent (DS Agent) analyzes the user's personal health data from wearables (e.g.,Fitbit) alongside population-level time-series data to derive numerical insights, such as estimating changes in running speed from workout logs (e.g., \"Has my running gotten faster since last month?').The Domain Expert Agent (DE Agent) draws on personal medical records, wearable data, and health knowledge bases to provide domain-specific and contextualized interpretations, such as explaining specific biomarkers or general health conditions (e.g., \"Is a blood pressure of 137 over 83 fine?\") or comparing a user's data to general population statistics. Finally, the Health Coach Agent (HC Agent) (a) User-Centered Design to Anatomize Personal Health Needs. (b) Architecture of the Personal Health Agent (PHA) (c) Evaluation Studies for Individual Sub-Agents and the Entire Multi-Agent PHA System. \napplies evidence-based psychological strategies, like motivational interviewing (Basar et al., 2024), to help users set appropriate goals, identify barriers, and develop personalized plans to foster lasting behavior change. \nThe roles of these sub-agents are not independent. A wide range of end-user queries naturally involve more than one agent. For instance, the query \"I want to understand my sleep data last week and know how I can improve my sleep\" would require the DS Agent to analyze sleep and other related wearable data and present the results, the DE Agent to provide evidence-based sleep knowledge and the HC Agent to engage in dialog with the user and provide personalized sleep advice. Therefore, to provide a comprehensive user experience, we developed the Personal Health Agent (PHA), a multi-agent system composed of an orchestrator and the three specialized sub-agents, as shown in Figure 1(a).By design, our PHA integrates capabilities that span data science, health domain expertise, and coaching, intentionally blurring the lines between consumer wellness and clinical knowledge domains.This approach is not intended to supplant human health providers, but to explore the technological frontier of consumer empowerment, envisioning how a unified agent can provide more holistic,actionable health insights. \nOur PHA enables coordinated, context-aware interactions across agents. Drawing inspiration from a range of multi-agent collaboration strategies, such as voting (Chen et al., 2024; Wang et al., 2022),iteration (Tang et al., 2024), debate (Du et al., 2024), and hierarchical team formation (Kim et al.,2024), we propose a set of design principles for multi-agent collaboration for health applications,leveraging the advantages of different individual agent capabilities, and design our PHA architecture with iterative orchestration and collaboration. This multi-agent framework enables dynamic collaboration across the three sub-agents and enables personalized interactions with end-users, seamlessly combining data analysis, domain expert knowledge, and health coaching to support a broad range of individual health needs. \nTo validate our system, we developed a holistic evaluation framework (Figure 1(b)) that assesses performance at two distinct levels: individual sub-agent capabilities and the integrated multi-agent system's overall efficacy. At the individual level, we evaluated each agent's core competencies: the DS Agent was benchmarked on its ability to generate robust analysis plans (Section 4); the DE Agent on its capacity for evidence-based reasoning and providing factual knowledge (Section 5);and the HC Agent through user-centered studies involving both end-users and experts to assess its coaching effectiveness (Section 6). At the system level, we evaluated the integrated multi-agent PHA framework through comprehensive studies with both end-users and health experts, using open-ended,multi-modal conversations that reflect diverse health scenarios (Section 7). In total, our evaluation comprises 10 benchmark tasks and draws upon over 7,000 human annotations across 1,120 hours of human effort (expert: 559 hours, end-users: 561 hours), validating the advantage of each specialized sub-agent and demonstrating the promising real-world applicability of our integrated multi-agent health agent. \nBy providing a comprehensive multi-agent architecture and a holistic evaluation framework, our work establishes a concrete foundation for the future development of accessible personal health agents to help improve individuals' everyday health and well-being.",
    "Image_paths": [
      "D:\\My_Codebase\\Github\\agentic_retrieval_chat\\sample_docs\\pdf_output\\The_Anatomy_of_a_Personal_Health_Agent\\page_metadata\\meta_images\\img_in_image_box_296_374_2173_752.jpg",
      "D:\\My_Codebase\\Github\\agentic_retrieval_chat\\sample_docs\\pdf_output\\The_Anatomy_of_a_Personal_Health_Agent\\page_metadata\\meta_images\\img_in_image_box_271_849_2194_1850.jpg"
    ],
    "Image_captions": [
      "Figure 1 | Overview of The Personal Health Agent. (a) We present the anatomy of personal health needs with a user-centered design process. (b) We then design an multi-agent system, powered by state-of-the-art large language models, consisting of specialist Data Science (Ds), health Domain.Expert (DE), and Health Coach (HC) agents to address diverse consumer health needs. \"Base model\".refers to the Gemini 2.0 family. (c) Our work represents the most comprehensive evaluation of a health agent to date (>7,000 annotations and 1,100 hours of effort on 10 benchmarks).."
    ],
    "Tables": [
      [
        {
          "0": "Assessment Goal",
          "1": "Assessment Goal",
          "2": "Evaluation",
          "3": "Scale",
          "4": "Human Hours"
        },
        {
          "0": "DS.1",
          "1": "The agent's ability to design a statistical analysis plan.",
          "2": "Human: Data Scientists; Auto Eval*",
          "3": "354 Plans x 2 Agent Conds (708 Results)",
          "4": "47"
        },
        {
          "0": "DS.2",
          "1": "The agent's ability to generate code to execute the analysis plan.",
          "2": "Human: Software Engineers; Auto Eval*",
          "3": "173 Tests (25 Queries) x 2 Agent Conds (346 Results)",
          "4": "75"
        },
        {
          "0": "DE.1",
          "1": "The agent's medical knowledge and its ability to reason over case studies mirroring real-world scenarios.",
          "2": "Auto Eval",
          "3": "2145 MCQs x 2 Agent Conds (4290 Results)",
          "4": "-"
        },
        {
          "0": "DE.2",
          "1": "The agent's ability to generate a differential diagnosis when provided user symptoms.",
          "2": "Auto Eval",
          "3": "1511 User Symptoms x 2 Agent Conds (3022 Diagnosis)",
          "4": "="
        },
        {
          "0": "DE.3",
          "1": "The agent's ability to personalize answers to medical questions based on context and information about the user",
          "2": "Human: Generalists (End-users)",
          "3": "50 Queries x 2 Agent Conds x 17 Raters (1700 Annotations)",
          "4": "13"
        },
        {
          "0": "DE.4",
          "1": "The agent's ability to reason and interpret multi-modal health data. (wearables, lab results, demographics, health surveys)",
          "2": "Human: Experts (MD physicians)",
          "3": "30 Personas x 2 Agent Conds x 5 Raters (300 Annotations)",
          "4": "77"
        },
        {
          "0": "HC.1",
          "1": "The agent's ability to conduct coaching conversations from end-users'. perspectives.",
          "2": "Human: Generalists (End-users)",
          "3": "31 Raters (Multi-turn) x 2 Agent Conds. (62 Annotations)",
          "4": "48"
        },
        {
          "0": "HC.2",
          "1": "The agent's ability to conduct coaching conversations from health coach. experts' perspectives.",
          "2": "Human: Experts (Health Coaches)",
          "3": "31 Interactions x 2 Agent Conds x 3 Raters (186 Annotations)",
          "4": "90"
        },
        {
          "0": "P.1",
          "1": "The multi-agent's ability to synthesize personal data (Ds), health domain Human: Generalists knowledge (DE), and coaching (HC) capability in a conversation about. achieving personal health goals, from end-users' perspectives.",
          "2": "(End-users)",
          "3": "50 Personas x 3 Agent Conds x 20 Raters (3000 Annotations)",
          "4": "500"
        },
        {
          "0": "P.2",
          "1": "The multi-agent's ability to synthesize personal data (Ds), health domain. knowledge (DE), and coaching (HC) capability in a conversation about. achieving personal health goals, from coach experts' perspectives..",
          "2": "Human: Experts (Health Coaches)",
          "3": "50 Personas x 3 Agent Conds x 5 Raters. (750 Annotations)",
          "4": "270"
        }
      ]
    ],
    "Table_captions": [],
    "Equations": {}
  },
  "Section 2": {
    "Text": "2. A User-Centered Design of Personal Health Agent Requirements\n \nWe adopted a user-centered approach to identify and categorize end-users' needs for a personal health agent. Our process integrated insights from three layers of complementary data collection:real-world user queries, targeted survey responses, and expert synthesis. Together, these layers form a \ncoherent pipeline: we began by capturing broad, naturalistic user intent through consumer health.queries; we then deepened this understanding through structured survey data that elicited users'goals and satisfaction. Lastly, we conducted expert workshops to distill and organize these findings into actionable requirements..\n2.1. User-Centric Data: \n \nThe collection and interpretation of user-centric data included three aspects: \nConsumer Health Queries. We compiled and analyzed consumer health queries from Google Search,.Gemini, the Fitbit community forum, from January 2024 to February 2025. In addition, in October 2024 through February 2025, we launched Insights Explorer through Fitbit Labs (Fitbit Community,2024). Participation was entirely voluntary and required users to explicitly opt-in and provide in-app consent for their data to be used for research and development. As part of this consented experience,.participants were able to ask health queries and receive responses from a prototype generative AI conversational agent. We composed a set of 1370 de-identified user queries that represent a wide range of questions regarding daily health and wellness from real-world users. Supplemental Table S1lists out some examples of these queries. \nFitbit Labs Survey Data. To enrich the query data, we also collected survey data from participants $\\scriptstyle(N\\scriptstyle=\\;555)$ + in the Insights Explorer Fitbit Labs to ask about their personal health goals and sentiments when interacting with the agent. Specifically, the survey includes questions about user needs (e.g.,.\"What are you hoping to achieve by using [the prototype]?\", \"Which of the following aspects of health and wellness, if any, did you explore in [the prototype]?\"), user experience and satisfaction (e.g.,\"Indicate any aspects of [the prototype] that have / have not met your expectations\") , engagement and retention (e.g., \" What was the main functionality or benefits that you continued using [the.prototype] for?\" ), and other user characteristics (e.g., prior experience with wearable sensing, AI,and conversational agents). Supplemental Table S2 lists out details of these lab surveys. \nExpert Workshop. To complement users' perspective, we further conducted a one-hour critical user.journey workshop with experts across user experience, product, research and engineering $(scriptstyle\\ N=14)$ I to categorize the data collected. The workshop procedure focused on summarizing the user journey goals and the specific associated tasks. After collecting multiple experts' perspectives, we adopted a standard thematic analysis process (Maguire and Delahunt, 2017). Two researchers independently coded the same subset of these queries and all survey data to jointly develop a codebook of hierarchical user journeys (categories and sub-categories). The same researchers then categorized the rest of the.user queries into these categories and expanded the codebook as needed.\n2.2. Major Categories of Use-Cases with Personal Health Agent: \n \nConsolidating results from user queries, surveys, and the expert workshop, we identified four representative categories of critical user journeys (CUJ) for a personal health agent. \nGeneral health knowledge: These queries focus on factual knowledge related to broad health topics across medical, fitness, and general wellness areas, where users aim to expand their understanding of specific health concepts and improve their overall health knowledge. This category includes questions aimed at discovering and comparing health-related facts, exploring the pros and cons of different treatments or behaviors,and staying up-to-date with recent health news. \nExamples: (i) \"How long is strep contagious?\" (ii) \"What are the pros and cons of HIIT (high intensity interval training)?\" (iii) \"What is the difference between HRV (heart rate variability) & RHR (resting heart rate)?\" \nPersonal data insights: This category includes queries where users seek to understand their own health data, whether collected through wearable devices or available in their personal medical records (e.g., lab results). These questions often involve interpret-.ing patterns in their health metrics, exploring the relationship between data aspects,comparing their measurements against clinical guidelines, and identifying meaningful changes. \nExamples: (i) \"What is my average resting heart rate on days when my sleep score is above 86?\" (ii) \"How did my steps impact my resting heart rate in the past month?\" (ii) \"Can you review my HRV data from this month and highlight any unusual occurrences?\" \nWellness advice: In this category, users mainly focus on seeking actionable insights and health advice, either general or data-specific, to enhance their wellness. Queries.include requests for general wellness tips, tailored advice based on personal data, and support in identifying potential areas of improvement. Some queries also involve asking.guidance on setting or adjusting goals, creating plans to achieve those goals, overcoming.obstacles, and tracking their progress.. \nExamples: (i) \"To improve my sleep score, what time should I go to bed?\" (ii) \"It's raining.Adjust my workout to be indoors today.\" (iii) \"What are some of the things I have done.that have the biggest impact on my blood pressure?\" \nCUJ 4Personal medical symptoms: This category involves asking about specific symptoms or assessing potential causes that are related to users' personal experience. Some queries require back-and-forth conversations to collect additional information from users.* \nExamples: (i) \"My throat is itchy and I have trouble swallowing. What might be going on?\"(ii) \"How can I tell if I have high blood pressure?\" (ii) \"I just threw up blood. What might be going on?\". \nIt is worth noting that these categories are not mutually exclusive at the level of an individual query or conversation. For example, a query about personal data can be embedded in another query for wellness advice. Furthermore, these do not exhaustively cover all user needs but rather encompasses.a wide range of needs and we exclude other queries that are less related to health (e.g., privacy, safety,.social relationships). Supplemental Table S1 provides a summary of these categories..",
    "Image_paths": [],
    "Image_captions": [],
    "Tables": [],
    "Table_captions": [],
    "Equations": {}
  },
  "Section 3": {
    "Text": "3. Personal Health Agent and Evaluation Framework\n \nOur analysis of user needs identified four CUJs, spanning data interpretation, knowledge seeking,wellness advice, and symptom assessment. Satisfying these diverse requirements demands more than.a single monolithic system and calls for a set of specialized, complementary capabilities. Our approach is therefore grounded in both the cognitive theory of modular cognition (Fodor, 1983), which posits.that the mind uses specialized subsystems for different tasks, and the empirical success of human.health teams, where experts with complementary skills collaborate. Inspired by this, we deconstruct.the problem space and propose a multi-agent personal health assistant that brings together three core roles, each embodied by a specialized sub-agent: data analysis, health domain expertise, and behavior coaching, working in concert to emulate a professional health support team..\n3.1. User-Centered Agent Design: \n \nData Science (Ds) Agent: This agent mainly focuses on addressing user queries about personal data. Existing research has explored LLMs' ability to answer data science questions (Cheng et al., 2023; Gu et al., 2024b; Hong et al., 2024; Hu et al., 2024a; Li et al., 2024;Merrill et al., 2024a; Wu et al., 2024a; Yin et al., 2023; Zhang et al., 2025, 2024), but their capacity to interpret open-ended, personalized data inquiries and perform statistically sound data analyses remains unclear, and there is a lack of open benchmarks assessing their reliability in this area. We develop the first personal open-ended data science agent capable of analyzing both personal and population-level time-series wearable data to provide numerical health insights. Our agent enables.users to ask diverse, open-ended, and personalized questions about their health data and provide meaningful, data-driven responses, filling a critical gap in personalized data analysis. We also opensource an evaluation benchmark for these capabilities. Example conversations between a user and the DS agent are shown in Supplemental Figure S3. \nDomain Expert (DE) Agent: This agent aims to answer any queries that are related to medical knowledge or daily symptoms. While prior work has evaluated LLMs' medical knowledge (Nori et al., 2023, 2024; Singhal et al., 2023), many essential skills of an ideal domain health expert are underexplored, such as interpreting data within contexts, e.g., population.and environmental factors, and performing summarization and reasoning across data modalities from wearables and medical records. We build a domain expert agent equipped with a suite of tools to address these challenges. The agent integrates users' wearable data, medical records, and contextual data to generate accurate, relevant responses to complex health inquiries. It advances the state-of-the-art LLMs by providing nuanced multi-modal interpretations that consider the broader context of personal health. Example conversations with the DE agent are shown in Supplemental Figure S4. \nHealth Coach (HC) Agent: The HC agent aims to provide personalized health and wellness advice, and to help users set up goals and motivations via conversations. Although previous studies have initiated explorations into the application of AI in personal coaching (Jorke et al., 2024; Mantena et al., 2025; Mercado et al., 2023), our agent is the first to support open-ended coaching conversations across a wide range of general wellness topics. Building on coaching expert insights, we significantly enhance the agent's coaching skills, including motivational interviewing.and personalized health recommendations. This agent adopts a modular structure to optimize the conversation flow, balancing information gathering, active listening, and recommendation delivery to support a smooth and effective coaching experience. Example multi-turn conversations with the HC.agent can be found in Supplemental Figure S9-Figure S10. \nPersonal Health Agent (PHA) - Multi-Agent Collaboration: The three sub-agents have G complementary skill sets and often need to collaborate together to provide appropriate support regarding a user's query. A central orchestrator manages this collaboration. It receives the user's initial query, parses it, and determines which agent(s) are best suited to respond.It decomposes the query into sub-tasks, routes them to the appropriate sub-agents, synthesizes the responses into a cohesive, user-friendly answer, and then reflect and iterate on it prior to responding.to users (Tran et al., 2025). Supplemental Figure S11-Figure S14 demonstrates a multi-turn conversation example with PHA.\n3.2. Agent Evaluation Framework: \n \nIn order to systematically evaluate our agent framework and identify the strengths and gaps in the design of each component, we created the evaluation framework shown in Figure 1(b). We took a mixed-methods approach and conducted a set of studies that examine a comprehensive set of factors:1. Interaction Turn: Single and multi-turn interaction. \nData Modalities: Agent processing with both single- and multi-modality data. \nEvaluation Setup: Automatic and human evaluations of conversations. \nHuman Perspective: Health experts' and end-users' perspectives on multi-turn conversations. \nThis multi-faceted approach provides a holistic understanding of the AI agent's performance and is the most comprehensive evaluation of a consumer AI health agent to date. \nSpecifically, our framework delineates and assesses the core capabilities of each component. The DS Agent is evaluated on its two-stage process of (Ds.1) generating robust analysis plans and (DS.2) translating them into accurate, executable code. The DE Agent is assessed across four key competencies: (DE.1) answering common health questions, (DE.2) handling diagnostic conversations,(DE.3) personalizing answers to medical questions based on context and information about the user,(DE.4) interpreting and reasoning with multi-modal health data (wearables, lab results, demographics,health surveys). The HC Agent's capability of providing personalized coaching experience is evaluated from (Hc.1) its effectiveness from the end-user's perspective and (Hc.2) its fidelity to human expert coaching principles. After connecting all three sub-agents, the final multi-agent PHA system has the comprehensive set of skills to support various end-user health queries at a human health expert level. It is assessed on its holistic conversational performance from complementary viewpoints of both (PHA.1) end-users and (PHA.2) experts.\n3.3. Choice of Base Language Model: \n \nWe used the Gemini 2 family (Flash/Pro) of models (The Gemini Team, 2023) for our experiments..Gemini has demonstrated strong performance on medical tasks (Khasentino et al., 2025; Saab et al.,.2024; Yang et al., 2024). In this work, our focus is not to train a new LLM but rather to design and construct the agentic system around a given base LLM. By selecting Gemini as the base model we could ensure that (i) there was no training data contamination, and (ii) no health data from.real participants would be logged by the model and used for training future models. While existing contamination was not a significant concern, as a majority of our analyses involve novel datasets that are unlikely to be part of any LLM training dataset, choosing Gemini allowed us to verify that any.testing of the models did not lead to data entering a future training set. We note that our design of sub-agents and the agent teams are agnostic to the base LLM. We expect that our proposed solutions will be generalizable to other LLMs.\n3.4. Real-World Dataset for Evaluation: \n \nTo ground our agent evaluation with real-world validity, we evaluated our framework's applicability and robustness against the Wearables for Metabolic Health (WEAR-ME) study, a large-scale $((mathrm{N}==1165)$ effort to investigate metabolic health (Metwally et al., 2025). The overall study design is outlined in Figure 2(a). As part of WEAR-ME, consented participants were asked to link their Fitbit account to Google Health Studies (GHs), authorizing the collection of their wearable data for the study's duration.and for up to three months before they joined. Once enrolled, participants were asked to (i) complete questionnaires covering demographics, health history, and personal health insights, (ii) schedule a blood draw with a Quest Patient Service Center, and (ii) wear their Fitbit or Pixel Watch throughout the day and night (for at least 3 out of every 4 days). In total, wEAR-ME contains multimodal data from each participant on their questionnaire answers, Fitbit records, and blood test results. More.details of the dataset are described in Supplemental Section B.2. The WEAR-ME study was conducted in full compliance with privacy regulations, with the study approved by the Advarra Institutional Review Board (IRB, #Pro 00074093). All participants provided informed e-consent for their data to be used in research and publications. \nThroughout our evaluation, we leveraged the WEAR-ME dataset for various evaluation tasks (see tasks DS.1, DE.3, DE.4, HC.1, HC.2, PHA.1, PHA.2 in Figure 1). As introduced in evaluation details in later (a) Recruiting and Collection Design of the WEAR-ME Study (b) WEAR-ME Subset Used for Evaluation Experiments \nsections, we contextualize our end-user and expert evaluations on a subset cohort of WEAR-ME for real-world validity. Working with clinicians, we identified the 10 most common health profiles among the participants and then randomly selected five individuals representing each of those conditions (see the specific 10 profiles and persona examples in Supplemental Section B.2). These 50 user personas were used to generate evaluation samples for our agents' multiple evaluation tasks (e.g.,Section 5.3.3, Section 5.3.4 for the DE agent, and Section 7.3.1, Section 7.3.2 for the PHA). The construction of the evaluation cohort is depicted in Figure 2(b).",
    "Image_paths": [
      "D:\\My_Codebase\\Github\\agentic_retrieval_chat\\sample_docs\\pdf_output\\The_Anatomy_of_a_Personal_Health_Agent\\page_metadata\\meta_images\\img_in_image_box_275_294_392_464.jpg",
      "D:\\My_Codebase\\Github\\agentic_retrieval_chat\\sample_docs\\pdf_output\\The_Anatomy_of_a_Personal_Health_Agent\\page_metadata\\meta_images\\img_in_image_box_286_1699_395_1875.jpg",
      "D:\\My_Codebase\\Github\\agentic_retrieval_chat\\sample_docs\\pdf_output\\The_Anatomy_of_a_Personal_Health_Agent\\page_metadata\\meta_images\\img_in_image_box_484_485_1993_1470.jpg",
      "D:\\My_Codebase\\Github\\agentic_retrieval_chat\\sample_docs\\pdf_output\\The_Anatomy_of_a_Personal_Health_Agent\\page_metadata\\meta_images\\img_in_image_box_527_1622_1959_2603.jpg"
    ],
    "Image_captions": [
      "Figure 2 | Overview of the wearables and metabolic health study (WEAR-ME) study. (a) The WEAR-ME study consists of Fitbit users who opted-in to participate in our study. Those who consented and enrolled in the WEAR-ME study were then asked to visit one of the Quest centers for blood biochemistry data collection. Each participant contains multimodal data from Fitbit, blood test results,and questionnaires. (b) For our human evaluations, we selected a subset of the WEAR-ME data based on prevalent health conditions present among the WEAR-ME participants. We identified 10 common profiles, and selected five individuals at random from those profiles. More details are introduced in Supplemental Section B."
    ],
    "Tables": [],
    "Table_captions": [],
    "Equations": {}
  },
  "Section 4": {
    "Text": "4. Data Science Agent\n \nData science is the extraction of knowledge from information and enables the identification of patterns and prediction of outcomes (Dhar, 2013). The increasing digitization of healthcare has resulted in an unprecedented volume of personal health data, including electronic health records, genomics,medical imaging, and wearable sensor data. While prior work has studied and evaluated data science agents capable of executing code given clear, precise instructions (e.g., \"Calculate the mean and standard deviation of temperature value in the 'Mar.2019' column') (Gu et al., 2025a; Guo et al.,2024; Hu et al., 2024b), it remains unclear whether LLM-based agents can interpret, decompose, and operationalize open-ended, underspecified, and personalized data inquiries (e.g., \"Am I sleeping well?\")into statistically sound analyses.. \nThis capability is especially critical in high-stakes domains like personal health, where flawed analytical decisions can lead to misleading or even harmful conclusions (Simonsohn et al., 2020; Steegen et al.,2016b). However, despite its importance, there is currently no systematic benchmark for evaluating analysis planning in the context of personal health. To address this gap, we focus on the Data Science (DS) Agent's ability to perform robust statistical analyses on personal health data and deliver valid and actionable numerical insights.\n4.1. Data Science Agent Capabilities: \n \nThe specific statistical methods that are necessary to answer a given user query can vary. For example,\"Does performing activities give me better sleep?\" and \"How much exercise do I get compared to my age group?\" require different reasoning steps. The DS Agent needs to handle diverse user queries,many of which are often underspecified and ambiguous, i.e., multiple valid approaches or perspectives in answering the question (Liu et al., 2020a, 2019; Steegen et al., 2016a). Nevertheless, an ideal DS Agent should be able to perform a logical and rigorous analysis that aligns with at least one of the core intents of the query (i.e., the user will get a satisfactory answer from the analysis) and provide numerically accurate and statistically robust results from which the user can derive confident conclusions (e.g., not claiming significant trends from an insufficient number of data points). \nWe focus on a set of capabilities for the data science agent that are useful, yet tractable to evaluate (Gu et al., 2024b; Merrill et al., 2024a; Wu et al., 2024b). These capabilities are informed by the types of user interactions and data that are expected in the context of the CUJs derived in Section 2.2. Broadly,we define the capability of the DS Agent to address queries that involve analysis of numerical data (e.g., wearable streams, medical records), with the goal as obtaining statistical outcomes that align with the query intent and context surrounding those statistics, such as timeframe, type of statistics,assumptions, etc. Specifically, the capabilities of the agent are to:",
    "Image_paths": [],
    "Image_captions": [],
    "Tables": [],
    "Table_captions": [],
    "Equations": {}
  },
  "Section 5": {
    "Text": "DS1\n \nDecompose An Open-Ended Query into a Structured Analysis Plan: For broad or ambiguous queries often posed by end-users without data science expertise, the DS Agent should translate them into appropriately specific, unambiguous, tractable queries and analysis plans by:. \na)Identifying conceptual variables (Jun et al., 2022): Recognizes the concepts in the \nquestion that can and should be represented in the data (e.g., representing fitness as average daily steps and resting heart rate for a query like 'Am I getting more fit over the past few months?\"). \nb)Formulating the correct and exact data transformation steps to operationalize the variables across multiple tables of user data and population-level data tables. This involves the following optional steps: \ni)Filtering relevant subsets of data (e.g., filter for activities in the last 3 months),ii)Cleaning messy data (e.g., handling missing or misrecorded wearable data). \nii)Joining multiple tables (e.g., joining the activities data table with the daily.summary data table on the same day), \niv)Aggregating data (e.g., getting average steps and resting heart rate grouped.by month). \nc)Recognizing, highlighting or addressing missing/insufficient data to prevent unreliable conclusions. \nd)Applying appropriate statistical tests and calculations based on the user's intent. \nNote: Given the open-ended nature of user queries, multiple interpretations and answers may be valid. Accordingly, in one single interaction, our DS Agent is designed to provide at least one appropriate response, rather than an exhaustive enumeration of all possibilities.Users can have follow-up conversations to explore other options with the agent. In the case.where the data in user queries is not available, the agent might check the availability of the data with users and request data from users, if needed.. \nCompute accurate numerical results through code generation, execution, and.feedback: Based on a precisely specified analysis plan (i.e., one without any ambiguity for alternative methods and models Liu et al. (2020b)), the agent should be able to generate, execute, and debug code that follows the exact specifications of the analysis plan, such that it can execute bug-free code and compute accurate results.\n4.2. Data Science Agent Architecture: \n \nBased on the delineated capabilities, the DS Agent's architecture is designed as a modular, two-stage.pipeline: (1) Analysis Plan Generation and (2) Code Generation with Iterative Execution (Gu et al.,.2024a). This decomposition is critical for mitigating the risk of factual inaccuracies and logical.errors common in end-to-end generation for complex data tasks (Jiang et al., 2024). It separates the semantic reasoning of \"what to do' from the syntactic task of \"how to do it.\" \nThe first stage translates an open-ended natural language query into a precise, structured analysis plan. To ground its reasoning, the planner is provided with a rich contextual summary of the available data, including table schemas, column descriptions with data types and semantic tags, and summary.statistics (Dibia, 2023). The output is a structured natural language paragraph that explicitly outlines the conceptual variables, required data transformations (e.g., filters, joins, aggregations), and the chosen statistical model, directly instantiating the components listed in Section 4.1.. \nThe second stage receives this structured plan and is tasked with generating executable code. This.focus on a well-defined specification allows the LLM to leverage its strong code generation capabilities.(Jiang et al., 2024). The generated code is then executed in a sandboxed environment. We employ.an iterative refinement loop: if execution fails due to syntactic errors or runtime exceptions, the.agent re-invokes the stage with the original query, plan, and the error message as feedback. This.self-correction cycle, which has been shown to improve robustness (Quoc et al., 2024), continues until bug-free execution is achieved and a numerical result is produced.. \nFigure 3(a) illustrates the architecture enabling these functions in the DS Agent. The details of agent \nprompt and architecture are included in Supplemental Section C.3..\n4.3. Data Science Agent Evaluation: \n \nTo ensure our DS Agent produces statistically robust and numerically accurate answers, we independently evaluate the two core components in two tasks.. \nEvaluation Goal. Given an open-ended query involving data analysis (e.g., 'Am I getting more fit recently?'), we measure the agent's ability to decompose and/or expand the query into a precise and robust statistical analysis plan. The plan should align with the query's intent and appropriately involve the analysis of personal health data.. \nIn this paper, we focus on the data analysis on the wearable data from the WEAR-ME dataset (Section 3.4). In particular, the DS Agent is given access to the following data tables for each individual user: (1) a daily summary table with daily activity and sleep wearable data, (2) an activities table where each row represents a recorded physical activity (e.g., running, biking, swimming etc.), and (3) a population summary table that provides daily wearable percentiles stratified by gender and age.group. We include the full data schema of all tables used by the DS Agent in Supplemental Section C.2. \nEvaluation Setup. An analysis plan involves many decisions that contribute to the robustness of the final result, such as handling missing data, choosing a statistical model, and operationalizing constructs (Liu et al., 2020b,c; Steegen et al., 2016b). Evaluating these decisions with human experts is prohibitively expensive. To scalably evaluate a wide range of analysis approaches, we develop another LLM as an autorater (Chiang and Lee, 2023; Zheng et al., 2023) tested against human expert raters to ensure its reliability.. \nEvaluation Rubrics and Point-Detection Metrics. To enable this, we first developed a detailed analysis plan rubric in consultation with a panel of four data science experts. The rubric is designed for objective assessment across six key dimensions:. \nTimeframe: Does the analysis consider a specific timeframe (e.g., past month, past 6 months)when it helps personalize and align with user intent?. \nData Transforms: Does the approach correctly reference existing columns and apply any necessary data transformations, without logical errors, in a way that others can reliably reproduce?3. Data Sufficiency: Does the approach reasonably and clearly assess whether there is enough data, both in terms of available rows and non-missing cells within columns, to support the intended transformations, calculations, and statistical analyses?. \nSummary Statistics: Does the approach appropriately calculate summary statistics where helpful, and are the chosen statistics reasonable given the context and question being answered?5. Statistical Tests: Does the approach appropriately apply statistical tests when helpful, with clear, reproducible methods, reasonable test choices, and proper consideration of distributional assumptions? \nOverall Alignment: Finally, given the evaluation of the above dimensions, does the approach align with the user's intent?. \nOur final rubric consists of 26 items arranged in a conditional hierarchy. This structure means some items only apply if a preceding one is satisfied; for example, we only assess the appropriateness of a statistical test if the initial hypotheses are sound. To quantify quality, we used a point-deduction system where errors or omissions lower the final score. Because of this hierarchy, a maximum of 16.points can be deducted in any single evaluation. The detailed rubric development process and point system are provided in Supplemental Section C.4.1.. (b) Expert Evaluations of Plan Generation and Code Implementation \nBenchmark Dataset of Query-Analysis Plan Pairs. Evaluating the quality of generated analysis plans and developing a scalable autorater requires a reliable benchmark. To this end, we created an expertrated dataset of 141 unique query-analysis plan pairs. These pairs were generated using various LLMs (Gemini 1.5-2.0, GPT-4o) across 100 distinct health queries (sampled from the end-user health query set collected in Section 2.1). 10 expert data scientists (8 males, 2 females, aged 25 - 45, all Ph.D.s in computer science, with 5 to 20 years of experience) spent 47 hours in total and provided 354 approach annotations based on the rubrics defined above (each query-analysis plan pair was annotated by 2-4experts), leading to a total of 6157 rubric item annotations. This expert-rated dataset demonstrated a moderate level of inter-rater reliability (IRR), with an average agreement of 80.2% and a Bennett's S of 0.622 (Bennett et al., 1954). Detailed statistics can be found in Supplemental Table S5.. \nDevelopment of a Reliable LLM Autorater. Using this human-rated dataset, we developed and validated an autorater based on Gemini 2.0 Flash. The dataset was split into development and test sets,with the test set comprising 96 evaluations of 25 unique queries. The development set with 75 queries was then used to train the autorater (the specific process is detailed in Suppmental Section C.5)..Our final autorater achieved an Intraclass Correlation Coefficient (ICC(3,1)) of O.838 when compared to the scores assigned by human experts, indicating high reliability for scaled evaluation (Zheng et al.,2023). \nWe then assess the quality of the DS Agent generated analysis plans using this developed autorater,and compare them to the baseline Gemini 2.0 Flash model. The plan generation prompt templates for both DS Agent and the baseline are listed in Supplemental.1 Section C.3.. \nEvaluation Results. The results presented in the left of Figure 3(b). The DS Agent demonstrated a statistically significant improvement in analysis plan quality compared to the baseline Gemini model.In general, the DS Agent achieved an average score of 75.6+1.4% (means.e., meaning 24.4% of.the points were deducted according to the rubric), marking a substantial increase over the baseline's $53.7{\\pm}1.8\\$ , Wilcoxon signed-rank tests with rank-biserial correlation coefficient.r as the effect size). Most notably, the agent demonstrated a transformative improvement on the Data.Availability Check, rising from a near-failure rate of.$5.3\\pm1.6\\$ . The base model's.deficiencies in detecting missingness and artifacts are well-documented limitations of current large language models (Gu et al., 2025b). Its ability to correctly identify the analytical Timeframe was also significantly enhanced, achieving a performance at.$Q6.0{\\pm}1.9\\$ '. Significant,.albeit more moderate, gains were also observed for Data Transforms $\\left(p=0.002,r=0.512\\right)$ I and.Alignment with the user's query $\\left(p=0.011,r=0.467\\right)$ -$\\left(p=0.011,r=0.467\\right)$ \nConversely, no significant difference was found for Statistical Tests.$(mathtt p0=0.630)$ , a category where the baseline model already performed strongly (81.3%). These findings suggest that while the agent's architecture provides a distinct advantage in planning and grounding an analysis in practical constraints, the selection of standard statistical methods is a task for which the foundational model is.already well-optimized. Full statistical details are available in Supplemental Table S6.. \nOverall, these results validate our architectural design for the analysis plan generation part. By first generating an analysis plan explicitly with a structured process (Section 4.1), the DS Agent outperforms the baseline in bridging the gap between a user's open-ended, underspecified queries and the concrete plan. This initial planning step is an essential prerequisite for the next step of analysis plan implementation. \nEvaluation Goal. Given a precisely specified analysis plan from Task 1, this task evaluates the agent's ability to translate that plan into code that is not merely executable, but also functionally correct and.robust. The primary goal is to measure the implementation's fidelity to the plan and the numerical \naccuracy of its results, especially its ability to handle key data science challenges. This includes managing edge cases (e.g., limited data, complex data transformations) and provide a correct answer to the user's question. \nEvaluation Setup. To objectively evaluate the code, we manually converted the analysis plan into a precise function header and doc string to the agent following prior work (Chen et al., 2021). The.code was written in Python in a sandbox environment with libraries such as numpy, pandas, and scipy..The details of implementation are included in the Supplemental Section C.3. \nIn particular, a group of 7 data scientists (5 males, 2 females, aged 22 - 45, with 5 to 10 years of experience) were recruited for the code generation evaluation task. They were provided with 25query-approach pairs with high ratings sampled from Section 4.3.1 and were asked to formulate each of them as a coding problem with a function header and doc string (clear return output structure).Next, experts were asked to write the function implementation and associated test cases as unit.tests. Going beyond simply checking for executable code, these unit tests were designed to rigorously assess the correctness and robustness of the underlying logic. They evaluated the system's ability to handle key analysis cases, including managing limited data, performing complex data transformations,applying appropriate statistical methods, and ultimately, providing an accurate answer to the user's question. In total, we collected code detailing comprehensive test cases for 173 unit tests from these 25 query-approach pairs, with the number of tests per pair varying based on the complexity of.the query (75 hours of human effort). Examples of unit tests are shown in Supplemental Section C.4.2. Our DS Agent and the base Gemini 2.0 Flash model were assessed for their code generation capabilities using these 25 test suites. \nThe pass rate was determined by calculating the ratio of successful test cases (across all 173 tests in 25 suites) to the total number of test cases. If code execution fails, the model will receive error messages and retry up to 5 times. \nEvaluation Results. The code generation assessment in the right of Figure 3(b) reveals a clear advantage for the DS Agent, which achieved a 75.5+3.3% pass rate (i.e., error rate of 24.3%) on its first attempt, significantly outperforming the 58.4+3.7% pass rate (error rate of 41.6%) of the Base Gemini model $(\\chi^{2}=26.3,p<0.001$ ., McNemar's Test). This demonstrates a marked improvement in generating functionally correct code in a single shot. The agent's performance further improves to.79.0% after five trials, confirming an effective, albeit modest, capacity for iterative self-correction.This indicates that while the primary performance gain stems from the agent's superior initial code generation, the iterative process further offers a valuable mechanism for incremental refinement.This dual capability positions the agent as a reliable and practical tool for automated code generation tasks. \nMore specifically, the DS Agent substantially outperforms the Base Gemini model in reducing critical code generation errors. The most substantial improvement was observed in data handling errors, which dropped from 25.43.3% to $11.0{\\pm}2.4\\$ '. Since these errors typically result in unusable code, their reduction indicates improved robustness in data pipeline construction. Moreover,generations free of data handling issues were subsequently evaluated for general programming,calculation/logic, and output content errors. Across all three categories, the DS Agent consistently showed lower error rates, though the differences did not reach statistical significance $(\\mathbf{e.g.,3.9{pm}}1.5\\$ vs.$7.0{\\pm}2.2\\%$ for logic errors,$Z=1.15,\\;p=0.249)$ '. Full details can be found in Supplemental Table S7. \nThe DS Agent's superior performance validates the code generation and execution component of our agent architecture. By successfully translating a robust plan into a reliable numerical output, this capability completes the agent's end-to-end workflow with the two critical components, ensuring that. \nusers receive not only a well-reasoned analysis but also a statistically sound and trustworthy answer.",
    "Image_paths": [
      "D:\\My_Codebase\\Github\\agentic_retrieval_chat\\sample_docs\\pdf_output\\The_Anatomy_of_a_Personal_Health_Agent\\page_metadata\\meta_images\\img_in_image_box_277_745_2208_1506.jpg",
      "D:\\My_Codebase\\Github\\agentic_retrieval_chat\\sample_docs\\pdf_output\\The_Anatomy_of_a_Personal_Health_Agent\\page_metadata\\meta_images\\img_in_image_box_379_1626_2200_2283.jpg"
    ],
    "Image_captions": [
      "Figure 3 | Schematic and evaluation results of the Data Science (Ds) Agent. (a) The Architecture of the DS Agent, which first generates a detailed, natural language statistical analysis plan from a user's query and a data schema summary (see Supplemental Section C.3 for prompting details).Subsequently, it produces and executes code to implement this plan. An iterative self-correction loop allows the agent to revise the code based on execution errors, the original query, and the analysis plan. (b) Results of the Data Science Agent's performance evaluation, demonstrating significant improvements over the base Gemini model in both analysis plan quality and code generation pass rates. Supplemental Section C.4 summarizes evaluation details and rubrics."
    ],
    "Tables": [],
    "Table_captions": [],
    "Equations": {}
  },
  "Section 6": {
    "Text": "5. Domain Expert Agent\n \nThe increasing volume of personal health data (e.g., medical records, wearables data) and rapidly evolving research literature can overwhelm users. This is particularly true for those without clinical expertise, who may struggle to translate such granular data into actionable health insights or care plans. While LLMs demonstrate impressive capabilities of retrieving general knowledge, including in medicine (e.g., Saab et al. (2024)) and personal health (e.g., Khasentino et al. (2025)), their application in health domains, together with data-driven insights from the DE Agent, demands rigorous attention to information reliability and accuracy. A general-purpose LLM, lacking specific medical grounding on authoritative sources, may produce plausible-sounding but ultimately inaccurate or misleading information (Asgari et al., 2025), posing potential risks to users' wellness. \nWhile studies have investigated the medical knowledge of LLMs (e.g., Kim et al. (2025c); Singhal et al.(2023); Xu et al. (2024)), essential capabilities for an ideal LLM-based health domain expert remain underexplored, especially on the aspect of personalizing responses based on contextual information (such as demographics or pre-existing conditions), and reasoning over multimodal data from wearables and medical records to generate summaries and proactive health assessments. \nTo address these limitations, we designed the Domain Expert (DE) Agent, as a research construct,.to function as a specialized cognitive layer for health applications. Its purpose is not merely to retrieve information, but to synthesize, contextualize, and validate it against authoritative sources and patient-specific data. By acting as a focused and reliable source of medical expertise, the DE Agent aims to minimize misinformation risk and translate complex data into clear, actionable insights,serving as a critical foundation for our multi-agent design.'\n5.1. Domain Expert Agent Capabilities: \n \nTo fulfill this role, the DE Agent needs to move beyond the simple query-response behavior of standard.LLMs. It should be equipped with a set of distinct yet interconnected skills that mirror the reasoning of a human clinician. This involves not only accessing factual knowledge but also applying it within the context of an individual's specific health status and history. To formalize this multifaceted expertise,we define and evaluate the agent's core competencies across four foundational capabilities: \nDE1.Accuracy of Health Information: The DE Agent should provide accurate and reliable medical knowledge spanning diverse personal health and clinical domains, ensuring information is consistent with expert-level understanding. \nDE2.Diagnostic Reasoning Acumen: The agent can perform differential diagnostic reasoning by generating and ranking plausible diagnoses based on a user's symptoms and relevant medical history conveyed through natural conversations. \nDE3.Contextualization and Personalization Efficacy: The agent should contextualize and personalize health information and advice, tailoring outputs to a user's specific demographic profile, pre-existing conditions, and health goals to ensure relevance,clarity, and safety.. \nDE4.Multimodal Data Synthesis and Reasoning: The DE Agent should synthesize and reason over heterogeneous data sources, generating coherent and informative summaries from complex real-world inputs that integrate wearable data (e.g., simple statistics or (a) Overview of the Domain Expert (DE) Agent (b) End-User Evaluation of Model Responses for Single-Turn Q&A (c) Clinician Evaluation of Multimodal Health Summaries \ncomplex outcomes computed by the DS agent) with medical records (e.g., lab results and medical history).\n5.2. Domain Expert Agent Architecture: \n \nThe DE Agent employs a multi-step planning, reasoning, and acting architecture (Yao et al., 2023), as shown in Figure 4(a). The agent receives a user query, wearable data, and medical records as input.The agent processes the query and chooses actions from a set of tools that include querying: Web Search (Google, 2025), the National Center for Biotechnology Information (NCBI, 2025) application programming interface (API), and the DataCommons API (Data Commons, 2025). A Python Sandbox is also available for executing APIs. \nThe multi-step reasoning framework enables the agent to address complex queries by interleaving reasoning with tool use. For any given query, the agent follows a Reason-Investigate-Examine cycle: It.begins with a reasoning and thinking stage, where the agent decomposes a user's health query into a series of verifiable steps, such as establishing a clinical baseline for a vital sign, finding literature to connect symptoms, or planning to analyze trends in personal data. Based on the reasoning from the previous step, the agent chooses the most suitable investigations by selecting the appropriate tool to execute its plan--for example, querying DataCommons for population statistics or NCBI for medical studies. The agent then examines the results of the previous steps, which informs the next cycle. This iterative cycle continues until the agent has synthesized sufficient evidence to construct a comprehensive answer. \nThis dynamic, iterative process creates a transparent and auditable reasoning trace. By synthesizing facts from authoritative sources with the user's personal health data, the agent ensures its final answer is not a black-box output but is instead a grounded, evidence-based conclusion. This architecture is therefore essential for fulfilling the agent's core capabilities of providing accurate, personalized, and reliable health insights. Supplemental Section D.2 details the prompt setup for the DE Agent.\n5.3. Domain Expert Agent Evaluation: \n \nWe conducted a systematic evaluation process to evaluate our DE Agent's four capabilities in four independent tasks. \nEvaluation Goal. We evaluate our agent's ability to accurately answer nuanced medical and wellness.questions (DE1). The evaluation consists of a comprehensive suite of expert-level multiple-choice questions covering personal health and medical knowledge domains. \nEvaluation Setup. Guided by clinical relevance and our user study, we selected endocrinology,cardiology, fitness, and sleep medicine as the main domains to evaluate for this task. These domains map directly to real-world daily health needs, including both high-prevalence chronic conditions (endocrinology, cardiology) and high-interest preventative wellness topics (fitness, sleep). Furthermore,they provide a scientifically robust testing ground; their complexity requires deep reasoning over case vignettes, allowing us to assess performance beyond simple fact retrieval. while a broad medical knowledge base is essential, the agent's foundational language model has already demonstrated strong performance on general medical benchmarks (e.g., (Saab et al., 2024; Singhal et al., 2023). Our evaluation is therefore specifically designed to assess the DE agent's ability to apply this foundational knowledge in challenging, specialist-level domains that are of high interest to the public, rather than re-validating the base model's established capabilities. \nTo this end, using board certification and coaching exam questions provides a standardized and quantitative metric for evaluating the DE Agent's ability to use its medical knowledge to reason \nover case studies mirroring real-world scenarios. Specifically, we took four curated multiple choice question (MCQ) datasets totalling more than 2o00 test questions to assess knowledge and capabilities at answering expert-level health questions: \nEndocrinology Examination: Leveraging StatPearls' 'American Board of Internal Medicine:Endocrinology, Diabetes, & Metabolism Exam\" preparatory quizzes (StatPearls Publishing,2024), we selected 570 questions at random from all available levels of difficulty (173 \"Expert\",198 \"Difficult\", and 199 \"Moderate and Easy\" questions).. \nCardiology Examination: We created a list of 399 Cardiology Board Certification questions using BoardVital's ABIM-based preparatory question bank (BoardVitals, 2024). We randomly selected questions from all difficulty levels (100 \"Hard\", 199 \"Moderate\", and 100 \"Easy\".questions). \nFitness \"Coaching\" Certification Exam: Compilation of 542 questions by Khasentino et al.(2025) from multiple question banks that emulate exam content for the Certified Strength and Conditioning Specialists. \nSleep Medicine Examination: A curated dataset of 634 multiple choice questions from BoardVitals sleep medicine Maintenance of Certification board review question banks by Khasentino et al. (2025). \nWe compare our DE Agent against the same backbone model (Gemini 2.0 Pro), which acted as the baseline without the agentic framework.. \nEvaluation Results. The DE Agent's framework improves the base model's performance across all datasets and across almost all difficulty levels: the DE Agent achieves an overall accuracy of 83.6%while the base Gemini achieves 81.8%, showing a statistically significant improvement across all questions $(\\chi^{2}=9.506,p=0.002$ , McNemar's Test, Odds Ratio 1.667). The detailed performance.metrics and statistics are summarized in Supplemental Table S10.. \nEvaluation Goal. This task evaluates the agent's ability to perform differential diagnosis through conversational reasoning (DE2). Unlike prior work that often dealt with complex case studies (Kanjee et al., 2023; Liu et al., 2020d; McDuff et al., 2025), we focus on the agent's performance when presented with common symptoms that a user would typically self-report. The goal is to assess the quality and accuracy of the generated differential diagnosis based on these realistic scenarios. \nEvaluation Setup. We conducted a large retrospective observational study to collect health case self reports from consumers.$_((\\mathrm=N2,000$ , aged 18 - 65, IRB approved #GH-SCD-001). The collected data were de-identified and utilized to evaluate the performance of the DE Agent. Participants had experienced symptoms in one of 39 pre-defined categories (see Supplemental Section D.4.2 for the full list) within the 3 months. Participants completed an online survey detailing their prior health events, their symptom descriptions in natural language, any online information-seeking behavior they engaged in, and the ultimate diagnosis they received.. \nWe compared our DE Agent against a baseline that we re-implemented: a Gemini 2.0 Pro-based state-of-the-art DDx Agent (McDuff et al., 2025). To generate predictions, we input all self-reported symptom descriptions into each model and prompted for top-10 most likely diagnoses (refer to Supplemental Section D.2.4 for the exact prompts). To evaluated the performance, we leveraged.a separate Gemini 2.5 Pro as an autorater (prompt details in Supplemental Section D.2.4) that systematically compare the ground truth against the texts of predictions ranked by the models. \nEvaluation Results. Our comparative analysis focuses on the top-10 predictions. The results showed that the DE Agent significantly outperforms the base model on the diagnostic capabilities $\\left(p\\right)$ \n$0.001,r=0.156$ , Wilcoxon signed-rank test, Supplemental Table S11). The DE Agent achieves a top-1 accuracy of 46.1% (whereas the DDx Agent achieves a top-1 accuracy of 41.4%, a performance.advantage $\\Delta=-4.7\\%$ '. In top-5 accuracy, the DE Agent reached 75.6%.$(\\Delta=3.8\\%)$ ), and its top-10accuracy was $84.5\\$ , see Supplemental Figure S6). Our results showcase that the domain expert agent can accurately predict diagnoses from common self-reported symptoms, outperforming the state-of-the-art agent on differential diagnosis, underscoring its potential for enhanced utility in practical, user-facing health query and discussion scenarios. \nEvaluation Goal. Generic medical advice, while potentially accurate in isolation, can be inappropriate or even harmful if it fails to account for an individual's unique health profile. For instance, encouraging a vigorous exercise plan could be beneficial for most people but may pose significant risks to someone.with certain cardiovascular conditions. Therefore, a critical capability of a reliable health agent is its ability to tailor responses to user-specific queries, integrating contextual information to provide personalized, trustworthy, and actionable guidance. This evaluation assesses the DE Agent's ability to personalize responses to health queries involving key contextual information (DE3). \nEvaluation Setup. We curated a set of 50 health-related questions that are modified from the query set in Section 2.1 and designed to elicit personalized responses. To test the models' contextual reasoning, each query was augmented with specific personal information falling into two categories: \nDemographic Differences: Queries included details such as age, biological sex, and race/ethnicity, which critically influence health risks, disease presentation, and preventative care recommendations $(text{e.g.}$ , screening schedules, risk stratification).. \nExisting Health Conditions: Queries were framed within the context of prevalent chronic conditions, including diabetes, cardiovascular disease, and impaired kidney, thyroid, or liver.function. Providing safe and relevant advice in these scenarios requires the agent to reason about potential contraindications and comorbidities.. \nWe generated responses to these contextualized queries from both our DE Agent and the base Gemini 2.0 Pro model. To mitigate presentation bias, all responses were programmatically normalized to a standard format. A cohort of 17 end-users (aged 25 - 50) were recruited and evaluated the model responses in a randomized order where they were blind to the model condition. In total,.end-users spent 13 hours on the evaluation. Inspired by Mallinar et al. (2025), our evaluations used precise boolean rubric questions, as well as side-by-side comparisons to assess dimensions of.\"personalization\", \"relevance\", \"credibility\", and \"trustworthiness\". See Supplemental Table s8 for the specific evaluation rubric. \nEvaluation Results. Our evaluation revealed that while the base model could produce relevant infor-.mation, the DE Agent was substantially more effective at generating more personalized, trustworthy,and well-supported responses and guidance essential for health applications, as shown in Figure 4(b).In binary rubric assessments, end-users had good to excellent IRR $\\left(\\mathrm{F l e s s}^{\\prime}\\kappa\\geq0.684\\right)$ . DE Agent's received significantly higher Trustworthiness ratings.$(96.9{\\pm}0.8\\$ ', whereas the base Gemini model's responses achieved only $38.7{\\pm}3.3\\$ , Binomial generalized linear mixed model, GLMM). Menawhile, both models performed similarly on measures of Relevance-to-Query.rubric $(97.6{\\pm}0.7\\$ and $98.1{\\pm}0.6\\$ , respectively,$p=0.465,\\beta=-0.269)$ | and Groundedness-in-Data,95.$_6{\\pm}1.3\\%$ and $98.6{\\pm}0.9,p<0.001,\\beta=-1.605)$ , indicating that both models successfully identified.the core intent of the user's question and provided solid data-driven responses. Supplemental Table S12 presents more statistical details. These results highlight a critical failure of the generalpurpose model to generate content that users perceive as reliable for health decisions. \nSide-by-side comparisons further highlighted the DE Agent's ability to personalize responses compared to the base model. When asked to choose the better response, end-users preferred the DE Agent over the base model in $71.9{\\pm}1.5\\$ of cases in the Personalization-and-Contextualization dimension $\\left(p<0.001,\\beta=1.667\\right)$ '. Furthermore, the DE Agent also showed better performance at DefiningMedical-Terms, winning $60.4{\\pm}1.7\\$ of comparisons $\\left(p=0.057,\\beta=0.664\\right)$ ', and won $76.9{\\pm}1.5\\$ )$\\left(p<0.001,\\beta=1.969\\right)$ + of comparisons in Credibility-of-Citations rubric. See Supplemental Table S13for more detailed results. \nThese outcomes showed that the DE Agent's design and domain-specific tools enable it to synthesize personal context with verifiable medical knowledge to produce more nuanced, comprehensible, and trusted responses compared to the base model. \nEvaluation Goal. This task evaluates the agent's ability to synthesize and reason over diverse,multimodal personal health data (wearable data and medical records) to generate comprehensive health summaries (DE4). Unlike simple information retrieval, this task demands a higher level of cognitive synthesis that mimics a clinician's ability to connect disparate data points, identify trends,and highlight potential concerns. The objective is to produce a concise and actionable overview of an.individual's health status proactively, without a specific user query or chief complaint. \nEvaluation Setup. To evaluate our agent's capabilities for this task, we leverage the following data modalities from participants from the WEAR-ME data: \nUnstructured Text: For each participant, we combine all contextual and background information collected during onboarding survey as one string. \nStructured Tables: This included tabular data representing wearable-derived digital measures,such as resting heart rate, heart rate variability, etc.. \nLab Result Documents (PDFs): Given that most users will have access to an electronic document.of their lab tests, we convert real participants' lab results to PDF documents for input to the compared models. \nWe sampled 30 participants from our persona pools defined in Section 3.4 with an equal distribution across 10 health profiles. We then prompted our DE Agent and the baseline Gemini 2.0 Pro model to generate a comprehensive health summary for each participant based on their complete data profile. A panel of 5 expert clinicians (MDs, 2 females, 3 males, aged 30 - 60) then independently evaluated the quality of these generated summaries in a side-by-side, blinded comparison, with a total of 77 hours of human effort. The clinicians used a detailed rubric to assess dimensions such as clinical relevance,accuracy of interpretation, and the actionability of the summary (see Supplemental Table s9 for the full rubric).. \nEvaluation Results. Our results demonstrate that the DE Agent generates comprehensive, clinically relevant, and useful multimodal health summaries compared to base model. As shown in Figure 4(c), on binary rubric questions, the DE Agent showed significant improvement in ClinicalSignificance $(96.4{\\pm}0.6{\\%})\\mathrm{~v s.~}73.8{\\pm}0.6{\\%},\\mathrm{~p~}<0.001,\\beta\\mathrm{~=~}2.461$ , Binomial GLMM), Cross-ModalAssociation $(79.4{\\pm}16.9\\$ vs.$50.0{\\pm}15.0\\$ , Comprehensiveness $(96.4{\\pm}1.1\\$ I vs.$64.2{\\pm}11.8\\$ , and Citation-Credibility $(77.6{\\pm}13.8\\$ 9$p<0.001,\\beta=3.721)$ I. More statistical details are listed in Supplemental Table S14. In direct side-by-side comparisons, clinicians overwhelmingly preferred the summaries generated by the DE Agent over those from the base model. When asked for an overall preference on Comprehensiveness,$93.3{\\pm}1.9\\$  of evaluations favored the DE Agent over the base model.$\\left(p<0.001,\\beta=2.652\\right)$ . On Trustworthiness, the DE Agent was again strongly preferred, winning $82.4{\\pm}3.0\\$  of comparisons.$\\left(p<0.001,\\beta=3.721\\right.$ , Supplemental Table S15). These results underscore a clear preference for the.",
    "Image_paths": [
      "D:\\My_Codebase\\Github\\agentic_retrieval_chat\\sample_docs\\pdf_output\\The_Anatomy_of_a_Personal_Health_Agent\\page_metadata\\meta_images\\img_in_image_box_274_416_2182_1073.jpg",
      "D:\\My_Codebase\\Github\\agentic_retrieval_chat\\sample_docs\\pdf_output\\The_Anatomy_of_a_Personal_Health_Agent\\page_metadata\\meta_images\\img_in_chart_box_291_1186_948_1732.jpg",
      "D:\\My_Codebase\\Github\\agentic_retrieval_chat\\sample_docs\\pdf_output\\The_Anatomy_of_a_Personal_Health_Agent\\page_metadata\\meta_images\\img_in_chart_box_1039_1191_2194_1733.jpg",
      "D:\\My_Codebase\\Github\\agentic_retrieval_chat\\sample_docs\\pdf_output\\The_Anatomy_of_a_Personal_Health_Agent\\page_metadata\\meta_images\\img_in_chart_box_272_1853_1102_2678.jpg",
      "D:\\My_Codebase\\Github\\agentic_retrieval_chat\\sample_docs\\pdf_output\\The_Anatomy_of_a_Personal_Health_Agent\\page_metadata\\meta_images\\img_in_chart_box_1090_1854_2210_2701.jpg"
    ],
    "Image_captions": [
      "Figure 4 | Schematic and evaluation results of the Domain Expert (DE) Agent. (a) Overview of the DE Agent's workflow, which takes a user's query and personal health data as input. The agent employs.an iterative process of reasoning, investigation, and examination, using a toolbox with access to specialized resources (see Supplemental Section D.2 for prompting details). (b) End-user evaluation.(N=17) of the DE Agent significantly outperforms a base Gemini model in multimodal dimensions in.single-turn, contextualized Q&A. (c) Clinician evaluation $(N(=5)$ I of multimodal health summaries shows same conclusions. Supplemental Section D.3 summarizes evaluation details and rubrics."
    ],
    "Tables": [],
    "Table_captions": [],
    "Equations": {}
  },
  "Summary": "The paper introduces a Personal Health Agent (PHA) system that combines three specialized sub-agents to provide personalized health recommendations using data from wearable devices and medical records. The system was designed through user-centered research that identified four critical categories of user needs: general health knowledge, personal data insights, wellness advice, and personal medical symptoms. The PHA consists of a Data Science Agent that analyzes time-series health data, a Domain Expert Agent that integrates health data with medical knowledge, and a Health Coach Agent that provides interactive goal-setting and behavioral guidance. These sub-agents collaborate through an orchestrator to address complex user queries requiring multiple capabilities. The researchers conducted comprehensive evaluations across 10 benchmark tasks involving over 7,000 annotations and 1,100 hours of human effort from health experts and end-users. The Data Science Agent demonstrated significant improvements in generating robust analysis plans and accurate code execution compared to baseline models. The Domain Expert Agent outperformed baseline models in medical knowledge application, differential diagnosis, personalized responses, and multimodal health data synthesis. The Health Coach Agent provides evidence-based psychological strategies to help users set goals and track progress. The PHA system represents the most comprehensive evaluation of a health agent to date and establishes a foundation for accessible personal health agents addressing diverse individual health needs. This multi-agent framework enables dynamic, personalized interactions to support everyday health and wellness.",
  "ID": "c66d326d-ea30-51f9-9607-9dc2f92cbb37"
}
{
  "File_Name": "Assessing_Visually-Continuous_Corruption_Robustness_of_Neural_Networks.pdf",
  "Section 0": {
    "Text": " Boyue Caroline Hu University of Toronto.boyue@cs.toronto.edu Huakun Shen University of Toronto.huakunshen@cs.toronto.edu Lina Marsso University of Toronto.lina.marsso@utoronto.ca Krzysztof Czarnecki University of Waterloo.kczarnec@gsd.uwaterloo.ca Marsha Chechik University of Toronto.chechik@cs.toronto.edu",
    "Image_paths": [],
    "Image_captions": [],
    "Tables": [],
    "Table_captions": [],
    "Equations": {}
  },
  "Title": "Assessing Visually-Continuous Corruption Robustness of Neural Networks. Relative to Human Performance",
  "Section 1": {
    "Text": "Abstract\n Neural Networks (NNs) have surpassed human accuracy in image classification on ImageNet, yet they often lack robustness against image corruption, i.e., corruption robustness, with such robustness being seemingly effortless for human perception. In this paper, we propose visuallycontinuous corruption robustness (VCR) - an extension of corruption robustness to allow assessing it over the wide and continuous range of changes that correspond to the human perceptive quality (i.e., from the original image to the full distortion of all perceived visual information), along with two novel human-aware metrics for NN evaluation. To compare VCR of NNs with human perception, we conducted extensive experiments on 14 commonly used image corruptions with 7,718 human participants and state-of-the-art robust NN models with different training objectives (e.g., standard,adversarial, corruption robustness), different architectures (e.g., convolution NNs, vision transformers), and different amounts of training data augmentation. Our study showed that: 1) assessing robustness against continuous corruption can reveal insufficient robustness undetected by existing benchmarks; as a result, 2) the gap between NN and human robustness is larger than previously known; and finally, 3)some image corruptions have a similar impact on human.perception, offering opportunities for more cost-effective robustness assessments.",
    "Image_paths": [],
    "Image_captions": [],
    "Tables": [],
    "Table_captions": [],
    "Equations": {}
  },
  "Section 2": {
    "Text": "1. Introduction\n For Neural Networks (NNs) used in safety-critical domains, ensuring robustness against potential corruptions is crucial [15]. As NNs in these domains automate tasks usually performed by humans, comparing their robustness to human performance is essential. Human VS NN robustness. Corruption robustness measures the average-case performance of an NN or humans on a set of image corruption functions [15]. Existing studies,including out-of-distribution anomalies [16], benchmarking [15, 18], and comparison with humans [10, 20], generally evaluate robustness against a pre-selected, fixed set of transformation parameter values that represent varying degrees of image corruption. However, these parameters may not capture how different levels of corruption affect human perception. For instance, the same parameter can impact visual perception differently depending on the image's brightness [20]. Humans can perceive a continuous spectrum of visual corruptions, from subtle to extreme [9, 41], so relying on fixed parameters may lead to incomplete coverage of the full range of visual corruptions and biased evaluations of NN robustness compared with humans. Contributions and Outlook. To address these issues, we introduce visually-continuous corruption robustness (VCR),focusing on NN robustness across a continuous range of image corruption levels. We also present two novel humanaware metrics (HMRI and MRSI) for comparing NN performance with human perception. Our extensive experiments,involving 7,718 Mechanical Turk participants and 14 common image transformations from three sources ', reveal a significant robustness gap between NNs and humans. No NN fully matches human performance across the entire continuous range of corruption levels in terms of both accuracy and prediction consistency, and only a few exceed human performance by a small margin in specific levels of corruption. Our experiments yield insightful findings about the robustness of human and state-of-the-art (SoTA) NNs concerning accuracy,degrees of visual corruption, and consistency of classification, which can contribute towards the development of NNs that match or surpass human perception. We also discovered classes of corruption transformations for which humans showed similar robustness (e.g., different types of noise), while NNs reacted differently. Recognizing these classes can contribute to reducing the cost of measuring human robustness and elucidating the differences between humans and computational models. Our validation set with 14 image corruptions, human robustness data, and the evaluation code is provided as a toolbox and a benchmark2.",
    "Image_paths": [],
    "Image_captions": [],
    "Tables": [],
    "Table_captions": [],
    "Equations": {}
  },
  "Section 3": {
    "Text": "2. Related Work\n We briefly review related work on the comparison of human and NN robustness, adversarial robustness, robustness benchmarks and improving robustness. Human VS NN Robustness. Prior studies have used hu-.man performance to study the existing differences between humans and neural networks [6, 55], to study invariant transformations [23], to compare recognition accuracy [19, 44], to compare robustness against image transformations [9, 10], or to specify expected model behaviour [20]. The main difference between our study and existing work, specifically, the most recent study by [10], is three-fold: 1) we are the first to quantify robustness across the full continuous visual corruption range, thus revealing previous undetected robustness gap; 2) our experiments for obtaining human performance are designed to include more participants for measuring the average human robustness, resulting in more generalizable results and reduced influence of outliers; 3) we identified visually similar transformations for humans but not NNs,potentially reducing experiment costs. Robustness Benchmarks.  Hendrycks et al. built the IMAGENET-C and -P benchmarks for checking NN model classification robustness against common corruptions and perturbations on ImAGeNeT images [15]. They have inspired other benchmarks for different corruption functions,datasets, and tasks [2, 21, 22, 32, 33, 46, 51]. However,these benchmarks generate images by applying corruption functions with only five pre-selected values per parameter.IMAGENET-CCC [36] is the only prior work targeting a more continuous range of corruptions, by using 20 preselected values per parameter. It does not check the coverage in terms of the visual effects on the images, which we do with an Image Quality Assessment (IQA) metric Visual Information Fedility (VIF) [41]. Further, their work focuses on continuous changes over time for benchmarking test-time adaptation, which is different from a general robustness benchmark, and the dataset has not been released as the time of writing. In contrast to all these previous works, our method randomly and uniformly samples parameter values to cover the full range of visual change that a corruption function can achieve, which is modeled and assessed for coverage using an IQA metric. Our work also compares robustness of NNs with humans. Adversarial Robustness. Adversarial robustness measures.the worst-case performance on images with added 'small' distortions or perturbations tailored to confuse a classifier [15]. However, changes that can be encountered in the real-world situations are often of a much bigger range [22].Thus, in this paper, we focus on average-case performance over a realistic range of changes. Improving Robustness. Numerous methods for improving model robustness have been proposed, e.g., data augmenta-.tion with corrupted data [8, 30, 31, 38], texture changes [11,14], image compositions [53, 54] and corruption functions [17, 52]. All of these have different abilities to generalize to unseen data [22]. While not our primary focus, we demonstrate that NN robustness compared to humans can be improved through data augmentation and fine-tuning with our generated images for VCR.",
    "Image_paths": [],
    "Image_captions": [],
    "Tables": [],
    "Table_captions": [],
    "Equations": {}
  },
  "Section 4": {
    "Text": "3. Visually-Continuous Corruption Robustness\n To study robustness against a wide and continuous spectrum of visual changes, we define visually-continuous cor-.ruption robustness (VCR) and describe our method for generating test sets. To study VCR of NNs in relation to humans,we also present the human-aware metrics.\n3.1. VCR Definition: \n A key difference between corruption robustness and VCR is that the latter is defined relative to the visual impact of image corruption on human perception, rather than the transformation parameter domain. To quantify visual corruption, VCR uses the Image Quality Assessment (IQA) met-.ric Visual Information Fidelity (VIF) [28, 41]. VIF measures the perceived quality of a corrupted image $x^{\\prime}$ compared to its original form x by measuring the visual information unaffected by the corruption. Thus, we define the change in the perceived quality caused by the corruption as $\\Delta_{v}(x,x^{\\prime})=m a x\\big(0,1-V I F(x,x^{\\prime})\\big)$ I. See App. C for more detail on $\\Delta_{v}$ . With $\\Delta_{v},$ . whose value ranges from 0 and 1, we can consider VCR against the wide, finite, and continuous spectrum of visual corruptions ranging from no degradation to visual quality (i.e., the original image)$(\\Delta_{v}=0)$ ' to the full distortion of all visual information $(\\Delta_{v}=1)$ Limitation: VCR is limited to image corruption applicable to the chosen IQA metric, thus by using VIF, VCR is limited to only pixel-level corruption. Metrics suitable for other types of corruption (e.g., geometric) need further research. For VCR, we consider a classifier NN $f\\;:\\;X\\;\\to\\;Y$ trained on samples of a distribution of input images $P_{X}$ 9a ground-truth labeling function $f^{*}$ , and a parameterized image corruption function $T_{X}$  with a parameter domain C.We are interested in robustness of $f$  against images with all degrees of visual corruption uniformly ranging from $\\Delta_{v}=0$ to $\\Delta_{v}=1.^{3}$ U $\\Delta_{v}=1.^{3}$ Given a value $v\\in[0,1]$ ., we define $P(x,x^{\\prime}|v)$ as the joint distribution of original images (x) and corresponding corrupted images $(x^{\\prime}=T_{X}(x,c),c\\in C)$ with $\\Delta_{v}(x,x^{\\prime})=v$ . We define VCR in the presence of a robustness property  that f should satisfy given $T_{X}$ : [EQ_0] In this paper, we instantiate VCR with two existing robustness properties (see Fig. 1). The first one is accuracy $(a_{v})$ requiring that the prediction on corrupted images should be correct, i.e.,$f(x^{\\prime})=f^{*}(x)$ . It is also used in the existing definition of corruption robustness [15]. Thus, [EQ_1] The second property is prediction consistency $(p_{v})$ , requiring consistent predictions before and after corruptions, i.e.,$f(x^{\\prime})=f(x)$ [20]. It is applicable when ground truth is not.available, which is common during deployment. Thus, [EQ_2] Summary of VCR Definitions. Fig. 1 gives a visual summary of the VCR metrics, starting with the general definition $\\mathcal{R}_{\\gamma}$ : at the top, and instantiating it for accuracy as $\\mathcal{R}_{o}$  and consistency as $\\mathcal{R}_{p}$ . Each of them is the average accuracy or prediction consistency, respectively, over the full and continuous range of visual change $\\Delta_{v}\\in[0,1]$\n3.2. Testing VCR: \n VCR of a subject (a human or an NN) is measured by first generating a test set through sampling and then estimating it using the sampled data. The test set is generated by sampling images and applying corruption to obtain $P(x,x^{\\prime}|v)$ ' for different $\\Delta_{v}$ values v. We sample $x\\sim P_{X}$ and $c\\sim U n i f o r m(C)$ , and obtain $x^{\\prime}=T_{X}(x,c)$ and $v=\\Delta_{v}(x,x^{\\prime})$ , resulting in samples $(x,x^{\\prime},c,v)$ . Then, we divide them into groups of $(x,x^{\\prime},c)$ ', each with the same v value. Next, by dropping c, we obtain groups of $(x,x^{\\prime})$ with the same v, which are samples from $P(x,x^{\\prime}|v)$ I. Note that we consider each group separately, thus this procedure requires only sufficient data in each group but not uniformity, i.e.,$v\\sim\\mathit{U n i f o r m}(0,1)$ is not required. The varying size of each group, i.e., the non-uniformity of v distribution,will not distort VCR estimates, but only impact the estimate uncertainty at a given v. Further, interpolation in the next step helps address any missing points. With the test set, we estimate the performance w.r.t. the property $\\gamma$ for each v. For each v in the test data, we compute the rate of accurate predictions $f(x^{\\prime})=f^{*}(x)$ to estimate accuracy, i.e.,$a_{v}=P_{x,x^{\\prime}\\sim P(x,x^{\\prime}|v)}(f(x^{\\prime})=f^{*}(x))$ [resp.consistent predictions $f(x^{\\prime})\\doteq f(x)$ to estimate consistency,i.e.,$p_{v}=_x{,}p{{}scriptstyle};}&&{{{}}}P_{x{,}x{^}\\ sim{}P(x,{}^\\\\rfloor{}|}(({{}}{}{}{}{\\ }{{}}){\\ {}}{}{{}}{=}{}{}(x{}{)}{\\phantom{}}){}$ . Then by plotting $(v,a_{v})$ and $(v,p_{v})$ and applying monotonic smoothing splines [25] to reduce randomness and outliers, we obtain smoothed spline curves $s_{a}$  and $s_{p}.$ , respectively. The curves $s_{\\gamma}$ .  (namely,$s_{a}$ and $s_{p})$ : describe how the performance w.r.t. the robustness property y (namely, a and $p)$ decreases as the visual corruption in images increases.$\\mathrm{F i}.$ nally, we estimate $\\mathcal{R}_{a}=\\mathbb{E}_{v\\sim U n\\mathit{i f o r m}(0,I}}(a_{v})$ [resp.$\\mathcal{R}_{p}=$ $\\mathbb{E}_{v\\sim\\mathit{U n i f o r m}(\\boldsymbol,\\boldsymbol I{I})}(p_{v})]$ as the area under the spline curve, i.e.,. $\\begin{array}{r}{\\hat{\\mathcal{R}}_{a}=A_{a}=\\int_{0}^{1}s_{a}(v)d v\\;[\\mathrm{r e s p.~}\\hat{\\mathcal{R}}_{p}=A_{p}=\\int_{0}^{1}s_{p}(v)d v]}\\\\ \\end{array}$ See Alg. 1 for the pseudo-code of VCR estimation.\n3.3. Human-Aware Metrics for VCR: \n A commonly used metric for measuring corruption robustness is the Corruption Error (CE) [15]the top-1 classification error rate on the corrupted images, normalized by the error rate of a baseline model. CE can be used to compare an NN with humans if the baseline model is set to be humans. However, CE is not able to determine whether an NN can exceed humans, and NN models could potentially have super-human accuracy for particular types of perturbations or in some $\\Delta_{v}$ , ranges. Inspired by CE, we propose two new human-aware metrics, Human-Relative Model Robustness Index (HMRI) that measures NN VCR relative to human $\\mathrm{V C R};$ : and Model Robustness Superiority Index (MRsI) that measures how much an NN exceeds human VCR. Auxiliary VCR metrics to compute HMRI and MRSI.HMRI and MRSI take as inputs the estimated spline curves for humans $(s_{\\gamma}^{h})$ and for NN $(s_{\\gamma}^{m})$ . We denote areas under these curves as .$A_{\\infty}^{h}$  and $A_{\\gamma}^{m}$ , respectively (see Fig. 2a). To compare NN model and human performance, VCR w.r.t.prediction consistency or accuracy is estimated using $\\mathrm{A l g},1$ using both model and human performance data, as illustrated by the yellow $(A_{\\gamma}^{h})$ ' and blue $(A_{\\gamma}^{m})$ + areas in Fig. 2a, respectively. Both the blue and yellow areas also include the green area representing their overlap. Additionally, the VCR lead of humans over a model $A_{\\gamma}^{h>m}$ , the girded area in Fig. 2a,and the VCR lead of a model over humans.$A_{\\gamma}^{m>h}$ ', the striped area in Fig. 2a, are estimated. The definitions of these four auxiliary metrics are summarized in Tab. 2b, and they are used to define HMRI and MRSI. Definition 1 (HMRI). Given $s_{\\gamma}^{h}$  and $s_{\\gamma}^{m}$ , let $A_{\\gamma}^{h>m}==$ $\\textstyle\\int_{0}^{1}(s_{\\gamma}^{h}(v)-s_{\\gamma}^{m}(v))^{+}d v$  denote the average (accuracy $o r$ preservation) performance lead of humans over a model across the visual change range, where the performance lead is defined as the positive part of performance difference,$i.e.$ $.e.,\\ s_{\\gamma}^{h}(v)-s_{\\gamma}^{m}(v))^{+}=m a x(0,s_{\\gamma}^{h}(v)-s_{\\gamma}^{m}(v))$ I. HumanRelative Model Robustness Index (HMRI), which quantifies the extent to which a DNN can replicate human performance,is defined as $\\begin{array}{r}{\\frac{A_{\\gamma}^{h}-A_{\\gamma}^{h>m}}{A_{\\gamma}^{h}}=1-\\frac{\\dot{A}_{\\gamma}^{h>m}}{A_{\\gamma}^{h}}}\\\\ \\end{array}$ The HMRI value ranges from $[0,1];a$ t higher HMRI indicates a NN model closer to human VCR, and $\\mathit{H M R I}=1$ . signifies that $s_{\\gamma}^{m}$  is the same as or completely above $s_{\\gamma}^{h}$ in the entire $\\Delta_{i}$ , domain, i.e., the NN is at least as robust as an average human (see Fig. 2a). Definition 2 (MRsr). Given $s_{\\gamma}^{h}$  and $s_{\\gamma}^{m}$ , let $A_{\\gamma}^{m>h}~=$ $\\textstyle\\int_{0}^{1}\\ s(\\ s_{\\gamma}^{m}(v)-s_{\\gamma}^{h}(v))^{+}$  dv denote the average performance.lead of a model over a human across the visual change Visually-Continuous Corruption Robustness (VCR) R range. Model Robustness Superiority Index (MRsI), which quantifies the extent to which a DNN model can surpass human performance, is defined as $\\frac{A_{\\gamma}^{m>h}}{A_{\\gamma}^{m}}$ The MsRI value ranges from [0, 1), with the higher value indicating better performance than humans.$\\mathit{M S R I}=0$ 1means that the given NN model performs worse than or.equal to humans in the entire $\\Delta_{}$ , domain. A positive MSRI value indicates that the given NN model performs better than humans at least in some ranges of $\\Delta_{v}$ (see Fig. 2a).Comparing humans and NNs with HMRI and MRSI yields three possible scenarios: (1) humans' performance fully exceeds $\\mathbf{N N s^s,\\ i i.e.,0<H M R I<1}$ and $Mmathit{R S I}=0;(2)$ $\\mathrm{N N s}}$  performance fully exceeds humans', i.e.,$\\mathit{H M R I}=1$ and $\\mathit{M R S I}>0$ ; and (3) humans' performance is better.than $\\mathrm{N N s^ $ in some $\\Delta_{v}$ . intervals and worse in others, i.e., $\\mathit{H M R I}<1\\;\\mathrm{a n d}\\;\\mathit{M R S I}>0$",
    "Image_paths": [],
    "Image_captions": [
      "Figure 1. Summary of VCR definitions with respect to accuracy and consistency."
    ],
    "Tables": [
      [
        {
          "0": "of a model f(x) w.r.t. a property $\\boxed{\\mathcal{R}_{\\gamma}=\\mathbb{E}_{v\\sim\\mathit{U n i f o r m}(0,I)}(P_{x,x^{\\prime}\\sim P(x,x^{\\prime}|v)}(\\gamma))}$",
          "1": "of a model f(x) w.r.t. a property $\\boxed{\\mathcal{R}_{\\gamma}=\\mathbb{E}_{v\\sim\\mathit{U n i f o r m}(0,I)}(P_{x,x^{\\prime}\\sim P(x,x^{\\prime}|v)}(\\gamma))}$",
          "2": "of a model f(x) w.r.t. a property $\\boxed{\\mathcal{R}_{\\gamma}=\\mathbb{E}_{v\\sim\\mathit{U n i f o r m}(0,I)}(P_{x,x^{\\prime}\\sim P(x,x^{\\prime}|v)}(\\gamma))}$",
          "3": "of a model f(x) w.r.t. a property $\\boxed{\\mathcal{R}_{\\gamma}=\\mathbb{E}_{v\\sim\\mathit{U n i f o r m}(0,I)}(P_{x,x^{\\prime}\\sim P(x,x^{\\prime}|v)}(\\gamma))}$",
          "4": "",
          "5": "",
          "6": "",
          "7": "",
          "8": ""
        },
        {
          "0": "Prediction accuracy ay $a_{v}=P_{x,x^{\\prime}\\sim P(x,x^{\\prime}|v)}(f(x^{\\prime})=f^{*}(x))$ $\\mathrm{w i t h\\;v i s u a l\\;c h a n g e\\;\\}v{}=\\varDelta_{v}(x,x^{\\prime}){:}$ $\\mathrm{o n\\;t r a n s f o r m e d\\;i m a g e s\\;}x^{\\prime}=T_{X}(x,c)$",
          "1": "Prediction accuracy ay $a_{v}=P_{x,x^{\\prime}\\sim P(x,x^{\\prime}|v)}(f(x^{\\prime})=f^{*}(x))$ $\\mathrm{w i t h\\;v i s u a l\\;c h a n g e\\;\\}v{}=\\varDelta_{v}(x,x^{\\prime}){:}$ $\\mathrm{o n\\;t r a n s f o r m e d\\;i m a g e s\\;}x^{\\prime}=T_{X}(x,c)$",
          "2": "Prediction accuracy ay $a_{v}=P_{x,x^{\\prime}\\sim P(x,x^{\\prime}|v)}(f(x^{\\prime})=f^{*}(x))$ $\\mathrm{w i t h\\;v i s u a l\\;c h a n g e\\;\\}v{}=\\varDelta_{v}(x,x^{\\prime}){:}$ $\\mathrm{o n\\;t r a n s f o r m e d\\;i m a g e s\\;}x^{\\prime}=T_{X}(x,c)$",
          "3": "$\\overbrace{\\quad\\ \\ ;h h i c e\\;o f\\;p r o p e r t y\\;\\gamma\\;\\ }{{\\}}$",
          "4": "$\\overbrace{\\quad\\ \\ ;h h i c e\\;o f\\;p r o p e r t y\\;\\gamma\\;\\ }{{\\}}$",
          "5": "Prediction consistency Pv on transformed images $x^{\\prime}=T_{X}(x,c)$ $\\ \\ \\ \\ i t h\\;\\ s\\ s a l\\;c h a n g e\\;v=\\varDelta_{v}(x,x^{\\prime}){ $",
          "6": "",
          "7": "",
          "8": ""
        },
        {
          "0": "Prediction accuracy ay $a_{v}=P_{x,x^{\\prime}\\sim P(x,x^{\\prime}|v)}(f(x^{\\prime})=f^{*}(x))$ $\\mathrm{w i t h\\;v i s u a l\\;c h a n g e\\;\\}v{}=\\varDelta_{v}(x,x^{\\prime}){:}$ $\\mathrm{o n\\;t r a n s f o r m e d\\;i m a g e s\\;}x^{\\prime}=T_{X}(x,c)$",
          "1": "Prediction accuracy ay $a_{v}=P_{x,x^{\\prime}\\sim P(x,x^{\\prime}|v)}(f(x^{\\prime})=f^{*}(x))$ $\\mathrm{w i t h\\;v i s u a l\\;c h a n g e\\;\\}v{}=\\varDelta_{v}(x,x^{\\prime}){:}$ $\\mathrm{o n\\;t r a n s f o r m e d\\;i m a g e s\\;}x^{\\prime}=T_{X}(x,c)$",
          "2": "Prediction accuracy ay $a_{v}=P_{x,x^{\\prime}\\sim P(x,x^{\\prime}|v)}(f(x^{\\prime})=f^{*}(x))$ $\\mathrm{w i t h\\;v i s u a l\\;c h a n g e\\;\\}v{}=\\varDelta_{v}(x,x^{\\prime}){:}$ $\\mathrm{o n\\;t r a n s f o r m e d\\;i m a g e s\\;}x^{\\prime}=T_{X}(x,c)$",
          "3": "(correct prediction) (consistent prediction) $f(x^{\\prime})=f(x)$ $f(x^{\\prime})=f^{*}(x).$",
          "4": "(correct prediction) (consistent prediction) $f(x^{\\prime})=f(x)$ $f(x^{\\prime})=f^{*}(x).$",
          "5": "Prediction consistency Pv on transformed images $x^{\\prime}=T_{X}(x,c)$ $\\ \\ \\ \\ i t h\\;\\ s\\ s a l\\;c h a n g e\\;v=\\varDelta_{v}(x,x^{\\prime}){ $",
          "6": "$p_{v}=P_{x,x^{\\prime}\\sim P(x,x^{\\prime}|v)}(f(x^{\\prime})=f(x))$",
          "7": "$p_{v}=P_{x,x^{\\prime}\\sim P(x,x^{\\prime}|v)}(f(x^{\\prime})=f(x))$",
          "8": "$p_{v}=P_{x,x^{\\prime}\\sim P(x,x^{\\prime}|v)}(f(x^{\\prime})=f(x))$"
        },
        {
          "0": "Defined with",
          "1": "VCR wrt. accuracy",
          "2": "VCR wrt. accuracy",
          "3": "VCR wrt. accuracy",
          "4": "VCR wrt. consistency Rp:",
          "5": "",
          "6": "",
          "7": "Defined with",
          "8": ""
        },
        {
          "0": "",
          "1": "i.e., average accuracy over the full andi.e., average consistency over the full and continuous range of change v $\\mathcal{R}_{a}=\\mathbb{E}_{v\\sim U n\\mathit{i f o r m}(0,I)}(a_{v})$ $\\mathcal{R}_{a}:$",
          "2": "i.e., average accuracy over the full andi.e., average consistency over the full and continuous range of change v $\\mathcal{R}_{a}=\\mathbb{E}_{v\\sim U n\\mathit{i f o r m}(0,I)}(a_{v})$ $\\mathcal{R}_{a}:$",
          "3": "i.e., average accuracy over the full andi.e., average consistency over the full and continuous range of change v $\\mathcal{R}_{a}=\\mathbb{E}_{v\\sim U n\\mathit{i f o r m}(0,I)}(a_{v})$ $\\mathcal{R}_{a}:$",
          "4": "continuous range of change v $\\mathcal{R}_{p}=\\mathbb{E}_{v\\sim U n\\mathit{i f o r m}(0,I)}(p_{v})$",
          "5": "continuous range of change v $\\mathcal{R}_{p}=\\mathbb{E}_{v\\sim U n\\mathit{i f o r m}(0,I)}(p_{v})$",
          "6": "continuous range of change v $\\mathcal{R}_{p}=\\mathbb{E}_{v\\sim U n\\mathit{i f o r m}(0,I)}(p_{v})$",
          "7": "",
          "8": ""
        }
      ]
    ],
    "Table_captions": [],
    "Equations": {
      "[EQ_0]": "\\mathcal{R}_{\\gamma}=\\mathbb{E}_{v\\sim\\mathit{U n i f o r m}(0,I)}(P_{x,x^{\\prime}\\sim P(x,x^{\\prime}|v)}(\\gamma))",
      "[EQ_1]": "\\mathcal{R}_{a}=\\mathbb{E}_{v\\sim\\mathit{U n i f o r m}(\\theta,I)}(P_{x,x^{\\prime}\\sim P(x,x^{\\prime}|v)}(f(x^{\\prime})=f^{*}(x)))",
      "[EQ_2]": "\\mathcal{R}_{p}=\\mathbb{E}_{v\\sim U n i f o o m(0,I)}(P_{x,x^{\\prime}\\sim P(x,x^{\\prime}|v)}(f(x^{\\prime})=f(x)))"
    }
  },
  "Section 5": {
    "Text": "4. Experiments\n In this section, we describe experiments that check the VCR of NN models against human performance. NN models. Tab. 1 summarizes models included in our study..We have selected a wide range of architectures (CNN and transformer architectures) and training methods (supervised,adversarial, semi-weakly, and self-supervised), including dinov2_giant [34], which is on the top of the IMAGENET-C leaderboard as of time of writing. In total, we studied 11standard supervised models, 4 adversarial learning models,$2\\;W W S\\ L$ : models, 1 CLIP (clip-vit-base-patch32) model and 3 ViT models. For CLIP, we used the prompt \"a picture of (ImageNet class)\" while tokenizing the labels.. Image Corruptions. As shown in Fig. 3, we focus on studying VCR of NNs in relation to humans regarding 14 commonly used image corruptions from three different sources:Shot Noise, Impulse Noise, Gaussian Noise, Glass Blur,Gaussian Blur, Defocus Blur, Motion Blur, Brightness and Frost from IMAGENeT-C [15]; Blur, Median Blur, Hue Saturation Value and Color Jitter from Albumentations [1]; and Uniform Noise from [9]. Crowdsourcing. Since VCR focuses on the average-case performance, we used crowdsourcing to measure human performance, as it allows for a larger participant pool and more accurate estimation. The experiment is designed following [20] and [9]. The experiment procedure is a forcedchoice image categorization task: humans were shown one image at a time for 200 ms to limit recurrent processing and asked to choose the correct category from 16 entry-level labels [9]. For NN models, the 1,000-class decision vector was mapped to the same 16 classes using the WordNet hierarchy [9]. The time to classify each image was set to ensure fairness in the comparison between humans and machines [6]. Between images, we showed a noise mask to minimize feedback influence in the brain [9]. Qualification tests and sanity checks were used to filter out misunder-.standings and instances of spam [35], resulting in 7,718participants with 70,000 predictions on corrupted images and 50,000 on original images. The same image was never shown to a participant more than once.\n4.1. Testing Robustness against Visual Corruption: \n IMAGENET-C is a SoTA benchmark for corruption robustness, using 5 pre-selected parameter values for each corruption type on IMAGENeT validation images [15]. This section compares robustness results from IMAGENET-C with.those from VCR across all 9 IMAGENET-C corruption functions in our study. Full results are available in the Appendix due to page limitations. Visual Corruption in Test Sets. For each corruption, our generated tests contain 50, 000 images, mirroring the size of the IMAGENET [39] validation set, while IMAGENET-C includes 5  50, 000 images. Due to differences in test set generation, the corruption distributions differ in coverage and peak at different values (e.g., Fig. 4). To assess the coverage of $\\Delta_{v}$ , in the test sets, Tab. 2 shows the percentage of the full $\\Delta,$  covered. The distribution is divided into 40 equal-width bins, with coverage defined as having 20 or more images per bin. IMAGENET-C exhibits low $\\Delta_{v}$ coverage, particularly for Gaussian blur at 56.4%,focusing mainly on the center and missing the low and high $\\Delta_{v}$ . values, leading to biased evaluation ( Fig. 4 and Tab. 2.In contrast, our test sets cover nearly the entire domain,with 97.4% coverage. Our test sets have consistently higher coverage than ImAGENET-Cfor other corruptions as well.Full details are in the Appendix. Among all corruptions studied, Shot Noise and Impulse Noise have relatively low coverage, because the level of noise these functions add is exponential to their parameters.As a result, uniform sampling of the parameter range C fails to cover small $\\Delta_{v}$ . values. When using uniform sampling. (a) Visualization of auxiliary metrics for model vs. human performance. (b) Summary of auxiliary metrics for defining HMRI and MRSI. over C, reaching the full coverage of $\\Delta_{v}$ , would require a large amount of data. Note, however,$\\mathrm{A l g}$ .1 still computes VCR over the full $\\Delta_{v}$ ... range of [0..1], and the lack of samples for low values of $\\Delta_{i}$ , has a limited impact on the VCR estimate. This is because we fit a monotonic spline that is anchored with a known initial performance for $\\Delta_{v}=0$ , as discussed in App. D.. Remark: The reported accuracy of ImAGENET-C can be directly impacted both by a lack of coverage and by nonuniformity, as it is computed as the average accuracy of all corrupted images. In contrast, the shape of the $\\Delta,$  distribution in the test images does not impact VCR once sufficient coverage is achieved to estimate the spline curves $s_{\\gamma}$ Robustness Evaluation Results. Next, we compare robustness evaluation results obtained with IMAGENET-C and VCR test sets. Consider results for Gaussian Noise in Fig. 5.NoisyM1x and NoisyM1x_NEw have almost the same robust accuracy on IMAGENET-C, but NoISYM1X_NEw has higher $\\hat{\\mathcal{R}}_{a}.$ ; similarly, SIN has higher ImAGENET-C robust. accuracy but lower $\\hat{\\mathcal{R}}_{a}$  than SIN IN IN due to the almost complete lack of coverage for $\\Delta_{v}<0.5$  for Gaussian Noise in ImAGENET-C (see Tab. 2), which can lead to biased evaluation results (i.e., biased towards $\\Delta_{v}\\geq0.5)$ I. Checking VCR allows us to detect such biases.. In addition to accuracy, VCR can also check whether the NN can preserve its predictions after corruption, i.e.,the prediction consistency property $p_{v}$ , giving additional information about NN robustness. Fig. 5b and Fig. 5c show that the model T1AN_DeIT.B has a higher $\\hat{\\mathcal{N}}_{o}$  than SINGHCONVNEXT-L-CONVSTEM but a lower $\\hat{\\mathcal{R}}_{p}$ ,. This suggests that even though T1AN_DeIT_B has better accuracy for corrupted images, it labels the same image with different labels before and after the corruption. Since ground truth can be hard to obtain during deployment, having low prediction consistency indicates issues with model stability and could raise concerns about when to trust the model prediction. Results for the remaining corruptions are in App. E.. Summary: Robustness must be tested before deploying NNs into an environment with a wide and continuous range of visual corruptions. Our results confirmed that testing robustness in this range using a fixed and pre-selected number of parameter values can lead to undetected robustness issues, which can be avoided by checking VCR. Also, accuracy cannot accurately represent model stability when facing corruptions, which can be addressed by testing $\\mathcal{R}_{p}.$\n4.2. VCR of DNNs Compared with Humans: \n In this experiment, we use HMRI and MRSI metrics and the data from the human experiment data to compare VCR of the studied models against human performance. For Gaussian Noise, Fig. 6 shows our measured HMRI and MRSI values for $\\mathcal{R}_{a}$ and $\\mathcal{R}_{p}.$ , where higher values indicate better robustness. Fig. 6a reveals that no NN achieves 1.0 for $H M R I_{a}$ , and in Fig. 6d, only 3 out of 21 NNs DINOV2_GIANT, TIAN_DEIT-B and SINGH-CONVNEXT-LCoNvSTEm reached 1.0 for $Hmathit{M M R I}_{p}$ , indicating that there are still unclosed gaps between human and NN robustness.These three top-performing models have also the highest HMRI values for both $\\mathcal{R}_{a}$  and $\\mathcal{R}_{p}.$ , making them closest to human robustness. In Fig. 6b, these three models have $M R S I_{a}$ values above O.0, indicating that they surpass human accuracy in certain ranges of visual corruption. This can be visualized by checking the estimated curves $s_{a}$  as shown in Fig. 6c. The top-three models exceed human accuracy (the red curve) when $\\Delta_{v}>0.85$ . For prediction consistency,Fig. 6e shows that all NNs have the.$M R S I_{p}$ , value above 0.0and this is because, as shown in Fig. 6f, all NN curves are above the human curve when the $\\Delta_{r}$ , value is small. Similarly, for Uniform Noise, as shown in Fig. 7a and Fig. 7d, no models reached 1.0 for $Hmathit{M M R I}_{a}$ , and the top-three models, reached 1.0 for $Hmathit{M M R I}_{p}$ . Together with Fig. 7b and Fig. 7e, we can see that for both $\\mathcal{R}_{a}$ and $\\mathcal{R}_{p},$ , TIAN_DEIT-B has higher HMRI values but T1AN_DeIT-S has higher MRSI values. This suggests that while T1AN_DeIT-B is closer to human performance, T1AN_De1T-S exceeds human performance more. This counter-intuitive result can be explained with the curves $s_{a}$  and $s_{p}$  . representing how the performance w.r.t. the robustness properties a and p decreases as $\\Delta_{v}$ , increases, as shown in Fig. 7c and $\\mathrm{F i g}$ . 7f. Based on $s_{a}$  and $s_{p}$ , TIAN_DeIT-B performs better than TIAN_DeIT-S when $\\Delta_{v}<0.8$ , resulting in a higher HMRI. However it performs worse and drops more rapidly when $\\Delta_{v}>0.8.$ , leading to a lower MRSI. This suggests that both HMRI and MRSI are useful for comparing NN robustness, and our curves $s_{a}$  and $s_{p}$ , can provide further information on NN robustness with different degrees of visual corruption. Summary: When considering the full range of visuallycontinuous corruption, no NNs can match human accuracy,especially for blur corruptions, though some can match human prediction consistency. Few NNs can outperform humans in specific degrees of corruption. This highlights a more substantial gap between human and NN robustness than previously identified by [10]. By evaluating VCR using our human-centric metrics, we can better understand the robustness gap and work towards models closer to human performance.\n4.3. Training with Data Augmentation: \n VCR considers a different distribution of corruptions in the images (i.e., continuous) than existing benchmarks (i.e.,selected parameter values), so model performance is expected to improve once the model is fine-tuned on the new distribution. We show a small retraining example to demonstrate the usefulness of our benchmark in improving VCR. The image classification model was fine-tuned with parameters optimized using stochastic gradient descent (learning rate=0.001, momentum=0.9) and Cross-Entropy Loss.The training set was generated from a subset sampled from the IMAGENET [39] training set with a size of around 12,000,and typically five epochs were sufficient to see progress.While state-of-the-art NNs are optimized for the corruption functions in IMAGENET-C, for certain corruption functions,such as Motion Blur, Frost and Glass Blur, ImAGeNeT-C images do not cover a wide range of visual changes, leaving room for robustness improvement, as detailed in Tab. 2. Results for SIN [11] and Standard.R50 [3] are shown in Tab. 3;addittional details can be found in the codebase?. Summary: Our results indicate that retraining with VCRgenerated images improves all metrics of NN model performance compared to human performance, even for models optimized for IMAGENET-C's corruption functions. VCR introduces a new distribution of corruptions that models weren't previously exposed to, highlighting that the gap between human and NN robustness is larger than benchmarks like IMAGENET-C detect. VCR not only identifies this gap but also helps to bridge it. Note: all numbers are rounded. Table 3. VCR comparison before and after retraining. Red indicates improvement..\n4.4. Visually Similar Corruption Functions: \n Our experiments revealed the existence of visually similar.corruption functions, which can potentially reduce experi-.ment costs and enhance our understanding of differences between humans and NNs. Different corruptions affect aspects like color, contrast, and noise, influencing human perception in varied ways [9]. For example, Gaussian and Impulse noise.may have similar visual effects, making them hard for an average human to distinguish. We call such functions visually similar. We postulate that since visually similar functions,. by definition, affect human perception similarly, they would affect human robustness similarly as well. This suggests that.human data for one function can be reused for other similar.functions, potentially lowering experiment costs. Since VCR is estimated with the spline curves $s_{a}^{h}$ and $s_{p}^{h}$ if the difference among the curves of a set of functions is statistically insignificant, human data (i.e., the spline curves)can be reused among the functions in this set. In Fig. 8, we plot the smoothed spline curves $s_{a}^{h}$  and $s_{p}^{h}$ obtained for all 14 corruption functions included in our experiments. We can observe that, for all corruption functions shown, human performance decreases slowly for small values of visual degrade (), but once $\\Delta,$ , reaches a turning point, human performance starts decreasing more rapidly. Then, we observe that spline curves obtained for certain blur and noise transformations have similar shapes, while those for dissimilar transformations start decreasing at different turning points with different slopes. More specifically, the differences between two spline curves are statistically insignificant if their 83% confidence intervals overlap [25]. Summary: By checking statistical significance with 83%confidence interval for each corruption function, we empirically observed two classes of visually similar corruptions in our experiments with humans: (1) the noise class: Shot.Noise, Impulse Noise, Gaussian Noise, and Uniform Noise;and (2) the blur class: Blur, Median Blur, Gaussian Blur,Glass blur, Defocus Blur. The remaining corruptions are dissimilar (see Fig. 8). NN Robustness for Visually Similar Corruption Functions. Due to fundamental differences between humans and NNs, such as computational power, NNs may respond differently to visually similar corruptions. VCR allows us to empirically analyze these differences. For instance, during deployment, NNs may encounter noise with unknown distributions (e.g., Uniform, Gaussian, Poisson) that do not affect humans as shown in Fig. 8, but could pose safety concerns if NNs are particularly sensitive to specific distributions. For example, Gaussian Noise and Uniform Noise (visually similar) both add noise to images but from different distributions. Our results in Fig. 6 and Fig. 7 suggest that the NNs detect the distribution difference. Models generally exhibit higher HMRI and MRsI values for Uniform Noise compared to Gaussian Noise. While performance differences are not statistically significant with low corruption $(\\Delta_{v}<0.8)$ , models perform better with Uniform Noise than.Gaussian Noise at higher corruption levels $(\\Delta)$ , between [0.8..1.0]). Studying VCR helps analyze how different noise distributions impact NN performance, an impractical task with human data for all possible distributions. Identifying visually similar corruption functions and reusing human data can significantly reduce experimental costs. Identifying Visually Similar Transformations. We propose a simple method for identifying classes of visually similar corruptions by determining if humans can distinguish between them. Participants view corrupted images and indicate if they believe the corruptions are the same or different. By measuring accuracy across multiple trials, we use a binomial test to assess statistical significance. Our method can detect visually similar transformations quickly,reducing experiment time from about 5.55 hours with 2,000images and five participants to just 5 minutes. Limitation: Our method's results depend on participants hav-.ing normal eyesight and basic knowledge of image corruptions, and may not always accurately identify visually similar transformations. For instance, transformations with differ-.ent visual effects but similar impacts on human robustness might not be detected. Despite these limitations, we hope our approach encourages further research into how NNs and.humans respond differently to corruptions.",
    "Image_paths": [
      "D:\\Workspace_2\\DOC_QA\\app\\docs\\pdf_output\\Assessing_Visually-Continuous_Corruption_Robustness_of_Neural_Networks\\imgs\\img_in_chart_box_282_311_812_754.jpg",
      "D:\\Workspace_2\\DOC_QA\\app\\docs\\pdf_output\\Assessing_Visually-Continuous_Corruption_Robustness_of_Neural_Networks\\imgs\\img_in_image_box_641_1292_1824_1907.jpg",
      "D:\\Workspace_2\\DOC_QA\\app\\docs\\pdf_output\\Assessing_Visually-Continuous_Corruption_Robustness_of_Neural_Networks\\imgs\\img_in_chart_box_1330_1996_2198_2346.jpg",
      "D:\\Workspace_2\\DOC_QA\\app\\docs\\pdf_output\\Assessing_Visually-Continuous_Corruption_Robustness_of_Neural_Networks\\imgs\\img_in_chart_box_537_301_1931_664.jpg",
      "D:\\Workspace_2\\DOC_QA\\app\\docs\\pdf_output\\Assessing_Visually-Continuous_Corruption_Robustness_of_Neural_Networks\\imgs\\img_in_chart_box_463_723_1997_1457.jpg",
      "D:\\Workspace_2\\DOC_QA\\app\\docs\\pdf_output\\Assessing_Visually-Continuous_Corruption_Robustness_of_Neural_Networks\\imgs\\img_in_chart_box_480_1629_1983_2371.jpg",
      "D:\\Workspace_2\\DOC_QA\\app\\docs\\pdf_output\\Assessing_Visually-Continuous_Corruption_Robustness_of_Neural_Networks\\imgs\\img_in_chart_box_532_259_1931_651.jpg"
    ],
    "Image_captions": [
      "Figure 2. Auxiliary VCR metrics to compute HMRI and MSRI.",
      "Figure 3. Image corruption functions.",
      "Figure 4. Histograms showing.$\\Delta_{v}$ distribution between ImAGENeT-C and our VCR test sets for Gaussian Blur.",
      "Figure 5. Comparison between ImAGeNeT-C and VCR with Gaussian Noise. Models discussed in the text are marked by a red triangle.",
      "Figure 6. VCR evaluation results for Gaussian Noise. Results include, for each NN, the estimated curves S $S_{a}$  and $s_{p}$  (representing how the performance w.r.t. the robustness properties a and p decreases as.$\\Delta_{i}$ , increases); and the corresponding HMRI and MRSI values. Results are.colored based on their category: Human, Vision Transformer, Supervised Learning, SWsL, Adversarial Training, CLIP..",
      "Figure 7. VCR evaluation results for Uniform Noise.",
      "Figure 8. Comparing human performance spline curves $s_{a}^{h}$ for similar and dissimilar corruption functions. For each curve, the coloured.region around the curve is the 83% confidence interval used for comparison of similarity. See $s_{p}^{h}$ in App. E."
    ],
    "Tables": [
      [
        {
          "Auxiliary metric (cf. 1 VCR of humans w.r.t. a property , $\\overline{{\\mathrm{F i g}~,~2a}})$": "estimated as an area under performance curve $\\begin{array}{r}{\\hat{\\mathcal{R}}_{\\gamma}^{h}=A_{\\gamma}^{h}=\\int_{0}^{1}\\;s_{\\gamma}^{h}(v)d v}\\end{array}$ $A_{\\gamma}^{h}.$"
        },
        {
          "Auxiliary metric (cf. 1 VCR of humans w.r.t. a property , $\\overline{{\\mathrm{F i g}~,~2a}})$": "VCR of a model . a property , estimated as an area under performance curve A: $\\begin{array}{r}{\\hat{\\mathcal{R}}_{\\gamma}^{m}=A_{\\gamma}^{m}=\\int_{0}^{1}s_{\\gamma}^{m}(v)d v}\\end{array}$ $\\overline{{f(x)\\mathrm{~w.r.t~}}}$"
        },
        {
          "Auxiliary metric (cf. 1 VCR of humans w.r.t. a property , $\\overline{{\\mathrm{F i g}~,~2a}})$": "VCR lead of humans over a model f(x) w.r.t. a property , $\\begin{array}{r}{\\hat{\\mathcal{R}}_{\\gamma}^{h>m}=A_{\\gamma}^{h>m}=\\int_{0}^{1}\\operatorname*{m a x}\\big(0,s_{\\gamma}^{h}(v)-s_{\\gamma}^{m}(v)\\big)d v}\\\\ {end{}$ $\\mathrm{e s t i m a t e d\\;a s\\;a\\;d i f f e r e n c e\\;a r e a\\;}e\\;A_{\\gamma}^{h>m};$"
        },
        {
          "Auxiliary metric (cf. 1 VCR of humans w.r.t. a property , $\\overline{{\\mathrm{F i g}~,~2a}})$": "VCR lead of a model f(x) over humans w.r.t. a property , $\\begin{array}{l}{\\hat{\\mathcal{R}}_{\\gamma}^{m>h}=A_{\\gamma}^{m>h}=\\int_{0}^{1}\\operatorname*{m a x}\\big(0,s_{\\gamma}^{m}(v)-s_{\\gamma}^{h}(v)\\big)d v}\\\\ \\end{array}$ $\\mathrm{e s t i m a t e d~a s~a~d i f f e r e n c e~a r e a~}A_{\\gamma}^{m>h}$ $A_{\\gamma}^{m>h}$"
        }
      ],
      [
        {
          "Model": "NOISYMIX NEW [5]",
          "Architecture": "ResNet-50",
          "Training Method": "Supervised"
        },
        {
          "Model": "SIN_IN [11]",
          "Architecture": "ResNet-50",
          "Training Method": "Supervised"
        },
        {
          "Model": "HMANY [14]",
          "Architecture": "ResNet-50",
          "Training Method": "Supervised"
        },
        {
          "Model": "STANDARD_R50 [12]",
          "Architecture": "ResNet-50",
          "Training Method": "Supervised"
        },
        {
          "Model": "TIAN_DEIT-S [47]",
          "Architecture": "DeiT Small",
          "Training Method": "Supervised ViT"
        },
        {
          "Model": "Do_50_2_LINF [40]",
          "Architecture": "WideResNet-50-2",
          "Training Method": "Adversarial"
        },
        {
          "Model": "LIU_CONVNEXT-L [43]",
          "Architecture": "ConvNeXt-L",
          "Training Method": "Adversarial"
        },
        {
          "Model": "SWSL_RESNET18 [49]",
          "Architecture": "ResNet-18",
          "Training Method": "Semi-weakly sup."
        },
        {
          "Model": "CLIP [37]",
          "Architecture": "Clip",
          "Training Method": "Supervised CLIP"
        },
        {
          "Model": "",
          "Architecture": "",
          "Training Method": ""
        }
      ],
      [
        {
          "0": "Corruption",
          "1": "Coverage",
          "2": "Coverage"
        },
        {
          "0": "Corruption",
          "1": "IMAGENET-C",
          "2": "VCR Test Set"
        },
        {
          "0": "Brightness",
          "1": "0.590",
          "2": "1.000"
        },
        {
          "0": "Gaussian Blur",
          "1": "0.564",
          "2": "0.974"
        },
        {
          "0": "Defocus Blur",
          "1": "0.538",
          "2": "0.923"
        },
        {
          "0": "Shot Noise",
          "1": "0.462",
          "2": "0.590"
        },
        {
          "0": "Frost",
          "1": "0.436",
          "2": "1.000"
        },
        {
          "0": "Gaussian Noise",
          "1": "0.436",
          "2": "0.872"
        },
        {
          "0": "Impulse Noise",
          "1": "0.385",
          "2": "0.641"
        },
        {
          "0": "Motion Blur",
          "1": "0.333",
          "2": "0.974"
        },
        {
          "0": "Glass Blur",
          "1": "0.333",
          "2": "0.949"
        }
      ],
      [
        {
          "0": "corruption function",
          "1": "",
          "2": "Results for Standard.R50 [3]",
          "3": "Results for Standard.R50 [3]",
          "4": "Results for Standard.R50 [3]",
          "5": "Results for Standard.R50 [3]",
          "6": "Results for Standard.R50 [3]",
          "7": "Results for Standard.R50 [3]",
          "8": "After Retraining",
          "9": "",
          "10": "",
          "11": "",
          "12": "",
          "13": "",
          "14": "",
          "15": "Results for SIN [11]",
          "16": "After Retraining",
          "17": ""
        },
        {
          "0": "corruption function",
          "1": "",
          "2": "Accuracy",
          "3": "Before Retraining",
          "4": "",
          "5": "Prediction similarity",
          "6": "",
          "7": "Accuracy",
          "8": "After Retraining",
          "9": "Prediction similarity",
          "10": "Accuracy",
          "11": "Before Retraining",
          "12": "Prediction similarity",
          "13": "",
          "14": "Accuracy",
          "15": "Results for SIN [11]",
          "16": "After Retraining",
          "17": ""
        },
        {
          "0": "Median Blur",
          "1": "Ra",
          "2": "HMRI",
          "3": "MRSI",
          "4": "",
          "5": "HMRI",
          "6": "MRSI Ra",
          "7": "HMRI MRSI",
          "8": "Rp HMRI",
          "9": "MRSI Ra",
          "10": "HMRI",
          "11": "MRSI",
          "12": "",
          "13": "",
          "14": "MRSI",
          "15": "",
          "16": "",
          "17": "Prediction similarity"
        },
        {
          "0": "Median Blur",
          "1": "",
          "2": "0.532 0.635",
          "3": "0.000",
          "4": "Rp 0.573",
          "5": "0.673",
          "6": "0.000 0.694",
          "7": "0.828 0.003",
          "8": "0.728 0.854",
          "9": "0.001",
          "10": "0.522 0.624",
          "11": "0.00",
          "12": "Rp 0.605",
          "13": "HMRI",
          "14": "Ra",
          "15": "HMRI",
          "16": "MRSI Rp",
          "17": "HMRI MRSI"
        },
        {
          "0": "Median Blur",
          "1": "",
          "2": "0.521",
          "3": "0.011",
          "4": "0.473",
          "5": "0.572",
          "6": "0.012 0.575",
          "7": "0.690 0.025",
          "8": "0.678 0.804",
          "9": "0.031",
          "10": "0.423 0.512",
          "11": "0.015",
          "12": "0.513",
          "13": "0.710 0.618",
          "14": "0.00 0.650 0.016 0.517",
          "15": "0.774 0.625",
          "16": "0.004 0.729 0.016 0.647",
          "17": "0.852 0.004"
        },
        {
          "0": "Frost Glass Blur",
          "1": "",
          "2": "0.429 0.468 0.569",
          "3": "0.003",
          "4": "0.502",
          "5": "0.603",
          "6": "0.003 0.647",
          "7": "0.770 0.024",
          "8": "0.744 0.866",
          "9": "0.034",
          "10": "0.334 0.407",
          "11": "0.000",
          "12": "0.397",
          "13": "0.478",
          "14": "0.000",
          "15": "0.572 0.687",
          "16": "0.016 0.684",
          "17": "0.768 0.031 0.809 0.018"
        }
      ]
    ],
    "Table_captions": [
      "Table 1. Summary of the models included in our study..",
      "Table 2.$\\Delta_{v}$ Coverage Comparison with IMAGENET-C."
    ],
    "Equations": {}
  },
  "Section 6": {
    "Text": "5. Conclusion\n In this paper, we introduce visually-continuous corruption robustness (VCR) and two novel human-aware metrics for evaluating NNs. Our findings reveal a larger robustness gap between humans and NNs than previously detected,.particularly for blur corruptions. We emphasize that using a comprehensive range of visual changes is crucial for accurate robustness estimation, as insufficient coverage can lead to biased results. We also identify classes of image corruptions that similarly affect human perception, which can reduce the cost of measuring human robustness and assessing gaps with computational models. While our study focused on object recognition, human and machine vision comparisons could extend to other aspects like neural data [27, 50], contrasting Gestalt effects [24], object similarity judgments [13], or midlevel properties [45]. We hope our results inspire further robustness research and offer our benchmark datasets and code as open source.",
    "Image_paths": [],
    "Image_captions": [],
    "Tables": [],
    "Table_captions": [],
    "Equations": {}
  },
  "Section 7": {
    "Text": "References\n [1] Alexander Buslaev, Vladimir I. Iglovikov, Eugene Khvedchenya, Alex Parinov, Mikhail Druzhinin, and Alexandr A.Kalinin. Albumentations: Fast and Flexible Image Augmentations. Information, 11(2), 2020. Licensed with MIT License.To view a copy of this license see https : //github.com/albumentations-team/albumentations/blob/master/LICENSE. 4[2]Prithvijit Chattopadhyay, Judy Hoffman, Roozbeh Mottaghi,\nand Aniruddha Kembhavi. RobustNav: Towards Benchmarking Robustness in Embodied Navigation. In 2021 IEEE/CVF International Conference on Computer Vision, ICCV 2021,\nMontreal, QC, Canada, October 10-17, 2021, pages 1567115680. IEEE, 2021. 2\n[3]Francesco Croce, Maksym Andriushchenko, Vikash Sehwag,\nEdoardo Debenedetti, Nicolas Flammarion, Mung Chiang,\nPrateek Mittal, and Matthias Hein. RobustBench: A Standardized Adversarial Robustness Benchmark. In Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual, 2021. Licensed with MIT license. To view a copy of this license see https : / /\ngithub. com/RobustBench/robustbench/blob/\nmaster/LICENSE. 6, 7\n[4]Keyan Ding, Kede Ma, Shiqi Wang, and Eero P. Simoncelli.\nImage quality assessment: Unifying structure and texture similarity. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(5):2567-2581, 2022. 12[5]N. Benjamin Erichson, Soon Hoe Lim, Winnie Xu, Francisco Utrera, Ziang Cao, and Michael W. Mahoney. NoisyMix:\nBoosting Model Robustness to Common Corruptions, 2022.\n5\n[6]Chaz Firestone. Performance vs. Competence in HumanMachine Comparisons. Proceedings of the National Academy of Sciences, 117(43):26562-26571, 2020. 2, 4[7]Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun. Vision Meets Robotics: The KITTI Dataset. Int. J.\nof Robotics Research (IJRR), 2013. 13[8]Robert Geirhos, Jorn-Henrik Jacobsen, Claudio Michaelis,\nRichard S. Zemel, Wieland Brendel, Matthias Bethge, and Felix A. Wichmann. Shortcut learning in deep neural networks.\nNat. Mach. Intell., 2(11):665-673, 2020. 2[9] R Geirhos, CR Medina Temme, J Rauber, HH Schutt, M Bethge, and FA Wichmann. Generalisation in Humans and Deep Neural Networks. In NeurIPs 2018, pages 7549-7561.\nCurran, 2019. 1, 2, 4, 5, 7\n[10]Robert Geirhos, Kantharaju Narayanappa, Benjamin Mitzkus,\nTizian Thieringer, Matthias Bethge, Felix A. Wichmann,\nand Wieland Brendel. Partial success in closing the gap between human and machine vision. In Marc'Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan, editors, Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021,\nDecember 6-14, 2021, virtual, pages 23885-23899, 2021. 1,\n2,6 [11] Robert Geirhos, Patricia Rubisch, Claudio Michaelis,Matthias Bethge, Felix A. Wichmann, and Wieland Brendel. ImageNet-trained CNNs are Biased Towards Texture;Increasing Shape Bias Improves Accuracy and Robustness.In 7th International Conference on Learning Representations,ICLR 2019, New Orleans, LA, USA, May 6-9, 2019, 2019. 2,5, 6, 7\n[12]Kaiming He, X. Zhang, Shaoqing Ren, and Jian Sun. Deep Residual Learning for Image Recognition. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),\npages 770-778, 2016. 5\n[13]Martin N Hebart, Charles Y Zheng, Francisco Pereira, and Chris I Baker. Revealing the multidimensional mental representations of natural objects underlying human similarity judgements. Nature human behaviour, 4(11):1173-1185,\n2020. 8\n[14]Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu,\nSamyak Parajuli, Mike Guo, Dawn Song, Jacob Steinhardt,\nand Justin Gilmer. The Many Faces of Robustness: A Critical Analysis of Out-of-Distribution Generalization. In 2021IEEE/CVF International Conference on Computer Vision,\nICCV 2021, Montreal, QC, Canada, October 10-17, 2021,\npages 8320-8329. IEEE, 2021. 2, 5[15]Dan Hendrycks and Thomas Dietterich. Benchmarking Neural Network Robustness to Common Corruptions and Perturbations. Proceedings of the International Conference on Learning Representations, 2019. Licensed with Apache-2.0 license. To view a copy of this license see https://github.com/hendrycks/robustness/\nblob/master/LICENSE. 1,2, 3,4[16]Dan Hendrycks and Kevin Gimpel. A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017,\nConference Track Proceedings, 2017. 1[17]Dan Hendrycks, Norman Mu, Ekin Dogus Cubuk, Barret Zoph, Justin Gilmer, and Balaji Lakshminarayanan. AugMix:\nA Simple Data Processing Method to Improve Robustness and Uncertainty. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020, 2020. 2, 5\n[18]Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt,\nand Dawn Song. Natural Adversarial Examples. In IEEE Conference on Computer Vision and Pattern Recognition,\nCVPR 2021, virtual, June 19-25, 2021, pages 15262-15271.\nComputer Vision Foundation / IEEE, 2021. 1[19]T. Ho-Phuoc. CIFAR10 to Compare Visual Recognition Performance between Deep Neural Networks and Humans.\nArXiv, abs/1811.07270, 2018. 2[20] Boyue Caroline Hu, Lina Marsso, Krzysztof Czarnecki, Rick Salay, Huakun Shen, and Marsha Chechik. If a Human Can See It, So Should Your System: Reliability Requirements for Machine Vision Components. In Proceedings of the 44th International Conference on Software Engineering (ICSE'2022), Pittsburgh, USA. ACM, 2022. 1, 2, 3, 4, 12[21]Christoph Kamann and Carsten Rother. Benchmarking the Robustness of Semantic Segmentation Models with Respect to Common Corruptions. Int. J. Comput. Vis., 129(2):462483, 2021. 2\n[22]Oguzhan Fatih Kar, Teresa Yeo, Andrei Atanov, and Amir Zamir. 3D Common Corruptions and Data Augmentation.\nIn IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24,\n2022, pages 18941-18952. IEEE, 2022. 2[23]Saeed Reza Kheradpisheh, Masoud Ghodrati, Mohammad Ganjtabesh, and Timothee Masquelier. Deep Networks Can Resemble Human Feed-forward Vision in Invariant Object Recognition. Scientific reports, 6(1):1-24, 2016. 2[24]B Kim, E Reif, M Wattenberg, S Bengio, and MC Mozer.\nNeural networks trained on natural scenes exhibit gestalt closure. arxiv. arXiv preprint arXiv:1903.01069, 2019. 8[25]Roger Koenker, Pin Ng, and Stephen Portnoy. Quantile Smoothing Splines. Biometrika, 81(4):673-680, 1994. 3,\n8, 15\n[26]Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton.\nImageNet Classification with Deep Convolutional Neural Networks. In Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural Information Processing Systems 2012. Proceedings of a meeting held December 3-6, 2012, Lake Tahoe, Nevada, United States, pages 1106-1114, 2012. 5\n[27]Jonas Kubilius, Martin Schrimpf, Kohitij Kar, Rishi Rajalingham, Ha Hong, Najib Majaj, Elias Issa, Pouya Bashivan,\nJonathan Prescott-Roy, Kailyn Schmidt, et al. Brain-like object recognition with high-performing shallow recurrent anns.\nAdvances in neural information processing systems, 32, 2019.\n8\n[28]Abhinau Kumar. Python 3 implementation of the visual information fidelity (vif) image quality assessment (iqa) metric. https://github.com/abhinaukumar/vif,\n2020. Licensed with MIT license. To view a copy of this license see https: //github. com/abhinaukumar/\nvif/blob/main/LICENSE.2[29]Chang Liu, Yinpeng Dong, Wenzhao Xiang, Xiao Yang, Hang Su, Jun Zhu, Yuefeng Chen, Yuan He, Hui Xue, and Shibao Zheng. A comprehensive study on robustness of image classification models: Benchmarking and rethinking, 2023. 5[30]Raphael Gontijo Lopes, Dong Yin, Ben Poole, Justin Gilmer,\nand Ekin D. Cubuk. Improving robustness without sacrificing accuracy with patch gaussian augmentation. CoRR,\nabs/1906.02611, 2019. 2\n[31]Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt,\nDimitris Tsipras, and Adrian Vladu. Towards Deep Learning Models Resistant to Adversarial Attacks. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings, 2018. 2[32]Claudio Michaelis, Benjamin Mitzkus, Robert Geirhos, Evgenia Rusak, Oliver Bringmann, Alexander S. Ecker, Matthias Bethge, and Wieland Brendel. Benchmarking Robustness in Object Detection: Autonomous Driving when Winter is Coming. arXiv preprint arXiv:1907.07484, 2019. 2[33] Eric Mintun, Alexander Kirillov, and Saining Xie. On Interaction Between Augmentations and Corruptions in Natural Corruption Robustness. In Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pages 3571-3583, 2021. 2[34]Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo,Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mahmoud Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes,Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat,Vasu Sharma, Gabriel Synnaeve, Hu Xu, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski.Dinov2: Learning robust visual features without supervision,2023. 4, 5\n[35]Dim P. Papadopoulos, Jasper R. R. Uijlings, Frank Keller, and Vittorio Ferrari. Training Object Class Detectors with Click Supervision. In 2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA,\nJuly 21-26, 2017, pages 180-189. IEEE Computer Society,\n2017. 4\n[36]Ori Press, Steffen Schneider, Matthias Kummerer, and Matthias Bethge. Rdumb: A simple approach that questions our progress in continual test-time adaptation, 2023. 2[37]Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision, 2021. 5[38]E. Rusak, Lukas Schott, Roland S. Zimmermann, Julian Bitterwolf, O. Bringmann, M. Bethge, and Wieland Brendel.\nIncreasing the Robustness of DNNs Against Image Corruptions by Playing the Game of Noise. ArXiv, abs/2001.06057,\n2020. 2\n[39]Olga Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S.\nMa, Zhiheng Huang, A. Karpathy, A. Khosla, M. Bernstein,\nA. Berg, and Li Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision,\n115:211-252, 2015. 4, 6, 14[40]Hadi Salman, Andrew Ilyas, Logan Engstrom, Ashish Kapoor,\nand Aleksander Madry. Do Adversarially Robust ImageNet Models Transfer Better? In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. 5[41]H. R. Sheikh and A. C. Bovik. Image Information and Visual Quality. IEEE Transactions on Image Processing, 15(2):430444, 2006. 1, 2, 12\n[42]H. R. Sheikh, M. F. Sabir, and A. C. Bovik. A Statistical Evaluation of Recent Full Reference Image Quality Assessment Algorithms. IEEE Transactions on Image Processing,\n15(11):3440-3451, 2006. 12[43]Naman D Singh, Francesco Croce, and Matthias Hein. Revisiting adversarial training for imagenet: Architectures, training and generalization across threat models, 2023. 5[44]J. Stallkamp, M. Schlipsing, J. Salmen, and C. Igel. Man vs.\nComputer: Benchmarking Machine Learning Algorithms for Traffic Sign Recognition. Neural Networks, 32:323 - 332,\n2012. Selected Papers from IJCNN 2011. 2 [45] Katherine R Storrs, Barton L Anderson, and Roland W Fleming. Unsupervised learning predicts human perception and misperception of gloss. Nature Human Behaviour,5(10):1402-1417, 2021. 8\n[46]Jiachen Sun, Qingzhao Zhang, Bhavya Kailkhura, Zhiding Yu, Chaowei Xiao, and Z. Morley Mao. Benchmarking Robustness of 3D Point Cloud Recognition Against Common Corruptions. CoRR, abs/2201.12296, 2022. 2[47]Rui Tian, Zuxuan Wu, Qi Dai, Han Hu, and Yu-Gang Jiang.\nDeeper Insights into the Robustness of ViTs towards Common Corruptions, 2022. 5\n[48]Zhou Wang, A.C. Bovik, H.R. Sheikh, and E.P. Simoncelli.\nImage Quality Assessment: From Error Visibility to Structural Similarity. IEEE Trans. on Image Processing, 13(4):600612, 2004. 12\n[49] I. Zeki Yalniz, Herve Jegou, Kan Chen, Manohar Paluri, and.\nDhruv Mahajan. Billion-scale semi-supervised learning for image classification. CoRR, abs/1905.00546, 2019. 5[50]Daniel LK Yamins, Ha Hong, Charles F Cadieu, Ethan A Solomon, Darren Seibert, and James J DiCarlo. Performanceoptimized hierarchical models predict neural responses in higher visual cortex. Proceedings of the national academy of Sciences, 111(23):8619-8624, 2014. 8[51]Chenyu Yi, Siyuan Yang, Haoliang Li, Yap-Peng Tan, and Alex C. Kot.  Benchmarking the Robustness of SpatialTemporal Models Against Corruptions. In Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021,\nDecember 2021, virtual, 2021. 2[52]Dong Yin, Raphael Gontijo Lopes, Jonathon Shlens, Ekin Dogus Cubuk, and Justin Gilmer. A Fourier Perspective on Model Robustness in Computer Vision. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 13255-13265, 2019. 2\n[53] Sangdoo Yun, Dongyoon Han, Sanghyuk Chun, Seong Joon Oh, Youngjoon Yoo, and Junsuk Choe. CutMix: Regularization Strategy to Train Strong Classifiers With Localizable Features. In 2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019, Seoul, Korea (South), October 27 - November 2, 2019, pages 6022-6031. IEEE, 2019. 2[54]Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, and David Lopez-Paz. mixup: Beyond Empirical Risk Minimization. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings, 2018. 2[55]Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman,\nand Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In 2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018,\nSalt Lake City, UT, USA, June 18-22, 2018, pages 586-595.\nComputer Vision Foundation / IEEE Computer Society, 2018.\n2\n[56]Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman,\nand Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 586-595,2018. 12",
    "Image_paths": [],
    "Image_captions": [],
    "Tables": [],
    "Table_captions": [],
    "Equations": {}
  },
  "Summary": "This research investigates deep learning models vulnerability to image corruption and adversarial attacks, highlighting the need for robust systems. Studies demonstrate that state-of-the-art models struggle with common distortions, particularly in real-world scenarios. Adversarial training, data augmentation, and transfer learning are employed to enhance model resilience. Human visual perception studies reveal that unsupervised learning can predict both correct and erroneous responses, suggesting a link between neural mechanisms and machine learning. Techniques like Mixup and CutMix improve robustness by enhancing feature localization and reducing sensitivity. Perceptual metrics offer benchmarks, though they lack comprehensive corruption analysis. Spatial-temporal models face unique vulnerabilities, demanding specialized training. Transferability to new tasks is observed, yet remains brittle against adversarial attacks. The field emphasizes the critical importance of robust training strategies and perceptual understanding.  Researchers are exploring new methods to address these challenges, including Rdumb, to continually adapt models. Ultimately, this research underscores the need for robust deep learning systems capable of reliable performance in imperfect data.",
  "ID": "7794495d-fb41-5cb5-8660-750458c1e209"
}
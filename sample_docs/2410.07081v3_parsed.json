{
  "File_Name": "2410.07081v3.pdf",
  "Section 0": {
    "Text": " Ahmed H. Salamah*, Kaixiang Zheng*, Yiwen Liu & En-Hui Yang. Department of Electrical and Computer Engineering, University of Waterloo {ahamsalamah, k56zheng, e3liu, ehyang}@uwaterloo.ca.",
    "Image_paths": [],
    "Image_captions": [],
    "Tables": [],
    "Table_captions": [],
    "Equations": {}
  },
  "Title": "JPEG INSPIRED DEEP LEARNING",
  "Section 1": {
    "Text": "ABSTRACT\n Although it is traditionally believed that lossy image compression, such as JPEG com-.pression, has a negative impact on the performance of deep neural networks (DNNs),.it is shown by recent works that well-crafted JPEG compression can actually improve.the performance of deep learning (DL). Inspired by this, we propose JPEG-DL, a novel DL framework that prepends any underlying DNN architecture with a trainable JPEG compression layer. To make the quantization operation in JPEG compression trainable, a new differentiable soft quantizer is employed at the JPEG layer, and then the quantization operation and underlying DNN are jointly trained. Extensive experiments show that in.comparison with the standard DL, JPEG-DL delivers significant accuracy improvements across various datasets and model architectures while enhancing robustness against adversarial attacks. Particularly, on some fine-grained image classification datasets, JPEGDL can increase prediction accuracy by as much as 20.9%. Our code is available on https://github.com/AhmedHussKhalifa/JPEG-Inspired-DL.git.",
    "Image_paths": [],
    "Image_captions": [],
    "Tables": [],
    "Table_captions": [],
    "Equations": {}
  },
  "Section 2": {
    "Text": "1 INTRODUCTION\n JPEG compression (Pennebaker & Mitchell, 1992) is the defacto lossy image compression technique with.ubiquitous presence in real-world applications. With the development of deep learning, more and more images, potentially compressed by JPEG, are consumed by deep neural networks (DNNs). Naturally, it's of interest to study how JPEG compression will impact DNN performance for computer vision tasks, and extensive research has been conducted along this line such as Dodge & Karam (2016); Liu et al. (2018)and Xie & Kim (2019). These initial explorations establish a widely accepted view that the information loss caused by JPEG obscures important features in the input image, thereby negatively impacting DNN performance. However, it was shown by Yang et al. (2021) that the above conventional wisdom does not hold anymore if JPEG compression is applied intelligently and adaptively on a per-image basis. Indeed, Yang et al. (2021)showed that if applied appropriately, JPEG compression can actually improve DNN performance, at least in theory. They further proposed to train a DNN based on images with various JPEG quality levels. While they managed to demonstrate improved performance with a specially designed DNN topology, the model is too cumbersome to be fully trained, leading to suboptimal performance. Moreover, the adherence to the default JPEG quantization also limits the effectiveness of this method. On the other hand, given a fixed DNN model,Zheng et al. (2023) and Salamah et al. (2024b) found that its performance could be slightly improved if the input images got compressed by JPEG with optimized quantization parameters. However, the performance gain is not significant due to the frozen DNN model. Although these existing works provide promising.insights to improve DNN performance with JPEG compression, a solid solution that can fully unleash the potential of this idea has yet to be found. To address the above issue, in this paper, we propose jointly optimizing both JPEG quantization operation.and a DNN to achieve greater effectiveness. To this end, we first introduce a trainable JPEG compression layer, the structure of which is illustrated in Fig. 1a. This layer follows the standard JPEG pipeline to convert a preprocessed input image, after passing through the input layer, into blockwise Discrete Cosine Transform (DCT) coefficients in the YCbCr color space. A novel differentiable soft quantizer $(Q_{d})$ is then applied to quantize the DCT coefficients at each frequency position, followed by the standard JPEG inverse process to reconstruct the RGB image. We then present JPEG-DL, a novel deep learning (DL) framework.that introduces a new DNN architecture by inserting a JPEG layer directly after the original input layer of any underlying DNN architecture. This layer can be considered an integral part of any underlying DNN architecture, forming a new unified DNN architecture, whose parameters get optimized jointly with DNN model weights during training. The core of JPEG-DL is $Q_{d}$  as it substitutes the non-differentiable, hard quantization in JPEG with a differentiable, soft quantization operation defined by a neat analytical formula,which not only facilitates gradient-based optimization for quantization parameters but also introduces additional trainable non-linearity to the overall image understanding pipeline. To validate the effectiveness of.JPEG-DL, we conducted extensive experiments for image classification on six datasets including four finegrained classification datasets (Wah et al., 2011; Khosla et al., 2011; Nilsback & Zisserman, 2008; Parkhi et al., 2012), CIFAR-100 (Krizhevsky et al., 2009) and ImageNet (Deng et al., 2009). Results show that JPEG-DL significantly and consistently outperforms the standard DL across various DNN architectures,with a negligible increase in model complexity. Specifically, JPEG-DL improves classification accuracy by up to 20.9% on some fine-grained classification dataset, while adding only 128 trainable parameters to the DL pipeline. Moreover, the superiority of JPEG-DL over the standard DL is further demonstrated by the enhanced adversarial robustness of the learned unified architecture.. The main contributions of this paper can be summarized as follows:. We introduce a novel trainable JPEG layer leveraging differentiable soft quantizers with nice ana-.lytical formulas. Based on the new JPEG layer, we propose a new DL framework dubbed JPEG-DL, which jointly optimizes the JPEG layer and the DNN model during training.. The outstanding performance of JPEG-DL over the standard DL is verified by comprehensive experimental results on various image classification datasets across multiple DNN architectures and.for a variety of tasks including adversarial defense..",
    "Image_paths": [
      "D:\\Workspace_2\\DOC_QA\\app\\docs\\pdf_output\\2410.07081v3\\imgs\\img_in_image_box_394_429_2171_1031.jpg"
    ],
    "Image_captions": [
      "Figure 1: (a) The JPEG-DL framework consists of a JPEG layer followed by a standard DNN, where the standard forward/inverse processes of JPEG depicted in white boxes are fixed and not trained. The JPEG pipeline receives a conventionally preprocessed input image $x\\in\\mathbb{R}^{3\\times W\\times H}$ sampled from the underlying.task's dataset. The JPEG layer, equipped with a differentiable soft quantizer, and the underlying DNN form a unified new DNN architecture, which are shown in blue indicating that they are trainable components. (b) As an example, we show $z_{1,:,:},\\mathrm{i.e.}$ ., the DCT representation of the Y channel of an image, by a tensor consisting.of B blocks of DCT coefficients. Each 88 block contains.$M=64\\:{D C T}$ ' frequencies, ordered from low to.high in a zigzag manner. Then, we show how $z_{1,1},$ : and $z_{1,M,}$ ,: are quantized by $Q_{d}(\\cdot;q=1,\\alpha=10)$ ! and $Q_{d}(\\cdot;q=0.5,\\alpha=16)$ ', respectively."
    ],
    "Tables": [],
    "Table_captions": [],
    "Equations": {}
  },
  "Section 3": {
    "Text": "2 BACKGROUND AND RELATED WORK\n JPEG applications. JPEG was originally developed as a lossy image compression technique based on transform coding, which reduces image file sizes by generating their compact representations. Beyond its traditional use, JPEG has found numerous applications in deep learning: (1) it has been utilized as a data augmentation technique to improve robustness of DNNs against image compression (Benbarrad et al., 2022);(2) it has been employed as an empirical defense method against adversarial attacks, effectively reducing adversarial perturbations and enhancing the adversarial robustness of DNNs (Dziugaite et al., 2016; Das et al., 2017; Guo et al., 2018); and (3) it has been integrated into the knowledge distillation (Hinton et al.,2015) framework by Salamah et al. (2024a), where it helps the teacher model transfer knowledge to the student model in a more effective way. In contrast, this paper focuses on leveraging JPEG to enhance the natural performance, instead of the robust performance, of DNNs without relying on any teacher model. Optimizing JPEG Compression for DNN vision. As a lossy image compression technique, JPEG is de-.veloped specifically for the human visual system. As a result, while the information loss introduced by JPEG is often imperceptible to humans, it can significantly degrade the performance of DNNs. This issue gives rise to a line of research which optimizes JPEG compression based on DNN perception. For instance,given a pretrained DNN, Xie & Kim (2019), Zheng et al. (2023) and Salamah et al. (2024b) first derive its sensitivity to different DCT frequencies, based on which they customize JPEG quantization tables for this DNN to reach the optimal rate-accuracy tradeoff. Another popular direction is to make JPEG trainable,which integrates the JPEG encoder and DNN into an end-to-end differentiable training framework (Luo et al., 2020; Xie et al., 2022). These methods create differentiable proxies for image quantization and bitrate calculation, so that JPEG compression can be optimized via backpropagation in order to minimize a total loss considering both DNN performance and bitrate. However, all the above works focus on mitigating DNN performance degradation in the presence of JPEG compressed images, but offer little to no improvement on DNN performance when raw images are given as input. On the contrary, this paper leverages JPEG purely as a tool to improve DNN performance on raw images, regardless of its compression capability. Improving DNN performance with JPEG. Due to the aforementioned reason, JPEG compression generally hurts DNN performance. However, it's recently demonstrated by Yang et al. (2021) that, with an oracle guiding the compression process, one can select an optimal quality level to compress each image, enabling the DNN to make its best possible prediction for it. By applying this adaptive JPEG compression across a set of images, one can actually improve the DNN prediction accuracy considerably. This phenomenon, termed \"compression helps\" in the original paper, is justified by the fact that compression can remove noise and disturbing background features, thereby highlighting the main object in an image, which helps DNNs make.better prediction. Due to the need of ground truth labels in the compression stage, the above adaptive compression scheme is not realizable in most real world applications; however, the discovery of \"compression helps\"' at least show the potential of using JPEG to improve DNN performance. Thus motivated, the authors in turn proposed an implementable way to improve DNN performance with JPEG. They built a new DNN topology that incorporates 11 parallel branches of an underlying pretrained model, with each branch obtain ing as input either the raw image or its compressed version at a varying quality level. The penultimate layer representations from all branches are concatenated and fed into a classification head, which is then trained together with those 11 model backbones as a unified DNN structure. This new DNN topology is clearly too complex to train, so the authors opted for partial training to mitigate the training complexity, therefore resulting in a suboptimal performance. In contrast, our proposed method introduces only 128 additional trainable parameters to the existing DL pipeline, causing negligible complexity increase. Moreover, it's noteworthy that our method is orthogonal to theirs, as our trainable JPEG quantization tables can be embedded into their framework to replace the default JPEG quantization tables, thereby further improving the performance.. Trainable Activation Functions. Relentless efforts have been made to search for activation functions that can improve DNN performance. Among all the directions, trainable activation functions have gained particular interest for their flexibility, expressive power, and adaptability during training. Chen & Chang (1996)propose the adjustable generalized hyperbolic tangent function, which extends the classic hyperbolic tangent function by introducing parameters to control the saturation level and slope of the function. He et al. (2015)introduce the parametric ReLU (PReLU), a variant of ReLU with a trainable parameter that adjusts the negative part of ReLU. More recently, Kolmogorov-Arnold Networks (KANs) (Liu et al., 2024) employ trainable activation functions on edges, with nodes simply summing all the incoming activations. Interestingly, the differentiable soft quantizers in our JPEG layer can be interpreted as trainable activation functions whose input is the DCT representations of images. These trainable soft quantizers effectively introduce additional nonlinearity to DNN models, thus improving their expressive power.",
    "Image_paths": [],
    "Image_captions": [],
    "Tables": [],
    "Table_captions": [],
    "Equations": {}
  },
  "Section 4": {
    "Text": "JPEG-DL: JPEG INSPIRED DL\n",
    "Image_paths": [],
    "Image_captions": [],
    "Tables": [],
    "Table_captions": [],
    "Equations": {}
  },
  "Section 5": {
    "Text": "3.1 I  PROBLEM FORMULATION\n In the JPEG pipeline, an RGB image.$x\\in\\mathbb{R}^{3\\times W\\times H}$ is first converted to the YCbCr color space and then partitioned into B non-overlapping.$8\\times8$ blocks, where DCT is applied to each of these blocks to obtain the corresponding DCT coefficients. For each color channel, the DCT coefficients in each block are then flattened following the zigzag order, resulting in $M=64$ : frequency positions ordered from low frequency to high frequency. Therefore, we denote the DCT coefficients of the image x as $z\\;=\\;[left[z_{l,m,n}\\right]$ where $l\\;=\\;1,2,3$ . corresponds to the color channel Y, Cb and Cr respectively,$1\\;\\leq\\;m\\;\\leq\\;M$ is the index for frequency position, and $1\\leq n\\leq B$ ' is the index for block. Up to this point, all operations involved are differentiable. Next, quantization tables $Q_{Y}=\\left[q_{1},q_{2},\\ldots,q_{M}\\right]\\operatorname{a n d}Q_{C}=\\left[q_{M+1},q_{M+2},\\ldots,q_{2M}\\right]$ are used for the luminance (Y) and chrominance (CbCr) channels respectively to quantize their DCT coefficients..Following uniform quantization, we obtain quantized DCT coefficients $\\hat{z}_{l,m,n}=\\left\\lfloor z_{l,m,n}/q_{m}\\right\\rfloor\\cdot q_{m}$ , for $l=1$ 9and $\\hat{z}_{l,m,n}=\\left[lfloor_{l,m,n}/q_{M+m}\\right\\rceil\\\\cdot q_{M+m}$  for $l=2,3$  Note that quantization is non-differentiable due to the use of the rounding operation. Finally, inverse operations including blockwise inverse DCT (IDCT)transform, blocks merging and YCbCr-to-RGB color space conversion are conducted over  sequentially to obtain the reconstructed RGB image x. Similar to their forward counterparts, all these inverse operations are differentiable. Denoting the composition of all the above operations as $\\mathcal{J}$ , we then have $\\hat{x}=\\hat{\\mathcal{J}}(x;Q)$ 9where $Q=(Q_{Y},Q_{C})$ . Hereafter, the mapping $\\mathcal{J}$  stands for the JPEG encoding-decoding operation. In supervised learning, each.$x\\in\\mathcal{X}$ corresponds to a ground truth label.$y\\in\\mathcal{Y}$ . Let $f_{\\theta}$ represent a DNN model with trainable weights $\\theta,$  and let $\\mathcal{L}$  denote the loss function used to train this DNN. In standard DL,the primary objective is to solve the following minimization problem: [EQ_0] In contrast, JPEG-DL tries to improve the performance of DNN by jointly training it with the JPEG operation. As a result, the formulation should be instead: [EQ_1] However, in order to solve (2) with gradient descent, the key challenge is caused by the non-differentiable.quantization operation, which makes the gradients w.r.t.$Q$ almost zero everywhere. To address this issue,we will introduce a differentiable soft quantizer $(Q_{d})$ in the next subsection, replacing the uniform quantizer $(Q_{u})$ I used in $\\mathcal{J}$",
    "Image_paths": [],
    "Image_captions": [],
    "Tables": [],
    "Table_captions": [],
    "Equations": {
      "[EQ_0]": "\\operatorname*{m i n}_{\\theta}\\;\\mathbb{E}[\\mathcal{L}(f_{\\theta}(x),y)].",
      "[EQ_1]": "\\operatorname*{m i n}_{\\theta,Q}\\;\\mathbb{E}[\\mathcal{L}\\big(f_{\\theta}(\\mathcal{J}(x;Q)),y)]."
    }
  },
  "Section 6": {
    "Text": "3.2 DIFFERENTIABLE SOFT QUANTIZER\n Denote the index set of uniform quantization as [EQ_0] For convenience,$\\mathcal{A}$ is also regarded as a vector of length $2L+1$ . Multiplying $\\mathcal{A}$ with a quantization step size q, we get the corresponding reconstruction space [EQ_1] Again, we will regard $\\hat{\\mathcal A $ as both a vector and a set. To randomly quantize a DCT coefficient z to an element in $\\hat{\\mathcal A $ , we invoke from Yang & Hamidi (2024) a trainable conditional probability mass function $(\\mathrm{C P M F})\\:P_{\\alpha}(\\cdot|z)$ over the reconstruction space $\\hat{\\mathcal A $  or equivalently the index set A given z, where $\\alpha>0$ is a trainable parameter: [EQ_2] [EQ_3] Extend $z\\mathrm{~\\text{o}~}$ a vector of length $2L+1$ , i.e.,$[z]_{2L+1}=[\\widetilde{z,\\ldots,z}]$ . Then, the CPMF $P_{\\alpha}(\\cdot|z)$ ', regarded as a vector of length $2L+1$ , can be easily computed via the softmax operation $\\sigma(\\cdot)$ : [EQ_4] With the CPMF $P_{\\alpha}(\\cdot|z)$ , z is now quantized to each $i q\\in\\hat{A}$ with probability $P_{\\alpha}(i q|z)$ .Note that as $\\alpha\\;\\to\\;\\infty,\\:P_{\\alpha}(\\cdot|z)$ approaches an one-hot vector with probability 1 at the nearest point to z in $\\hat{\\mathcal A $ and 0elsewhere. Therefore, the resulting random quantizer effectively functions as the deterministic uniform.quantizer $Q_{u}(z)=\\lfloor z/q\\rfloor\\cdot q$ Based on the CPMF $P_{\\alpha}(\\cdot|z)$ , we can now define a differentiable soft quantizer $Q_{d}$  as the conditional expectation of iq given z, i.e., [EQ_5] Similarly, as $\\alpha\\to\\infty,Q_{d}$ : also goes to $Q_{u}$ . Fig. 2 shows how the shape of $Q_{d}$ varies w.r.t a, given a fixed $q_{1}$ This soft quantizer $Q_{d}$ serves as an analytical proxy for $Q_{u}$ . It's differentiable everywhere, allowing gradients to flow through it smoothly. More importantly, compared to.$Q_{u},Q_{d}$ : involves a trainable parameter a which can adjust the softness of the quantizer, thereby introducing more flexibility. In view of these nice properties,$Q_{d}$ : is the ideal candidate in place of.$Q_{u}$  used in $\\mathcal{J}$ As a side note, for the reader who is not familiar with quantization, but familiar with the attention operation used in transformer models (Vaswani, 2017),$Q_{d}$ can be regarded as an attention operation in broad sense,with the query being z, the key and value being $\\hat{\\mathbf{}\\\\}}$ , and the similarity metric between the query and key being negative squared distance instead of dot product.",
    "Image_paths": [],
    "Image_captions": [],
    "Tables": [],
    "Table_captions": [],
    "Equations": {
      "[EQ_0]": "\\mathcal{A}=\\{-L,-L+1,\\ldots,0,\\ldots,L-1,L\\}.",
      "[EQ_1]": "\\hat{\\mathcal{A}}=q\\times[-L,-L+1,\\ldots,0,\\ldots,L-1,L].",
      "[EQ_2]": "P_{\\alpha}(i q|z)=\\frac{e^{-\\alpha(z-i q)^{2}}}{\\sum_{j\\in\\mathcal{A}}e^{-\\alpha(z-j q)^{2}}},\\:\\forall i\\in\\mathcal{A}.",
      "[EQ_3]": "2L+1\\;\\mathrm{t i m e s}",
      "[EQ_4]": "\\big[P_{\\alpha}(\\cdot|z)\\big]_{2L+1}=\\sigma\\Big(-\\alpha\\times\\left(\\big[z\\big]_{2L+1}-\\hat{\\mathcal{A}}\\right)^{2}\\Big).",
      "[EQ_5]": "Q_{d}(z)=\\mathbb{E}[i q|z]=\\sum_{i\\in\\mathcal{A}}P_{\\alpha}(i q|z)\\cdot i q."
    }
  },
  "Section 7": {
    "Text": "3.3 OVERALL FRAMEWORK OF JPEG-DL\n Substituting $Q_{u}$ I in $\\mathcal{J}$ with $Q_{d}$ , we get a differentiable JPEG layer $\\hat{\\mathcal I $  parameterized by Q and , where $\\alpha=$ $(\\mathbf{\\alpha}_{Y},\\mathbf{\\alpha}_{C}).\\;\\mathbf{\\alpha}_{Y}=[\\alpha_{1},\\alpha_{2},\\ldots,\\alpha_{M}]\\;\\mathit{a n d}\\;\\mathbf{\\alpha}_{C}=[\\alpha_{M+1},\\alpha_{M+2},\\ldots,\\alpha_{2M}]$ are a tables for the luminance and.chrominance channels respectively, used in conjunction with $Q_{\\mathrm{}mathrm}}$ r and $Q_{C}\\mathrm{t o}$ ' quantize DCT coefficients. Following the proposed soft quantization, we obtain quantized DCT coefficients $\\hat{z}_{l,m,n}=Q_{d}(z_{l,m,n};q_{m},\\alpha_{m})$ for $l=1$ , and $\\hat{z}_{l,m,n}=Q_{d}(z_{l,m,n};q_{M+m},\\alpha_{M+m})$ for $l=2,3.$ , where $Q_{d}(z;q,\\alpha)$ denotes a differentiable.soft quantizer defined in (7) parameterized by a quantization step q and a scaling factor a. Overall, for an input image x, we have.$\\hat{\\boldsymbol{x}}=\\hat{\\mathcal{J}}(\\boldsymbol{x};\\boldsymbol{Q},\\boldsymbol{\\alpha})$ I. Therefore, we can rewrite (2), the JPEG-DL formulation, as [EQ_0] where the expectation can be approximated by the empirical mean over a mini-batch in actual training.Thanks to the use of.$Q_{d},(8)$ can now be solved by gradient descent with ease (see Appendix A.1 for the derivative analytical formulas). After training with JPEG-DL, we get the optimized parameters $\\theta^{*},\\;Q^{*}$ and $\\alpha^{*}$ .Then, we consider the composition of the JPEG layer and the underlying DNN as a unified DNN model $\\hat{f}(x;\\theta^{*},Q^{*},\\boldsymbol{\\alpha}^{*})$ $f_{\\theta^{*}}\\big(\\hat{\\mathcal{J}}(x;Q^{*},\\boldsymbol{\\alpha}^{*})\\big)$ I to do validation. Concretely, any raw input x should be fed into $\\hat{\\mathcal I $ , instead of directly to $f_{\\theta*}$ , thus allowing the reconstructed image x to be fed into the underlying DNN $f_{\\theta^*}$ . In other words, the JPEG layer is prepended as the first layer of any underlying DNN.. To conclude this section, we refer readers to Fig. 1a and Fig. 1b for an illustration of the inner workings of the JPEG-DL framework.",
    "Image_paths": [],
    "Image_captions": [],
    "Tables": [],
    "Table_captions": [],
    "Equations": {
      "[EQ_0]": "\\operatorname*{m i n}_{\\theta,Q,\\alpha}\\;\\mathbb{E}[\\mathcal{L}(f_{\\theta}(\\hat{\\mathcal{J}}(x;Q,\\alpha)),y)],"
    }
  },
  "Section 8": {
    "Text": "4 EXPERIMENTS\n CIFAR-100. We evaluate our proposed method using a transformer-based architecture and four state-of-theart convolutional neural networks (CNNs): EfficientFormer-L1 (Li et al., 2022), ResNet (He et al., 2016),VGG (Simonyan & Zisserman, 2014), MobileNet (Sandler et al., 2018), and ShuffleNet (Ma et al., 2018).For ResNet, we employ CIFAR-specific versions: ResNet32, ResNet56, and ResNet110. For VGG, we.utilize VGG8 and VGG13. All CNN architectures follow the training recipe from CRD (Tian et al., 2020)(see Appendix A.3), while for the transformer-based architecture, EfficientFormer-L1, we adhere to the setup proposed by Xu et al. (2023) (see Appendix A.5). Fine-grained Tasks. These tasks involve visually similar classes and typically feature fewer training samples per class compared to conventional classification tasks. We evaluate our method on four datasets:CUB-200-2011 (Wah et al., 2011), Stanford Dogs (Khosla et al., 2011), Flowers (Nilsback & Zisserman,2008), and Pets (Parkhi et al., 2012). For CNN architectures, we employ PreAct ResNet-18 (He et al., 2016) and DenseNet-BC (Huang et al., 2017), following the experimental setup and architecture modifications of Zhang et al. (2017). For the transformer-based architecture, we use EfficientFormer-L1, adopting the setup outlined by Xu et al. (2023) (see Appendices A.4 and A.5).. JPEG-layer settings for CIFAR-100 and fine-grained tasks. To train our JPEG layer, we first study the gradient nature of $\\bar{Q}_{d}$ with respect to q and a in (5) by changing one parameter and fixing the other. Both q.and a are sharable parameters among different numbers of blocks per image. During the calculation of the gradient of $Q_{d}$ : with respect to a, our analysis shows that when a is sufficiently large, such that $Q_{d}$ is not too far from $Q_{u},$ , the gradient magnitude for a is almost zero, as shown in Appendix A.1. This indicates that  will not be updated effectively if initialized within a reasonable range. Experiments also confirmed that making  trainable does not significantly impact model performance. As a result, we choose not to train a in our framework. However, when we explore the calculation of the gradient of $Q_{d}$ with respect to q, the accumulated gradient received from different blocks per image during backpropagation results in unstable gradient magnitudes at a fixed value of $\\alpha,$ , as shown in our analysis in Appendix A.1. To address this, we use the ADAM optimizer, which adapts the learning rate for each trainable parameter q in our JPEG layer,enabling more efficient training and better convergence. For CIFAR-100, we set the learning rate to 0.003across all tested models. For the fine-grained datasets, we set the JPEG learning rate to O.005. Across these datasets, we fix $\\alpha_{m}=5\\:o r\\:l l l\\\\ 1\\leq m\\leq2M$ , and set $L=2^{b-1}\\operatorname{i n}\\left(3\\right)$ ., where b is a tunable hyperparameter set to 8. ImageNet-1K. For all experiments on this dataset, we utilize the standard training recipes shown by Paszke et al. (2019) without any modifications. We use SqueezeNet (Iandola, 2016), ResNet-18 and ResNet-34 as our testing underlying models.. JPEG-layer settings for ImageNet-1K. We will utilize specific settings to control the gradient magnitude to ensure more stable updates for Q. We define the Gradient Scaling Constants.$\\hbar_{m}=\\alpha_{m}q_{m}^{2},1\\leq m\\leq2M$ 9which allows us to control the magnitude of gradients w.r.t $Q.$ Specifically, we fix $\\hbar_{m}=0.7$ ' for all $1<$ $m\\leq2M$ . During training, we update the value of $\\alpha_{m}$  to be $\\hbar_{m}/q_{m}^{2}$ before calculating the gradients w.r.t $Q.\\;\\mathrm{A s}\\;c$ t result of controlling the maximum gradient magnitude, we can optimize Q using an SGD optimizer.with O.5 learning rate, instead of an ADAM optimizer. This approach ensures stable and efficient training for quantization table updates (see Appendix A.2 for more details). For this dataset, b is equal to 11 across all tested models. Quantization Table Initialization. For CIFAR-100 and fine-grained tasks, we initialize the quantization table Q based on the reciprocal of sensitivity for each DCT frequency, given a pre-trained model of the underlying DNN architecture, following the approach described by Zheng et al. (2023); Salamah et al. (2024b).Broadly speaking, the sensitivity of a frequency indicates the rate of change of the loss function w.r.t. the perturbation on this frequency, so we favor a smaller quantization step for a more sensitive frequency to limit the distortion amount on it. For ImageNet-1K, we adopt the strategy from Esser et al. (2019), where the initialization of $q_{m}$  is based on the average of absolute values of DCT coefficients across all blocks in all training images that will be quantized by.$q_{m}$ . Specifically,$\\begin{array}{l}{{_q_{m}=2\\sum_{k=1}^{N}\\sum_{n=1}^{B}|z_{1,m,n}^{(k)}|/(N B\\sqrt{2^{b-1}})}\\\\ \\end{array}$ for $1\\leq m\\leq M$ , i.e.$Q_{Y}$ :, and $\\begin{array}{l}{{_q_{m}=\\sum_{k=1}^{N}\\sum_{l=2}^{3}\\sum_{n=1}^{B}|z_{l,m,n}^{(k)}|/(N B\\sqrt{2^{b-1}})}\\\\ \\end{array}$ for $M+1\\leq m\\leq2M$ , i.e.,$Q_{C}$ , where k is the index for image and N is the number of training images.. CIFAR-100 and Fine-grained tasks Results. The performance of JPEG-DL is shown in Tables 1 and 2.Across all seven tested models for CIFAR-100, JPEG-DL consistently provides improvements, with gains of up to 1.53% in top-1 accuracy. In the fine-grained tasks, JPEG-DL offers a substantial performance increase, with improvements of up to 20.90% across all datasets using two different models. Additional.results for CIFAR-100 and fine-grained tasks using a transformer-based model are shown in Appendices A.5. We further extend our results on fine-grained tasks to include a comparison with additional baselines that address the non-differentiability and zero derivative problems of JPEG quantization (see Appendix A.6).After demonstrating the nonlinearity introduced by inserting the JPEG layer after the input layer, we extend our evaluation to include the replacement of the ReLU activation function with our JPEG layer, which.highlights the potential of new forms of nonlinear operations in DNN architectures (see Appendix A.7).Furthermore, we provide comprehensive experiments on the impact of chrominance subsampling on JPEGDL, as well as a comparison with both non-learnable and learnable preprocessing methods (see Appendix A.8). ImageNet-1K Results. The performance of JPEG-DL is shown in Table 3. With a trivial increase in complexity (adding 128 parameters), JPEG-DL achieves a gain of 0.31% in top-1 accuracy for SqueezeNetV1.1compared to the baseline using a single round of $Q_{d}$ quantization operation. By increasing the number of quantization rounds to five, we observe an additional improvement of 0.20%, leading to a total gain of 0.51%over the baseline. The best results are indicated in bold, and values in parentheses indicate relative accuracy gains over the baseline..",
    "Image_paths": [
      "D:\\Workspace_2\\DOC_QA\\app\\docs\\pdf_output\\2410.07081v3\\imgs\\img_in_chart_box_386_2289_2159_2639.jpg"
    ],
    "Image_captions": [
      "Figure 2: Illustration of.$Q_{u}$ : vs.$Q_{d}$ with $\\alpha=1,3,5,10$ , where $L$ and q are set to 3 and 1, respectively.."
    ],
    "Tables": [
      [
        {
          "Method": "Baseline",
          "Res32": "71.14",
          "- Res56": "72.34",
          "Res110": "73.79",
          "- VGG8": "70.36 71.10+0.41 (+0.74)",
          "VGG13": "73.77 75.320.10 (+1.55)",
          "MobileNetV2": "64.6 65.910.11 (+1.31)",
          "ShuffleNetV2": "71.82"
        },
        {
          "Method": "JPEG-DL",
          "Res32": "71.920.31 (+0.78)",
          "- Res56": "73.390.19 (+1.05)",
          "Res110": "74.46+0.11 (+0.67)",
          "- VGG8": "73.040.16 (+1.22)",
          "VGG13": "",
          "MobileNetV2": "",
          "ShuffleNetV2": ""
        }
      ],
      [
        {
          "Model": "ResNet-18",
          "Method": "Baseline JPEG-DL",
          "CUB-200": "54.001.43 58.810.12 (+4.81)",
          "Dogs": "63.710.32 65.570.37 (+1.86)",
          "Flowers": "57.131.28 68.760.57(+11.63)",
          "Pets": "70.370.84 74.840.66 (+4.47)"
        },
        {
          "Model": "DenseNet-121",
          "Method": "Baseline JPEG-DL",
          "CUB-200": "57.700.44 61.320.43 (+3.62)",
          "Dogs": "66.610.17 69.670.58(+3.06)",
          "Flowers": "51.320.57 72.221.05(+20.90)",
          "Pets": "70.260.79 75.900.68(+5.64)"
        }
      ],
      [
        {
          "Method": "Baseline JPEG-DL",
          "SqueezeNetV1.1": "57.95 58.26 (+0.31)",
          "Resnet18": "69.75 70.13 (+0.38)",
          "Resnet34": "73.31 73.54 (+0.23)"
        }
      ]
    ],
    "Table_captions": [
      "Table 1: Top-1 validation accuracy (%) for Baseline and JPEG-DL on CIFAR-100. The Baseline results are from Tian et al. (2020). For JPEG-DL, we report the mean and standard deviation of experimental results over three runs.",
      "Table 2: Top-1 validation accuracy (%) on various fine-grained image classification tasks and model architectures. We report the mean and standard deviation of experimental results over three runs..",
      "Table 3: Top-1 validation accuracy (%) on ImageNet with different model architectures."
    ],
    "Equations": {}
  },
  "Section 9": {
    "Text": "5 ANALYSIS AND DISCUSSION\n Robustness. JPEG has generally been used as an empirical defense mechanism against adversarial attacks by mitigating adversarial perturbations and enhancing the robustness of DNNs, as discussed in Section 2.To evaluate the adversarial robustness of JPEG-DL models in comparison to standard DNN, we conduct experiments using two attack methods, FGSM and PGD, on CIFAR-100 with two different models from Table 1. The perturbation budget, Epsilon, ranged from 1 to 4 for both attack methods. For PGD, we applied 5 steps with a perturbation step size of $(2.5\\times E\\ s i{o l o n})/s t e p s$ , following the setup used by Madry (2017).As shown in Fig. 3, the JPEG-DL models significantly improve the adversarial robustness compared to the standard DNN models, with improvements of up to 15% for FGSM and 6% for PGD. We will show some examples of the quantization tables used in this study in the next subsection. (a) FGSM for VGG13 (b) PGD for VGG13 (c) FGSM for Res56 (d) PGD for Res56 Designed Quantization Tables. Fig. 4 presents the Y and CbCr quantization tables, both at initialization and after convergence, for VGG13 trained on CIFAR-100 and ResNet-18 trained on the CUB200 dataset.The estimated sensitivity values used to initialize these quantization tables are provided in Appendix A.10.. Feature maps visualization. Fig. 5 presents the feature maps extracted after the first dense block in.DenseNet-121 for both the JPEG-DL model and the baseline model, trained on the CUB200 dataset using the model from Table 2. The output of the feature maps at this stage is of size 5656, and both sets are shown in the same sequence using the same original image. The shown example was incorrectly classified by the baseline model, while the JPEG-DL model correctly classified it. In this figure, it is evident.that the feature maps from the JPEG-DL model show significantly better contrast between the foreground information (the bird) and the background compared to the feature maps generated by the baseline model..Specifically, the foreground object in the JPEG-DL feature maps is enclosed within a well-defined contour,making it visually distinguishable from the background. In contrast, the baseline model's feature maps show a more blended structure, where the foreground contains higher energy in low frequencies, causing it to blend more smoothly with the background. Additionally, another example in Appendix A.11 shows a similar phenomenon in addition to background information being more effectively removed, following the same.setup as the first example. These discrepancies in feature map clarity and contrast are propagated through subsequent blocks of DenseNet-121, eventually contributing to the misclassification problem observed in.the baseline model. Interpretability using CAM. Fig. 6 illustrates GradCAM++ visualizations (Chattopadhay et al., 2018) for two examples from the CUB-200 dataset, comparing the baseline model with JPEG-DL using ResNet-18as our underlying model. In both instances, the baseline model incorrectly classifies the input, while the JPEG-DL model correctly classifies it. The visualizations highlight how JPEG-DL focuses more precisely (a) Baseline (b) Input (c) JPEG-DL (a) Baseline (b) JPEG-DL (d) JPEG-DL (c) Baseline on the main object in the image, demonstrating the model's improved attention to key regions that contribute to correct classification. This highlights the effectiveness of JPEG-DL in enhancing model interpretability and performance, and it also supports the case for background removal discussed in the previous subsection..",
    "Image_paths": [
      "D:\\Workspace_2\\DOC_QA\\app\\docs\\pdf_output\\2410.07081v3\\imgs\\img_in_chart_box_379_433_2165_772.jpg",
      "D:\\Workspace_2\\DOC_QA\\app\\docs\\pdf_output\\2410.07081v3\\imgs\\img_in_chart_box_380_1246_2180_1584.jpg",
      "D:\\Workspace_2\\DOC_QA\\app\\docs\\pdf_output\\2410.07081v3\\imgs\\img_in_image_box_417_1445_776_1807.jpg",
      "D:\\Workspace_2\\DOC_QA\\app\\docs\\pdf_output\\2410.07081v3\\imgs\\img_in_image_box_865_1446_1229_1806.jpg",
      "D:\\Workspace_2\\DOC_QA\\app\\docs\\pdf_output\\2410.07081v3\\imgs\\img_in_image_box_1769_1444_2127_1805.jpg",
      "D:\\Workspace_2\\DOC_QA\\app\\docs\\pdf_output\\2410.07081v3\\imgs\\img_in_image_box_1317_1444_1678_1805.jpg"
    ],
    "Image_captions": [
      "Figure 3: Evaluate the adversarial robustness of JPEG-DL models in comparison to standard DNN on VGG13 and Res56 for CIFAR-100 against FGSM and PGD attacks.",
      "(a) VGG13 (Y Channel)(b) VGG13 (CbCr Channel) (c) ResNet-18(Y Channel) (d) ResNet-18 (CbCr Channel)Figure 4: Initial and final quantization tables for VGG13 trained on CIFAR-100 and ResNet18 trained on CUB200, with frequency indices arranged in the default zigzag order.",
      "Figure 5: Feature maps of size 5656 are shown after the first dense block in DenseNet-121 for both JPEGDL and baseline models Figs. in 5a and 5c, respectively, using an original input shown in Fig.5b. The JPEG-.DL model highlights the foreground (bird) more distinctly, while the baseline model shows less contrast,contributing to its misclassification.",
      "Figure 6: GradCAM++ visualization for baseline and JPEG-DL models on CUB200 using two examples,where the baseline model incorrectly classified them and the JPEG-DL model correctly classified them."
    ],
    "Tables": [],
    "Table_captions": [],
    "Equations": {}
  },
  "Section 10": {
    "Text": "CONCLUSION\n In contrast to the conventional understanding that JPEG compression negatively impacts the DL performance, this paper introduces a novel trainable JPEG compression layer into the DL pipeline to improve.DL performance, termed JPEG-DL. This layer enables gradient-based optimization for quantization parameters and can be integrated seamlessly with any underlying DNN to be trained jointly. To validate the effectiveness of JPEG-DL, we conduct extensive experiments on six image classification datasets and show significant gains over the standard DL. Additionally, we demonstrate that JPEG-DL improves the adversarial.robustness of the learned model..",
    "Image_paths": [],
    "Image_captions": [],
    "Tables": [],
    "Table_captions": [],
    "Equations": {}
  },
  "Section 11": {
    "Text": "ACKNOWLEDGMENTS\n This work was supported in part by the Natural Sciences and Engineering Research Council of Canada under Grant RGPIN203035-22, and by the Canada Research Chairs Program.",
    "Image_paths": [],
    "Image_captions": [],
    "Tables": [],
    "Table_captions": [],
    "Equations": {}
  },
  "Section 12": {
    "Text": "REFERENCES\n Johannes Balle, Valero Laparra, and Eero P Simoncelli. End-to-end optimized image compression. arXiv preprint arXiv:1611.01704, 2016.\nTajeddine Benbarrad, Salaheddine Kably, Mounir Arioua, and Nabih Alaoui. Compression-based data augmentation for cnn generalization. In International Conference on Cybersecurity, Cybercrimes, and Smart Emerging Technologies, pp. 235-244. Springer, 2022.\nYoshua Bengio, Nicholas Leonard, and Aaron Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013.Aditya Chattopadhay, Anirban Sarkar, Prantik Howlader, and Vineeth N Balasubramanian. Grad-cam++:Generalized gradient-based visual explanations for deep convolutional networks. In 2018 IEEE winter conference on applications of computer vision (WACV), pp. 839-847. IEEE, 2018.Chyi-Tsong Chen and Wei-Der Chang. A feedforward neural network with function shape autotuning.Neural networks, 9(4):627-641, 1996.\nNilaksh Das, Madhuri Shanbhogue, Shang-Tse Chen, Fred Hohman, Li Chen, Michael E. Kounavis, and Duen Horng Chau. Keeping the bad guys out: Protecting and vaccinating deep learning with jpeg compression. ArXiv, abs/1705.02900, 2017.\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248-255. Ieee,2009.\nSamuel Dodge and Lina Karam. Understanding how image quality affects deep neural networks. In 2016eighth international conference on quality of multimedia experience (QoMEX), pp. 1-6. IEEE, 2016.Gintare Karolina Dziugaite, Zoubin Ghahramani, and Daniel M. Roy. A study of the effect of JPG compression on adversarial images. CoRR, abs/1608.00853, 2016.\nSteven K Esser, Jeffrey L McKinstry, Deepika Bablani, Rathinakumar Appuswamy, and Dharmendra S Modha. Learned step size quantization. arXiv preprint arXiv:1902.08153, 2019.Chuan Guo, Mayank Rana, Moustapha Cisse, and Laurens van der Maaten. Countering adversarial images using input transformations. In International Conference on Learning Representations, 2018.Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing humanlevel performance on imagenet classification. In Proceedings of the IEEE international conference on computer vision, pp. 1026-1034, 2015.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770-778, 2016.Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2(7), 2015. Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.4700-4708, 2017.\nForrest N Iandola. Squeezenet: Alexnet-level accuracy with 50x fewer parameters andj 0.5 mb model size.arXiv preprint arXiv:1602.07360, 2016.\nAditya Khosla, Nityananda Jayadevaprakash, Bangpeng Yao, and Fei-Fei Li. Novel dataset for fine-grained image categorization: Stanford dogs. In Proc. CVPR workshop on fine-grained visual categorization (FGVC), volume 2. Citeseer, 2011.\nAlex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009..Yanyu Li, Geng Yuan, Yang Wen, Eric Hu, Georgios Evangelidis, Sergey Tulyakov, Yanzhi Wang, and Jian Ren. Efficientformer: Vision transformers at mobilenet speed. arXiv preprint arXiv:2206.01191, 2022.Zihao Liu, Tao Liu, Wujie Wen, Lei Jiang, Jie Xu, Yanzhi Wang, and Gang Quan. DeepN-JPEG: A deep neural network favorable JPEG-based image compression framework. In Proceedings of the 55th Annual Design Automation Conference, pp. 18. ACM, 2018.\nZiming Liu, Yixuan Wang, Sachin Vaidya, Fabian Ruehle, James Halverson, Marin Soljacic, Thomas Y Hou, and Max Tegmark. Kan: Kolmogorov-arnold networks. arXiv preprint arXiv:2404.19756, 2024.Xiyang Luo, Hossein Talebi, Feng Yang, Michael Elad, and Peyman Milanfar. The rate-distortion-accuracy.tradeoff: Jpeg case study. arXiv preprint arXiv:2008.00605, 2020.\nNingning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian Sun. Shufflenet v2: Practical guidelines for efficient cnn architecture design. In Proceedings of the European conference on computer vision (ECCV), pp. 116-131, 2018.\nAleksander Madry. Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083, 2017.\nMaria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number of classes. In 2008 Sixth Indian conference on computer vision, graphics & image processing, pp. 722-729.IEEE, 2008.\nOmkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and CV Jawahar. Cats and dogs. In 2012 IEEE conference on computer vision and pattern recognition, pp. 3498-3505. IEEE, 2012.Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen,Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019.William B. Pennebaker and Joan L. Mitchell. JPEG Still Image Data Compression Standard. Kluwer Academic Publishers, Norwell, MA, United States, 1992. ISBN 0442012721.Ahmed H Salamah, Shayan Mohajer Hamidi, and En-Hui Yang. A coded knowledge distillation framework for image classification based on adaptive jpeg encoding. Pattern Recognition, pp. 110966, 2024a.Ahmed H Salamah, Kaixiang Zheng, Linfeng Ye, and En-Hui Yang. Jpeg compliant compression for dnn.vision. IEEE Journal on Selected Areas in Information Theory, 2024b.\nMark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mobilenetv2:Inverted residuals and linear bottlenecks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 4510-4520, 2018. Richard Shin and Dawn Song. Jpeg-resistant adversarial images. In NIPs 2017 workshop on machine learning and computer security, volume 1, pp. 8, 2017.\nKaren Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.\nYonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive representation distillation. In International Conference on Learning Representations, 2020.\nZhengzhong Tu, Peyman Milanfar, and Hossein Talebi. Muller: Multilayer laplacian resizer for vision. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 6877-6887, 2023.A Vaswani. Attention is all you need. Advances in Neural Information Processing Systems, 2017..Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie. The caltech-ucsd birds200-2011 dataset. 2011.\nXiufeng Xie and Kyu-Han Kim. Source compression with bounded dnn perception loss for iot edge computer vision. In The 25th Annual International Conference on Mobile Computing and Networking, pp. 1-16,2019.\nXiufeng Xie, Ning Zhou, Wentao Zhu, and Ji Liu. Bandwidth-aware adaptive codec for dnn inference offloading in iot. In European Conference on Computer Vision, pp. 88-104. Springer, 2022..Zhiqiu Xu, Yanjie Chen, Kirill Vishniakov, Yida Yin, Zhiqiang Shen, Trevor Darrell, Lingjie Liu, and Zhuang Liu. Initializing models with larger ones. In The Twelfth International Conference on Learning Representations, 2023.\nEn-Hui Yang and Shayan Mohajer Hamidi. Systems and methods for training deep learning models. 2024.En-Hui Yang, Hossam Amer, and Yanbing Jiang. Compression helps deep learning in image classification.Entropy, 23(7):881, 2021.\nSukmin Yun, Jongjin Park, Kimin Lee, and Jinwoo Shin. Regularizing class-wise predictions via selfknowledge distillation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 13876-13885, 2020.\nHongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. arXiv preprint arXiv:1710.09412, 2017.\nKaixiang Zheng, Ahmed H. Salamah, Linfeng Ye, and En-Hui Yang. Jpeg compliant compression for dnn vision. In 2023 IEEE International Conference on Image Processing (ICIP), pp. 1875-1879, 2023. doi:10.1109/ICIP49359.2023.10221982.",
    "Image_paths": [],
    "Image_captions": [],
    "Tables": [],
    "Table_captions": [],
    "Equations": {}
  },
  "Section 13": {
    "Text": "A APPENDIX\n",
    "Image_paths": [],
    "Image_captions": [],
    "Tables": [],
    "Table_captions": [],
    "Equations": {}
  },
  "Section 14": {
    "Text": "A.1PARTIAL DERIVATIVES OFS $Q_{d}$\n With the CPMF $P_{\\alpha}(\\cdot|z)$ shown in (6), z is now quantized to each.$\\hat{z}\\in\\hat{A}$ with probability $P_{\\alpha}(\\hat{z}|z)$ . Denote this random mapping by. [EQ_0] Note that given $z,\\hat{z}$  is a random variable taking values in $\\hat{\\mathcal A $ with distribution $P_{\\alpha}(\\cdot|z)$\nA.1.1 QUANTIZATION STEP AND INPUT: \n The partial derivatives of.$Q_{d}(z)$ w.r.t. q and z are obtained as. [EQ_1] where for any random variable $V$ [EQ_2]\nA.1.2SCALING FACTOR Q: \n The partial derivatives of.$Q_{d}(Z)$ I w.r.t.  obtained as [EQ_3] [EQ_4] Plugging (2) in (1) yields [EQ_5] To gain a better understanding, we fix q and $b,$  and analyzed how $\\frac{\\partial Q_{d}(z)}{\\partial\\alpha}$ behaves for different values of Q. From Fig. 7, it is evident that the gradient magnitude decreases as a increases. This indicates that $Q_{d}$ approaches the shape of the uniform quantizer $Q_{u},$  . as shown previously in Fig. 2, leading to a reduction in.the gradient magnitude for a. As a result, when a becomes sufficiently large, the gradient approaches zero,effectively preventing further updates to a, making it act as a non-trainable parameter at a certain point of the training process. Based on this observation and verified experimental results, we opt not to train a for simple image classification tasks like CIFAR-100 and fine-grained datasets, as it does not impact the overall performance.",
    "Image_paths": [
      "D:\\Workspace_2\\DOC_QA\\app\\docs\\pdf_output\\2410.07081v3\\imgs\\img_in_chart_box_858_895_1687_1348.jpg"
    ],
    "Image_captions": [
      "Figure 7: This Figure illustrates the partial derivatives of $Q_{d}(z)$  w.r.t. the scaling factor a, where $b=3$ and  $q=1$"
    ],
    "Tables": [],
    "Table_captions": [],
    "Equations": {
      "[EQ_0]": "\\hat{z}=Q_{\\mathrm{p}}(z).",
      "[EQ_1]": "\\begin{aligned}{\\frac{\\partial Q_{d}(z)}{\\partial z}}&{{}=2\\alpha\\mathrm{V a r}\\bigl\\{Q_{p}(z)\\bigr\\},}\\\\ {\\frac{\\partial Q_{d}(z)}{\\partial q}}&{{}=\\frac{1}{q}\\Big(\\mathbb{E}\\bigl\\{Q_{p}(z)\\bigr\\}+(2\\alpha z)\\mathrm{V a r}\\bigl\\{Q_{p}(z)\\bigr\\}}\\\\ {}&{{}\\quad-(2\\alpha)\\mathrm{S k e w}_{\\mathrm{u}}\\bigl\\{Q_{p}(z)\\bigr\\}\\Big),}\\\\ \\end{aligned}1",
      "[EQ_2]": "\\operatorname{S k e w}_{\\mathtt{u}}(V)\\overset{\\Delta}{=}\\sum_{v}v^{3}\\mathbb{P}_{V}(v)-\\Big(\\sum_{v}v\\mathbb{P}_{V}(v)\\Big)\\Big(\\sum_{v}v^{2}\\mathbb{P}_{V}(v)\\Big).",
      "[EQ_3]": "\\begin{aligned}{\\frac{\\partial Q_{d}(z)}{\\partial\\alpha}}&{{}=\\frac{\\partial\\sum_{i\\in\\mathcal{A}}i q P_{\\alpha}(i q|z)}{\\partial\\alpha}}\\\\ {}&{{}=\\sum_{i\\in\\mathcal{A}}i q\\frac{\\partial P_{\\alpha}(i q|z)}{\\partial\\alpha}}\\\\ \\end{aligned}",
      "[EQ_4]": "\\begin{aligned}{\\frac{\\partial P_{\\alpha}(i q|z)}{\\partial\\alpha}}&{{}=\\frac{-(z-i q)^{2}e^{-\\alpha(z-i q)^{2}}}{\\sum_{j\\in\\mathcal{A}}e^{-\\alpha(z-j q)^{2}}}+\\frac{e^{-\\alpha(z-i q)^{2}}\\sum_{j\\in\\mathcal{A}}(z-j q)^{2}e^{-\\alpha(z-j q)^{2}}}{(\\sum_{j\\in\\mathcal{A}}e^{-\\alpha(z-j q)^{2}})^{2}}}\\\\ {}&{{}=P_{\\alpha}(i q|z)\\sum_{j\\in\\mathcal{A}}(z-j q)^{2}P_{\\alpha}(j q|z)-(z-i q)^{2}P_{\\alpha}(i q|z)}\\\\ \\end{aligned}",
      "[EQ_5]": "\\begin{aligned}{\\frac{\\partial Q_{d}(z)}{\\partial\\alpha}}&{{}=\\sum_{i\\in\\mathcal{A}}i q P_{\\alpha}(i q|z)\\sum_{j\\in\\mathcal{A}}(z-j q)^{2}P_{\\alpha}(j q|z)-\\sum_{i\\in\\mathcal{A}}i q(z-i q)^{2}P_{\\alpha}(i q|z)}\\\\ {}&{{}=\\mathbb{E}\\{Q_{p}(z)\\}\\mathbb{E}\\{(z-Q_{p}(z))^{2}\\}-\\mathbb{E}\\{Q_{p}(z)(z-Q_{p}(z))^{2}\\}}\\\\ {}&{{}=-C o v\\{Q_{p}(z),(z-Q_{p}(z))^{2}\\}}\\\\ \\end{aligned}"
    }
  },
  "Section 15": {
    "Text": "A.2  GRADIENT SCALING\n Figures 8a and 8b demonstrate the impact of varying a and q on the gradient magnitude of $Q_{d}(z)$ with respect to q. As observed, decreasing the value of q while keeping a constant leads to a decrease in the gradient magnitude. Similarly, decreasing the value of a while maintaining a fixed q also results in a reduction of the gradient magnitude. To address the instability in updating the trainable parameters $Q$ , we propose utilizing  to regulate the magnitude of $\\frac{\\partial Q_{d}(z)}{\\partial q}$ , as illustrated in Figure 8b. By leveraging this control mechanism.of adjusting a, we can stabilize the magnitude of the gradients that update $q_{1}$ .This behavior suggests a relationship between q and a in controlling the gradient magnitude of $Q_{d}(z)$ To explore the relationship between a and $q,$  we refer to the exponent in (5),$\\alpha(z-i q)^{2}$ , which can be rewritten by expressing $z=c q$ , resulting in the form $\\alpha q^{2}(c-i)^{2}$ . From this, we define a new term,.$\\hbar=\\alpha q^{2}$ referred to as the Gradient Scaling Constant. To further illustrate this relationship, in Fig. 9, we set $\\hbar=2$ and by selecting different pairs of a and q values, we demonstrate that the maximum magnitude of $\\frac{\\partial Q\\_{d}(z)}{\\partial q}$ remains invariant. This confirms that the gradient magnitude can be effectively controlled by adjusting a based on the last updated value of $q,$  according to the specified h value. This adjustment allows for controlled quantization updates, reducing the potential instability during training. Moreover, by controlling the gradient magnitude, we simplify the optimization process, enabling the use of a single learning rate for all q values by using the SGD optimizer, instead of the ADAM optimizer. This gradient scaling mechanism is analogous to the ADAM optimizer, which adapts different learning rates for individual trainable parameters based on momentum and recent gradient magnitudes.",
    "Image_paths": [
      "D:\\Workspace_2\\DOC_QA\\app\\docs\\pdf_output\\2410.07081v3\\imgs\\img_in_chart_box_409_453_2134_1003.jpg",
      "D:\\Workspace_2\\DOC_QA\\app\\docs\\pdf_output\\2410.07081v3\\imgs\\img_in_chart_box_775_1271_1770_1820.jpg"
    ],
    "Image_captions": [
      "Figure 8: Figures 8a and ?? illustrate the partial derivatives of.$Q_{d}(z)$ I w.r.t. the parameter q for cases where.$\\alpha=8$  and q varies, and a varies and $q=2,$ , respectively. For both figures, we set $b=6$",
      "Figure 9: Demonstrating impact of h on gradient magnitude for various combinations of q and a. We fixed $\\hbar{=}2$  and $b{=}6$ '."
    ],
    "Tables": [],
    "Table_captions": [],
    "Equations": {}
  },
  "Section 16": {
    "Text": "A.3CNN-BASED ARCHITECTURES SETTING\n For CIFAR100, we deploy a stochastic gradient descent (SGD) optimizer with a momentum of 0.9, a weight decay of O.o005, and a batch size of 64. We initialize the learning rate as O.05, and decay it by 0.1 every 30 epochs after the first 150 epochs until the last 240 epoch. For MobileNetV2, ShuffleNetV1 and ShuffleNetV2, we use a learning rate of O.01 as this learning rate is optimal for these models in a grid search,while 0.05 is optimal for other models. For fine-grained classification tasks, all networks are trained from scratch and optimized by SGD with a momentum of O.9, weight decay of O.o001, and an initial learning rate of 0.1,. The learning rate is divided by 10 after epochs 100 and 150 for all datasets, and the total epochs are 200. We set batch size 32 for these fine-grained classification tasks. We use the standard data augmentation technique for ImageNet (Deng et al.,2009), i.e., flipping and random cropping. This experimental setup is also outlined by Yun et al. (2020)..",
    "Image_paths": [],
    "Image_captions": [],
    "Tables": [],
    "Table_captions": [],
    "Equations": {}
  },
  "Section 17": {
    "Text": "A.4 FINE-GRAINED MODEL ARCHITECTURES\n We use standard ResNet-18 with 64 filters and DenseNet-121 with a growth rate of 32 for image size.$224\\times$ 224. For fine-grained classification tasks, we use PreAct ResNet-18 (He et al., 2016), which modifies the first convolutional layer with kernel size.$3\\times3$ , strides 1 and padding 1, instead of the kernel size.$7\\times7$ 9strides 2 and padding 3, for image size $32\\times32$ by following Zhang et al. (2017). We use DenseNet-BC structure (Huang et al., 2017), and the first convolution layer of the network is also modified in the same way as in PreAct ResNet-18 for image size $32\\times32$",
    "Image_paths": [],
    "Image_captions": [],
    "Tables": [],
    "Table_captions": [],
    "Equations": {}
  },
  "Section 18": {
    "Text": "A.5 TRANSFORMER-BASED SETTINGS AND RESULTS\n In this section, we compare the performance of JPEG-DL compared to its baseline using the EfficientFormer-.L1 model (Li et al., 2022) on CIFAR-100, as well as two fine-grained datasets, Flowers and Pets. We followed the experimental setup described by Xu et al. (2023), adhering to the same configurations mentioned in Table 4. The learning rate was set to 0.003 for CIFAR-100 and 0.005 for the fine-grained tasks, aligning.with our standard settings in Section 4. Interestingly, we found that the best performance for transformer-based architectures was achieved when the quantization tables were initialized with all ones, effectively representing the highest quality factor quanti-.zation table for JPEG. The top-1 validation accuracy performance is presented in Table 5, demonstrating the significant improvements achieved by JPEG-DL over the baseline.",
    "Image_paths": [],
    "Image_captions": [],
    "Tables": [
      [
        {
          "Settings": "batch size",
          "CIFAR-100": "512",
          "Pets": "512",
          "Flowers": "512"
        },
        {
          "Settings": "warmup epochs",
          "CIFAR-100": "50",
          "Pets": "100",
          "Flowers": "100"
        },
        {
          "Settings": "training epochs",
          "CIFAR-100": "300",
          "Pets": "600",
          "Flowers": "600"
        }
      ],
      [
        {
          "Model": "CIFAR-100",
          "Method": "Baseline JPEG-DL",
          "Accuracy (%)": "80.270.33 80.490.23(+0.22)"
        },
        {
          "Model": "Flowers",
          "Method": "Baseline JPEG-DL",
          "Accuracy (%)": "73.050.12 (+3.27) $69.78_{\\pm0.33}$"
        },
        {
          "Model": "Pets",
          "Method": "Baseline JPEG-DL",
          "Accuracy (%)": "65.521.23 69.280.42(+3.76)"
        }
      ]
    ],
    "Table_captions": [
      "Table 4: Hyper-parameter setting on EfficientFormer-L1..",
      "Table 5: Top-1 accuracy (%) on CIFAR-100 and two fine-grained image classification tasks using EfficientFormer-L1. We report the mean and standard deviation of the experimental results over three runs with different random seeds. The best results are indicated in bold, and values in parentheses indicate relative accuracy gains over the baseline."
    ],
    "Equations": {}
  },
  "Section 19": {
    "Text": "A.6 COMPARISON WITH MORE BASELINES\n In Table 6, we have included comparisons against other baselines that employed JPEG quantization in the pipeline, but with heuristic approaches to handle the non-differentiability and zero derivative problems of JPEG quantization. For example, Balle et al. (2016) allow optimization via stochastic gradient descent by.replacing the quantizer with an additive i.i.d. uniform noise, which has the same width as the quantization bins, where $\\hat{z}=z+q*U(-0.5,0.5)$ '. However, during validation, they employed $Q_{u}$  using the trained quantization tables. Shin & Song (20i7) employ a third-order polynomial approximation of the rounding function to make JPEG differentiable. The gradient approximation through the round function is a key aspect of certain neural network approaches. Esser et al. (2019)' employ a straight-through estimator, originally proposed by Bengio et al. (2013), to achieve this approximation. This method treats the round function as a pass-through operation during backpropagation, allowing for effective gradient estimation. Our experimental results demonstrate that we consistently outperform all tested baselines. Even though these methods find a way to make the quantization differentiable, they still hurt the performance. This is consistent with the well-known wisdom in the literature, which also shows that they did not take advantage of the higher level of non-linearity introduced into DNN architectures, as proposed by our JPEG-DL framework.",
    "Image_paths": [],
    "Image_captions": [],
    "Tables": [
      [
        {
          "0": "Model",
          "1": "Method",
          "2": "CUB-200",
          "3": "CUB-200",
          "4": "Dogs",
          "5": "Dogs",
          "6": "Flowers",
          "7": "Flowers",
          "8": "Pets",
          "9": "Pets"
        },
        {
          "0": "ResNet-18",
          "1": "Baseline Balle et al. (2016) Shin & Song (2017)",
          "2": "54.001.43",
          "3": "54.001.43",
          "4": "63.710.32",
          "5": "63.710.32",
          "6": "57.131.28",
          "7": "57.131.28",
          "8": "70.370.84",
          "9": "70.370.84"
        },
        {
          "0": "ResNet-18",
          "1": "Baseline Balle et al. (2016) Shin & Song (2017)",
          "2": "50.782.21 1(-3.22)",
          "3": "53.477.37. 63.030.56",
          "4": "(-10.24) (-0.68)",
          "5": "55.460.59",
          "6": "(-1.67)",
          "7": "56.1417.16",
          "8": "(-14.23)",
          "9": ""
        },
        {
          "0": "ResNet-18",
          "1": "Baseline Balle et al. (2016) Shin & Song (2017)",
          "2": "55.340.14 51.580.18",
          "3": "(+1.34) (-2.42)",
          "4": "60.450.23",
          "5": "",
          "6": "55.781.44 58.040.58",
          "7": "(-1.35) (+0.91)",
          "8": "71.451.01 68.810.55",
          "9": "(+1.08)"
        },
        {
          "0": "DenseNet-121",
          "1": "Esser et al. (2019) JPEG-DL",
          "2": "58.810.12 (+4.81)",
          "3": "65.570.37",
          "4": "(-3.26) (+1.86)",
          "5": "68.760.57",
          "6": "",
          "7": "(+11.63)",
          "8": "74.840.66",
          "9": "(-1.56) (+4.47)"
        },
        {
          "0": "DenseNet-121",
          "1": "",
          "2": "57.700.44",
          "3": "",
          "4": "",
          "5": "",
          "6": "",
          "7": "",
          "8": "",
          "9": ""
        },
        {
          "0": "DenseNet-121",
          "1": "Baseline",
          "2": "52.001.41",
          "3": "66.610.17",
          "4": "",
          "5": "",
          "6": "51.320.57",
          "7": "",
          "8": "70.260.79",
          "9": ""
        },
        {
          "0": "DenseNet-121",
          "1": "Balle et al. (2016)",
          "2": "(-5.70)",
          "3": "60.076.41",
          "4": "(-6.54)",
          "5": "",
          "6": "46.602.87",
          "7": "(-4.72)",
          "8": "61.911.88",
          "9": "(-8.35)"
        },
        {
          "0": "",
          "1": "Shin & Song (2017)",
          "2": "57.190.78 (-0.51)",
          "3": "66.900.13",
          "4": "",
          "5": "(+0.29)",
          "6": "51.040.87",
          "7": "(-0.28)",
          "8": "69.951.21",
          "9": "(-0.31)"
        },
        {
          "0": "",
          "1": "Esser et al. (2019)",
          "2": "56.460.30 (-1.24)",
          "3": "",
          "4": "64.890.12",
          "5": "(-1.72)",
          "6": "55.980.24",
          "7": "(+4.60)",
          "8": "69.580.59",
          "9": "(-0.68)"
        },
        {
          "0": "",
          "1": "JPEG-DL",
          "2": "61.320.43 (+3.62)",
          "3": "69.670.58",
          "4": "(+3.06)",
          "5": "",
          "6": "72.221.05",
          "7": "(+20.90)",
          "8": "75.900.68",
          "9": "(+5.64)"
        }
      ]
    ],
    "Table_captions": [
      "Table 6: Top-1 validation accuracy (%) on various fine-grained image classification tasks and model architectures. We report the mean and standard deviation of experimental results over three runs."
    ],
    "Equations": {}
  },
  "Section 20": {
    "Text": "A.7LAYER REPLACEMENT\n We extend our evaluation of the JPEG-DL framework to include layer replacement, where we substitute the ReLU activation function with our JPEG layer in ResNet-18. This replacement is implemented directly after the first convolution layer, which has 64 kernels and outputs a feature map of 112x112. For this layer replacement, we did not perform any color space conversion and added a quantization table with a size of 8x8 for each kernel output. We follow the experimental setup mentioned in Section 4, except we use a learning rate of O.001,  equal to 2, and b equal to 6 bits. The performance of this approach is shown in the last row of the following table and is compared to the performance of JPEG-DL when the JPEG layer is placed directly after the input layer shown in Fig. 1. The results shown in Table 7 confirm that this new architecture, resulting from layer replacement with the JPEG layer or inserting the JPEG layer directly after the Input Layer, indeed improves deep learning performance by a significant margin, thereby showing the potential of new forms of nonlinear operation in DNN architectures.",
    "Image_paths": [],
    "Image_captions": [],
    "Tables": [],
    "Table_captions": [],
    "Equations": {}
  },
  "Section 21": {
    "Text": "A.8DURRREEENNENG DOTNR PNR DARNRRNNNEND PRRC PIRRN-NTND HOEG PRRRERIREG METHODS\n JPEG-based Data Augmentation. We have compared and implemented JPEG-based data augmentation across three different sets, each with varying ranges of quantity factor (QF). For each tested range, we randomly select a QF for each image within the mini-batch. These sets have been tested on fine-grained datasets for all the models evaluated. Table 8 shows corresponding results for these tested sets. Consistent with the common knowledge in the literature, JPEG-based data augmentation in general degrades the accuracy performance. Non-Learnable and Learnable Preprocessing Methods. We have tested the performance of non-learnable.and learnable preprocessing methods during training and validation across all tested models on all tested finegrained datasets, as demonstrated in Table 8. For non-learnable preprocessing, we have considered applying denoising using a Gaussian kernel and histogram equalization for each image within a mini-batch. As for the learnable one, we have compared it with Tu et al. (2023) as a learnable resize module, which has a bandpass nature in that it learns to boost details in certain frequency subbands that benefit the downstream recognition models. We have applied the same setup mentioned in their paper, in which they fine-tuned the model to achieve some improvement 2. Impact of chrominance subsampling on JPEG-DL. In Table 8, we evaluate various subsampling schemes for all tested models across all tested fine-grained datasets. Notably, although there is no clear winner among different chroma subsampling methods, DNNs indeed respond to color information differently from human; color information is more important to DNNs than human since chroma subsampling formats 4:4:4and 4:2:2, in general, give rise to better accuracy performance than the 4:2:0 subsampling format which is.adopted predominantly in the image and video coding.",
    "Image_paths": [],
    "Image_captions": [],
    "Tables": [
      [
        {
          "Method": "JPEG-DL (Input Layer) JPEG-DL (1st Conv Layer)",
          "CUB-200": "58.810.12 (+4.81) 59.270.04 (+5.27)",
          "Dogs": "65.570.37 (+1.86) 65.330.07 (+1.62)",
          "- Flowers": "68.760.57(+11.63) 72.101.46 (+14.97)",
          "Pets": "74.840.66 (+4.47) 76.110.37 (+5.74)"
        }
      ]
    ],
    "Table_captions": [
      "Table 7: Top-1 validation accuracy (%) on various fine-grained image classification tasks and model architectures. We report the mean and standard deviation of experimental results over three runs.."
    ],
    "Equations": {}
  },
  "Section 22": {
    "Text": "A.9 FIXING HYPERPARAMTERS ACROSS DIFFERENT DATASETS\n In Table 9, we evaluate the difference between the hyperparameters used for CIFAR-100 and the fine-grained task, where the only difference lies in the learning rate. In the following setup, we present the results when a learning rate of O.003 is applied to the fine-grained dataset. We observe that the performance difference is marginal, and in some cases, we can even achieve higher gains in model performance. This suggests that the choice of learning rate can have a significant impact on the effectiveness of the model..",
    "Image_paths": [],
    "Image_captions": [],
    "Tables": [],
    "Table_captions": [],
    "Equations": {}
  },
  "Section 23": {
    "Text": "A.1O SENSITIVITY AND Q TABLE INITIALIZATION\n Figure 10 presents the estimated sensitivity values for all models considered in the CUB200 dataset and a subset of models used for CIFAR-100, as proposed by Salamah et al. (2024b); Zheng et al. (2023). These estimated sensitivity values are used to initialize the quantization table for JPEG-DL. Figure 11 illustrates the initial and converged values of the quantization table at the end of training.. (a) ResNet18 (b) DenseNet-121 (d) Res56 (e) ShuffleNetV2 (c) VGG13",
    "Image_paths": [
      "D:\\Workspace_2\\DOC_QA\\app\\docs\\pdf_output\\2410.07081v3\\imgs\\img_in_chart_box_649_900_1269_1352.jpg",
      "D:\\Workspace_2\\DOC_QA\\app\\docs\\pdf_output\\2410.07081v3\\imgs\\img_in_chart_box_1278_903_1894_1349.jpg",
      "D:\\Workspace_2\\DOC_QA\\app\\docs\\pdf_output\\2410.07081v3\\imgs\\img_in_chart_box_1008_1483_1604_1897.jpg",
      "D:\\Workspace_2\\DOC_QA\\app\\docs\\pdf_output\\2410.07081v3\\imgs\\img_in_chart_box_1658_1483_2223_1898.jpg",
      "D:\\Workspace_2\\DOC_QA\\app\\docs\\pdf_output\\2410.07081v3\\imgs\\img_in_chart_box_408_1483_975_1900.jpg"
    ],
    "Image_captions": [
      "Figure 10: The estimated sensitivity is shown for two pre-trained models on the CUB200 (first row) and three pre-trained models on the CIFAR-100 (second row), with sensitivity indices arranged in the default zigzag order."
    ],
    "Tables": [
      [
        {
          "0": "Dataset",
          "1": "Method Baseline",
          "2": "ResNet-18",
          "3": "DenseNet-121 57.700.44 55.720.72 (-1.98)"
        },
        {
          "0": "CUB-200",
          "1": "54.001.43 52.860.69 (-1.14)",
          "2": "Rand. QF [1:50] Rand. QF [50:100] Rand. QF [1:100]",
          "3": ""
        },
        {
          "0": "CUB-200",
          "1": "54.530.41 (+0.53) 53.910.43 (-0.09)",
          "2": "56.980.90 (-0.72) 57.200.33 (-0.50) 56.021.00 (-1.68)",
          "3": "Denoising Equalization"
        },
        {
          "0": "CUB-200",
          "1": "54.320.57",
          "2": "(-3.38)",
          "3": ""
        },
        {
          "0": "CUB-200",
          "1": "Learnable Resize",
          "2": "57.350.22 (-0.35) 60.550.71 (+2.85)",
          "3": "JPEG-DL (4:2:0) JPEG-DL (4:2:2) JPEG-DL (4:4:4)"
        },
        {
          "0": "61.510.41 (+3.81) 61.320.43 (+3.62)",
          "1": "",
          "2": "",
          "3": ""
        },
        {
          "0": "61.510.41 (+3.81) 61.320.43 (+3.62)",
          "1": "Dogs",
          "2": "Baseline",
          "3": "63.710.32"
        },
        {
          "0": "61.510.41 (+3.81) 61.320.43 (+3.62)",
          "1": "Rand. QF [1:50] Rand. QF [50:100] Rand. QF [1:100]",
          "2": "60.610.17 (-3.10) 63.140.30 (-0.57) 62.120.16 (-1.59)",
          "3": "64.930.17 (-1.68) 66.970.18 (+0.36) 65.590.54 (-1.02)"
        },
        {
          "0": "61.510.41 (+3.81) 61.320.43 (+3.62)",
          "1": "Denoising Equalization",
          "2": "62.520.41 62.380.29",
          "3": "(-1.19) (-1.33)"
        },
        {
          "0": "61.510.41 (+3.81) 61.320.43 (+3.62)",
          "1": "Learnable Resize",
          "2": "63.100.43 (-0.61)",
          "3": "67.900.32 (+1.29)"
        },
        {
          "0": "61.510.41 (+3.81) 61.320.43 (+3.62)",
          "1": "JPEG-DL (4:2:0) JPEG-DL (4:2:2) JPEG-DL (4:4:4)",
          "2": "65.570.31 (+1.86) 65.640.16 (+1.93) 65.570.37 (+1.86)",
          "3": "68.900.08 (+2.29) 69.850.78 (+3.24) 69.670.58 (+3.06)"
        },
        {
          "0": "Flowers",
          "1": "Baseline",
          "2": "57.131.28",
          "3": "51.320.57"
        },
        {
          "0": "Flowers",
          "1": "Rand. QF [1:50] Rand. QF [50:100] Rand. QF [1:100]",
          "2": "58.330.48 (+1.20) 55.980.89 (-1.15) 57.550.66 (+0.42)",
          "3": "51.440.79 (+0.12) 51.671.10 (+0.35) 52.320.73 (+1.00)"
        },
        {
          "0": "Flowers",
          "1": "Denoising Equalization",
          "2": "57.281.06 (+0.15) 60.560.98 (+3.43)",
          "3": "50.370.79 (-0.95) 61.720.21 (+10.40)"
        },
        {
          "0": "Flowers",
          "1": "Learnable Resize",
          "2": "57.410.48 (+0.28)",
          "3": "51.210.82 (-0.11)"
        },
        {
          "0": "Flowers",
          "1": "68.011.17 (+16.69) 68.791.08 (+17.47) 72.221.05 (+20.90)",
          "2": "",
          "3": ""
        },
        {
          "0": "Flowers",
          "1": "JPEG-DL (4:2:0) JPEG-DL (4:2:2) JPEG-DL (4:4:4)",
          "2": "67.581.50 67.751.19",
          "3": "(+10.45) (+10.62)"
        },
        {
          "0": "Pets",
          "1": "Baseline",
          "2": "70.370.84",
          "3": "70.260.79"
        },
        {
          "0": "Pets",
          "1": "Rand. QF [1:50] Rand. QF [50:100] Rand. QF [1:100]",
          "2": "69.710.54 (-0.66) 70.000.57 (-0.37) 69.520.62 (-0.85)",
          "3": "68.930.50 (-1.33) 68.630.82 (-1.63) 70.601.05 (+0.34)"
        },
        {
          "0": "Pets",
          "1": "Denoising Equalization",
          "2": "69.151.07 (-1.22) 73.000.79 (+2.63)",
          "3": "69.031.13 (-1.23)"
        },
        {
          "0": "Pets",
          "1": "75.041.30",
          "2": "(+4.78)",
          "3": ""
        },
        {
          "0": "Pets",
          "1": "Learnable Resize",
          "2": "70.040.72 (-0.33)",
          "3": "68.831.85 (-1.43)"
        },
        {
          "0": "Pets",
          "1": "JPEG-DL (4:2:0) JPEG-DL (4:2:2) JPEG-DL (4:4:4)",
          "2": "74.641.34 (+4.27) 74.810.51 (+4.44) 74.840.66 (+4.47)",
          "3": "76.210.35 (+5.95) 76.171.83 (+5.90) 75.900.68 (+5.64)"
        }
      ],
      [
        {
          "Model": "ResNet-18",
          "Learning Rate": "0.005 0.003",
          "CUB-200": "58.810.12 (+4.81) 58.620.50 (+4.61)",
          "- Dogs": "65.570.37 (+1.86) 65.450.17 (+1.74)",
          "- Flowers": "68.760.57 (+11.63) 69.611.16 (+12.48)",
          "Pets": "74.840.66 (+4.47) 74.900.82 (+4.53)"
        },
        {
          "Model": "DenseNet-121",
          "Learning Rate": "0.005 0.003",
          "CUB-200": "61.320.43 (+3.62) 60.920.50 (+3.22)",
          "- Dogs": "69.670.58 (+3.06) 69.530.49 (+2.92)",
          "- Flowers": "72.221.05 (+20.90) 72.450.92 (+21.13)",
          "Pets": "75.900.68(+5.64) 75.830.64(+5.56)"
        }
      ]
    ],
    "Table_captions": [
      "Table 8: JPEG-DL is compared to JPEG-based augmentation (orange), non-learnable (yellow), and learnable.preprocessing (blue) methods. JPEG-DL employs three different chrominance subsampling schemes shown.in green color. Top-1 validation accuracy (%) on various fine-grained image classification tasks and model architectures. We report the mean and standard deviation of experimental results over three runs. The best.results are indicated in bold, and values in parentheses indicate relative accuracy gains over the baseline.",
      "Table 9: Top-1 validation accuracy (%) on various fine-grained image classification tasks and model architectures. We report the mean and standard deviation of experimental results over three runs.."
    ],
    "Equations": {}
  },
  "Section 24": {
    "Text": "A.11FEATURE MAPS VISUALIZATION\n Figure 12 presents the feature maps extracted after the first dense block in DenseNet-121 for both the JPEGDL and the baseline models, trained on the Flowers dataset using the model from Table 2. The shown.example was incorrectly classified by the baseline model, while the JPEG-DL model correctly classified it..",
    "Image_paths": [
      "D:\\Workspace_2\\DOC_QA\\app\\docs\\pdf_output\\2410.07081v3\\imgs\\img_in_chart_box_650_431_1897_1510.jpg"
    ],
    "Image_captions": [
      "Figure 11: Initial and final quantization tables for Res56 trained on CIFAR-100 and DenseNet-121 trained on CUB200."
    ],
    "Tables": [],
    "Table_captions": [],
    "Equations": {}
  },
  "Section 25": {
    "Text": "A.12MITIGATING THE INFERENCE OVERHEAD OF DIFFERENTIABLE SOFT QUANTIZERS\n The reconstruction space $\\hat{\\mathcal A $ is configured with a specific size, determined by the parameter L, which is set to $2^{b-1}$ :, as mentioned in Section 4. For CIFAR-100 and Fine-grained datasets, b is set to 8 bits, resulting in a reconstruction space length of 513. For ImageNet-1K, b is set to 11 bits, resulting in a length of 2047.This high dimensionality of the reconstruction space poses a computational challenge during inference when calculating the conditional probability mass function (CPMF)$P_{\\alpha}(\\cdot|z)$ I. To address this, the support of each CPMF is restricted to the five closest points in $\\hat{\\mathcal A $ to the coefficient z being quantized. This simplification,referred to as the masked CPMF, significantly reduces computational complexity without compromising the effectiveness of the quantization scheme $\\mathcal{Q}_{d}$ Figure 13 demonstrates the rapid decay of probability mass in CPMFs as the reconstruction level  deviates from the quantized value the uniform quantizer.$\\mathcal{Q}_{u}(z)$ '. The set $\\mathcal{M},$ , consisting of five reconstruction levels.centered around $\\mathcal{Q}_{u}(z)$ , captures a significant portion of the total probability mass, even in extreme cases where the quantization step q and the parameter a are very small. This observation suggests that restricting the CPMF support to $\\mathcal{M}$ (masked CPMF) is a valid simplification. Instead of computing the full-space CPMF with length $2L+1$ ., we can efficiently calculate the masked CPMF with length 5 using softmax on $\\mathcal{M}$ . The impact of this simplification on the quantization scheme.$\\mathcal{Q}_{d}$ is negligible, particularly for high values of a, as shown in Table 10. Therefore, using the masked CPMF in our implementation is a justified simplification that significantly reduces computational complexity without compromising accuracy. Specifically, Fig. 13 illustrates how fast the probability mass of a reconstruction level  decays in CPMFs as  deviates from the uniform quantizer.$\\bar{\\mathcal{Q}}_{u}(z)$ '.It's clear that the set $\\mathcal{M}\\;=\\;\\{\\mathcal{Q}_{u}(z)\\ -2q,\\mathcal{Q}_{u}(z)\\;-$ $q,\\mathcal{Q}_{u}(z),\\mathcal{Q}_{u}(z)+q,\\mathcal{Q}_{u}(z)+2q\\}$ : of reconstruction levels retain a total probability mass close to 1, even. in an adversarially chosen scenario where both q and a are extremely small3 (i.e., 1) and the coefficient.C being quantized lies exactly at a quantization threshold. Therefore, instead of computing the full-space.CPMF with length $2L+1$ following (6), we only conduct softmax on M to get the masked CPMF with.length 5. The error in $\\mathcal{Q}_{d}$ caused by this masking is negligible as shown in Table 10, especially in the.high  case. Therefore, it's totally justified to simplify the full-space CPMF to the masked CPMF in our.implementation. Table 11 presents a comparison of inference time and throughput between the standard model and our unified.model, which incorporates the JPEG layer with the underlying model. The inference times for both the full-.space CPMF and the masked CPMF are measured. The reported inference time is averaged across the entire.validation set of ImageNet using Resnet18 from Table 3. These results demonstrate the effectiveness of the proposed approach in addressing the computational complexity of the JPEG-DL framework.",
    "Image_paths": [
      "D:\\Workspace_2\\DOC_QA\\app\\docs\\pdf_output\\2410.07081v3\\imgs\\img_in_image_box_529_504_2018_1165.jpg",
      "D:\\Workspace_2\\DOC_QA\\app\\docs\\pdf_output\\2410.07081v3\\imgs\\img_in_chart_box_651_433_1886_1552.jpg"
    ],
    "Image_captions": [
      "Figure 12: Feature maps of size 56x56 are shown after the first dense block in DenseNet-121 for both JPEGDL and baseline models in Figs. 12a and 12c, respectively, using an original input shown in Fig. 12b. The JPEG-DL model highlights the foreground (flower) more distinctly, while the baseline model shows less contrast, contributing to its misclassification.",
      "Figure 13: Partial visualization of CPMFs.$P_{\\alpha}(\\cdot|z=0.5)$ : computed using (6), with $L=1023,q=1$ , and a values of (a) 1, (b) 3, (c) 5, and (d) 10. The reconstruction levels between two red dashed lines represent M.."
    ],
    "Tables": [
      [
        {
          "0": "o",
          "1": "$\\mathcal{Q}_{d}(z=0.5)$",
          "2": "$\\mathcal{Q}_{d}(z=0.5)$"
        },
        {
          "0": "o",
          "1": "Full Space",
          "2": "Masked $\\Delta$"
        },
        {
          "0": "1",
          "1": "0.5",
          "2": "0.5027 0.0027"
        },
        {
          "0": "3",
          "1": "0.5",
          "2": "0.5 5.96e-08"
        },
        {
          "0": "5",
          "1": "0.5",
          "2": "0.5 5.96e-08"
        },
        {
          "0": "10",
          "1": "0.5",
          "2": "0.5 0"
        }
      ],
      [
        {
          "Inference Settings": "Baseline",
          "Inference time (milliseconds)": "5.46",
          "Inference Throughput (img/sec)": "180.65"
        },
        {
          "Inference Settings": "JPEG-DL (Masked CPMFs)",
          "Inference time (milliseconds)": "7.43",
          "Inference Throughput (img/sec)": "133.79"
        },
        {
          "Inference Settings": "JPEG-DL (Full Space)",
          "Inference time (milliseconds)": "18.86",
          "Inference Throughput (img/sec)": "52.37"
        }
      ]
    ],
    "Table_captions": [
      "Table 10:$\\mathcal{Q}_{d}(z=.55;q=1,\\alpha)$ values resulting from the full-space CPMF and the masked CPMF at different $\\alpha,$ alongside the absolute difference  between the full-space and masked cases.",
      "Table 11: Comparison of inference time between different inference settings measured for Resnet18 on ImageNet, following the experimental setup mentioned in Section 4. Evaluation is conducted on a single NVIDIA RTX A5000 GPU."
    ],
    "Equations": {}
  },
  "Summary": "This research introduces JPEG-DL, a novel deep learning framework integrating learnable JPEG quantization as a nonlinear operation within neural networks, offering a new approach to enhancing model performance. Unlike existing methods, JPEG-DL leverages the inherent nonlinearity of JPEG compression to improve accuracy without compromising traditional preprocessing. The framework employs a masked conditional probability mass function (CPMF) to significantly reduce computational complexity during inference, restricting support to five reconstruction levels. Extensive experiments demonstrate JPEG-DL consistently outperforms baseline models across diverse datasets, including CIFAR-100 and the CUB-200 bird dataset, achieving substantial gains. The integration of data augmentation techniques, specifically JPEG-based methods, and chroma subsampling formats reveals the importance of color information for deep neural networks. Sensitivity analysis and table initialization strategies are explored to optimize the quantization table, minimizing computational overhead. The team addresses inference overhead by simplifying the CPMF calculation, demonstrating minimal impact on accuracy. Furthermore, JPEG-DL successfully incorporates differentiable soft quantization into DNN architectures, offering a new method for handling non-differentiable operations. This approach consistently outperforms prior methods, achieving significant gains in model performance, particularly after the first dense block. The frameworks effectiveness is demonstrated through strategic placement, such as after the first convolution layer, and robustness to hyperparameters, with learning rates as low as 0.001 yielding strong results. JPEG-DLs potential extends to fine-grained tasks and layer replacement scenarios, enhancing classification accuracy.  Overall, JPEG-DL represents a significant advancement in deep learning, establishing JPEG quantization as a valuable and efficient component.",
  "ID": "9f7efeec-4140-5f72-802e-5d882a1c0d48"
}